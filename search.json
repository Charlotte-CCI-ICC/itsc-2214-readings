[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ITSC 2214 - Data Structures and Algorithms - Summer 2023",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "01_on_computers_and_computing.html",
    "href": "01_on_computers_and_computing.html",
    "title": "1  On Computers and Computing",
    "section": "",
    "text": "2 NASA High Speed Flight Station “Computer Room”\nEarly “computers” at work, summer 1949. In the terminology of that period, computers were employees who performed the arduous task of transcribing raw data from rolls of celluloid film and strips of oscillograph paper and then, using slide rules and electric calculators, reducing into standard engineering units.\nIt’s the science of computers; it answers questions like"
  },
  {
    "objectID": "01_on_computers_and_computing.html#introduction-to-computer-science",
    "href": "01_on_computers_and_computing.html#introduction-to-computer-science",
    "title": "1  On Computers and Computing",
    "section": "1.1 Introduction to Computer Science",
    "text": "1.1 Introduction to Computer Science"
  },
  {
    "objectID": "01_on_computers_and_computing.html#topics-that-will-be-covered",
    "href": "01_on_computers_and_computing.html#topics-that-will-be-covered",
    "title": "1  On Computers and Computing",
    "section": "1.2 Topics that will be covered:",
    "text": "1.2 Topics that will be covered:\n\nWhat do these ‘Computer Science’ words mean?\nWhat exactly does it mean to compute?\nWhat’s the history of Computing?"
  },
  {
    "objectID": "01_on_computers_and_computing.html#science",
    "href": "01_on_computers_and_computing.html#science",
    "title": "1  On Computers and Computing",
    "section": "1.3 Science",
    "text": "1.3 Science\n\nA particular area of science\nA systematically organized body of knowledge on particular subject\nknowledge of any kind\nThe intellectual and practical activity encompassing the systematic study of the structure and behavior of the physical and natural world through observation and experiment"
  },
  {
    "objectID": "01_on_computers_and_computing.html#computer",
    "href": "01_on_computers_and_computing.html#computer",
    "title": "1  On Computers and Computing",
    "section": "1.4 Computer",
    "text": "1.4 Computer\n\nThe term “computer”, in use from the early 17th century (the first known written reference dates from 1613) meant “one who computes”: a person performing mathematical calculations, before electronic computers became commercially available.\nAlan Turing described the “human computer” as someone who is “supposed to be following fixed rules; he has no authority to deviate from them in any detail.” Teams of people, from the late nineteenth century onwards, were used to undertake long and often tedious calculations; the work was divided so that this could be done in parallel. The same calculations were frequently performed independently by separate teams to check the correctness of the results.\nSince the end of the 20th century, the term “human computer” has also been applied to individuals with prodigious powers of mental arithmetic, also known as mental calculators."
  },
  {
    "objectID": "01_on_computers_and_computing.html#lets-say-there-is-an-alternate-universe",
    "href": "01_on_computers_and_computing.html#lets-say-there-is-an-alternate-universe",
    "title": "1  On Computers and Computing",
    "section": "5.1 Let’s say there is an alternate Universe",
    "text": "5.1 Let’s say there is an alternate Universe\nWhere two guys live in adjacent houses, Cory and Jamal. There’s no phones or computers yet in this universe, so Cory and Jamal do things like playing scrabble in their free time. Cory is the only one that owns a scrabble set, and since you can’t play alone the only way they can play is if Jamal goes to Cory’s place. But Cory’s parents are rather religious, and their religion’s scripture says that there will one day be a game that youngster’s play that will bring about the end of the world. Cory’s parents thus never want Cory to go near Scrabble.\nIt then becomes a game of cat and mouse between Cory and his parents, with him needing to hide his playing of scrabble. Cory’s father is a nurse who sometimes must do whole night shifts at the hospital. Cory’s mother is a software engineer that sometimes works very late and is not home till 2am. These are they times Cory and Jamal get to play Scrabble. But they don’t have a fixed schedule, so they cannot be relied on to be absent every night. Cory has two table lamps at his desk which can be seen from Jamal’s window. So they come up with a scheme. If Cory’s mother isn’t home, Cory turns on the right lamp. If Cory’s father isn’t home he turns on the left lamp. Meaning that–if both lamps are on–it’s like an invite for Jamal to come play Scrabble if he feels like.\nNow, sometimes Cory’s sister who’s usually at the University is at home, and on such days, they can only play till 1 am, which is when she comes out of her room to grab something to eat. And so Cory and Jamal decide that if his sister is home, he’ll turn on the ceiling fan in his room and Jamal will know that he can come only for a little while. For every extra piece of information Cory wanted to give to Jamal, they needed on more thing. If it’s was only about his mom, just one lamp being on or off gives the requisite information – one thing. If you add his dad into the mix, you need two lamps-two things. If you add his sister to the mix, you need two lamps and a ceiling fan– three things.\n\n5.1.1 A processor in a computer kind of works in similar ways\n\nA processor in a computer has a set of operations it can do. Copy data, move data, Add data, subtract, multiply, divide, jump to another instruction, branch conditionally, etc.\nWhen you want to tell computer to do a certain operation, you need several things to convey this information about what to do to the Processor.\nInstead of lamps or fans being on or off, computers have very, very small wires with or without electricity.\nPresence or absence of Electricity conveys information about what to do. This, together, is known as one “Instruction\nSometimes, just telling a processor to do something isn’t enough. Like for jumping, you also need to tell the processor where to jump to, So you need additional wires to provide information on what Data to do the instruction on.\nSometimes, like for say adding. One piece of information isn’t enough. You need to know two numbers to add them together.\nSometimes, along with adding, you also need to tell the processor where to store the result of whatever it is we are adding.\nEach of these 0 or 1 is known as a “bit”. 8 such bits together is known as a “byte”. The “size” of an instruction (the number of bits it has) depends on the kind of processor. Most modern CPUs use 64 bits.\n\n\n\n5.1.2 Consider a Rube Goldberg machine\n\nIt is a giant, very, very, complex contraption that has a bunch of lanes (each line below is a lane) which takes in the presence or absence of marbles, and the marbles rolling through the contraption causes some sort of side effect and this side effect is designed to be the intended operation.\nIn the processor’s case, presence or absence of electricity rolls through the processor on a bunch of lanes and triggers some side effect and the processor is designed such that the side effect is our intended operation.\nIt just so happens that the processor itself is very tiny and everything happens so fast that there is no good tools available to us that would let us see the operation of this rube Goldberg machine.\n\n\n\n5.1.3 Memory\n\nMemory is just a giant grid of cells which holds “electricity”.\nIf we consider our previous example, think of it as a giant grid which as a bunch of balls stored in it and that you can open lanes from any cell to let the marble (electricity) roll on into the processor to do stuff with it.\nThe processor has instructions which lets it select which cells to “open” roll marbles (data) from and use this data as instructions or operands, whichever.\nWhen I say Memory I mean RAM or Active memory.\n\n\n\n5.1.4 A program\n\nIs a region of your memory which holds instructions and data. When the first instruction from this region of memory starts going to the Processor we say that the program has started execution.\n\n\n\n5.1.5 Java Compiler\n\nIs a program that takes the “code” you write as text and turns it into a Program.\nThat is, turns your code into a bunch of “bits” that are VALID sequence of instructions for your processor to execute to do the thing that your code wants to do.\n\n\n\n5.1.6 Program\n\nWhen we write and compile a program to “solve” an assignment, essentially we are creating sequences in memory filled with instructions that carry out, for example, the objective of assignment 1.\nOnce the processor is done carrying out all these instructions, the assignment is complete.\nThis is not exactly true, but the “GHz” or the gigahertz you see next to a processors specifications refers to the number of instructions the processor can execute in one second.\nIf you have a 1GHz processor, you can execute 1 billion (10^12) instructions per second."
  },
  {
    "objectID": "09_graphs.html#background-and-motivation",
    "href": "09_graphs.html#background-and-motivation",
    "title": "2  The Graph Data Structure",
    "section": "2.1 Background and Motivation",
    "text": "2.1 Background and Motivation\nImagine you want to represent the connections between you and your Instagram followers in a data structure. The diagram below (Figure 2.1) shows a simple representation of your followers as numbered vertices and the edges represent the connections between you and your followers.\n\n\n\n\n\n\n\n\nG\n\n  \n\nYou\n\n You   \n\n1\n\n 1   \n\nYou–1\n\n   \n\n2\n\n 2   \n\nYou–2\n\n   \n\n3\n\n 3   \n\nYou–3\n\n   \n\n4\n\n 4   \n\nYou–4\n\n   \n\n5\n\n 5   \n\nYou–5\n\n   \n\n6\n\n 6   \n\nYou–6\n\n   \n\n7\n\n 7   \n\nYou–7\n\n   \n\n8\n\n 8   \n\nYou–8\n\n   \n\n9\n\n 9   \n\nYou–9\n\n   \n\n10\n\n 10   \n\nYou–10\n\n   \n\n11\n\n 11   \n\nYou–11\n\n   \n\n12\n\n 12   \n\nYou–12\n\n   \n\n13\n\n 13   \n\nYou–13\n\n   \n\n14\n\n 14   \n\nYou–14\n\n   \n\n15\n\n 15   \n\nYou–15\n\n   \n\n16\n\n 16   \n\nYou–16\n\n   \n\n17\n\n 17   \n\nYou–17\n\n   \n\n18\n\n 18   \n\nYou–18\n\n   \n\n19\n\n 19   \n\nYou–19\n\n   \n\n20\n\n 20   \n\nYou–20\n\n  \n\n\nFigure 2.1: A representation of your Instagram followers.\n\n\n\n\nAt first glance, it might appear to be a tree structure, but that is not the case. Your followers can follow other people, who in turn can have their followers. This creates a recursive relationship that cannot be represented using a tree data structure (see Figure 2.2).\n\n\n\n\n\n\n\n\nG\n\n  \n\nYou\n\n You   \n\n1\n\n 1   \n\nYou–1\n\n   \n\n2\n\n 2   \n\nYou–2\n\n   \n\n3\n\n 3   \n\nYou–3\n\n   \n\n4\n\n 4   \n\nYou–4\n\n   \n\n5\n\n 5   \n\nYou–5\n\n   \n\n6\n\n 6   \n\nYou–6\n\n   \n\n7\n\n 7   \n\nYou–7\n\n   \n\n8\n\n 8   \n\nYou–8\n\n   \n\n9\n\n 9   \n\nYou–9\n\n   \n\n10\n\n 10   \n\nYou–10\n\n   \n\n11\n\n 11   \n\n1–11\n\n   \n\n12\n\n 12   \n\n1–12\n\n   \n\n13\n\n 13   \n\n1–13\n\n   \n\n14\n\n 14   \n\n1–14\n\n   \n\n15\n\n 15   \n\n1–15\n\n   \n\n16\n\n 16   \n\n2–16\n\n   \n\n17\n\n 17   \n\n2–17\n\n   \n\n18\n\n 18   \n\n2–18\n\n   \n\n19\n\n 19   \n\n2–19\n\n   \n\n20\n\n 20   \n\n2–20\n\n   \n\n12–1\n\n   \n\n12–3\n\n   \n\n12–5\n\n   \n\n12–7\n\n   \n\n14–4\n\n   \n\n14–8\n\n   \n\n14–12\n\n   \n\n16–2\n\n   \n\n16–6\n\n   \n\n16–10\n\n   \n\n16–14\n\n   \n\n19–9\n\n   \n\n19–13\n\n   \n\n19–17\n\n  \n\n\nFigure 2.2: A representation of your instagram followers where they’re allowed to follow other people too.\n\n\n\n\nTo accurately capture this complex relationship, we need to use a graph data structure. Graphs consist of a set of vertices (or nodes) and a set of edges that connect them. In the context of Instagram followers, the vertices represent the users, and the edges represent the connections between them.\nUsing a graph data structure allows us to represent the recursive nature of the relationship between you and your followers, enabling us to model and analyze the complex network of connections in a more accurate way.\nSure! I will translate the given block according to the specifications you provided. Here’s the updated version:\nFurthermore, the relationship between you and your followers is even more complex. For instance, you can follow someone who does not follow you back, creating a directed relationship where the edges have a direction. In this case, the edges represent the connections from you to your followers, but not the other way around. Such relationships are often modeled using directed graphs, which are a common use case for graphs. Figure 2.3 visualizes this directed relationship.\n\n\n\n\n\n\n\n\nG\n\n  \n\nYou\n\n You   \n\n1\n\n 1   \n\nYou-&gt;1\n\n    \n\n2\n\n 2   \n\nYou-&gt;2\n\n    \n\n3\n\n 3   \n\nYou-&gt;3\n\n    \n\n4\n\n 4   \n\nYou-&gt;4\n\n    \n\n5\n\n 5   \n\nYou-&gt;5\n\n    \n\n6\n\n 6   \n\nYou-&gt;6\n\n    \n\n7\n\n 7   \n\nYou-&gt;7\n\n    \n\n8\n\n 8   \n\nYou-&gt;8\n\n    \n\n9\n\n 9   \n\nYou-&gt;9\n\n    \n\n10\n\n 10   \n\nYou-&gt;10\n\n    \n\n11\n\n 11   \n\n1-&gt;11\n\n    \n\n12\n\n 12   \n\n1-&gt;12\n\n    \n\n13\n\n 13   \n\n1-&gt;13\n\n    \n\n14\n\n 14   \n\n1-&gt;14\n\n    \n\n15\n\n 15   \n\n1-&gt;15\n\n    \n\n16\n\n 16   \n\n2-&gt;16\n\n    \n\n17\n\n 17   \n\n2-&gt;17\n\n    \n\n18\n\n 18   \n\n2-&gt;18\n\n    \n\n19\n\n 19   \n\n2-&gt;19\n\n    \n\n20\n\n 20   \n\n2-&gt;20\n\n    \n\n12-&gt;1\n\n    \n\n12-&gt;3\n\n    \n\n12-&gt;5\n\n    \n\n12-&gt;7\n\n    \n\n14-&gt;4\n\n    \n\n14-&gt;8\n\n    \n\n14-&gt;12\n\n    \n\n16-&gt;2\n\n    \n\n16-&gt;6\n\n    \n\n16-&gt;10\n\n    \n\n16-&gt;14\n\n    \n\n19-&gt;9\n\n    \n\n19-&gt;13\n\n    \n\n19-&gt;17\n\n   \n\n\nFigure 2.3: A directed representation of your Instagram followers. Here, an arrow going from vertex \\(A\\) to vertex \\(B\\) indicates that \\(A\\) follows \\(B\\), but \\(B\\) does not necessarily follow \\(A\\)."
  },
  {
    "objectID": "09_graphs.html#introduction",
    "href": "09_graphs.html#introduction",
    "title": "2  The Graph Data Structure",
    "section": "2.2 Introduction",
    "text": "2.2 Introduction\nA graph is a non-linear data structure that consists of a set of vertices (also called nodes) and a set of edges (or connections) that connect these vertices. In this data structure, the arrangement of vertices and edges allows for a more flexible and complex representation of relationships between data elements compared to linear data structures like arrays, lists, or queues.\nThe concept of adjacency refers to the relationship between two vertices in a graph. If there is an edge connecting two vertices, they are said to be adjacent. Incidence is the relationship between a vertex and an edge. A vertex is said to be incident to an edge if it is one of the two vertices connected by that edge.\nGraphs have numerous real-life applications, and some examples include:\n\nSocial networks, where vertices represent people and edges represent friendships or connections\nTransportation networks, where vertices represent locations and edges represent roads or routes\nCoronavirus transmission networks, where vertices represent individuals and edges represent transmission paths"
  },
  {
    "objectID": "09_graphs.html#graph-terminology",
    "href": "09_graphs.html#graph-terminology",
    "title": "2  The Graph Data Structure",
    "section": "2.3 Graph Terminology",
    "text": "2.3 Graph Terminology\nBefore diving into the implementation of graph data structures, let’s discuss some basic terms and properties of graphs.\n\n2.3.1 Basic Terms and Properties\n\nA graph is a data structure for representing connections among items and consists of vertices connected by edges.​\nA vertex (or node) represents an item in a graph.​\nAn edge represents a connection between two vertices in a graph.\nTwo vertices are adjacent if connected by an edge.​\nDirected vs Undirected: In an undirected graph, the edges have no specific direction, meaning that if there is an edge between vertices A and B, the connection is mutual. In a directed graph (also called a digraph), the edges have a direction, indicating an asymmetrical relationship between vertices. (See Figure 2.4 and Figure 2.5 for examples.)\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nB–C\n\n   \n\nC–A\n\n  \n\n\nFigure 2.4: Example of an undirected graph.\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA-&gt;B\n\n    \n\nC\n\n C   \n\nB-&gt;C\n\n    \n\nC-&gt;A\n\n   \n\n\nFigure 2.5: Example of a directed graph.\n\n\n\n\n\nWeighted vs Unweighted: In an unweighted graph, all edges have equal importance, while in a weighted graph, each edge is assigned a value (or weight), representing the importance, cost, or distance between the connected vertices. (See Figure 2.6 and Figure 2.7 for examples.)\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nB–C\n\n   \n\nC–A\n\n  \n\n\nFigure 2.6: Example of an unweighted graph.\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n 2   \n\nC\n\n C   \n\nB–C\n\n 3   \n\nC–A\n\n 1  \n\n\nFigure 2.7: Example of a weighted graph.\n\n\n\n\n\nSimple vs Multigraph: A simple graph has no more than one edge between any pair of vertices and does not contain any self-loops (edges that connect a vertex to itself). A multigraph can have multiple edges between the same pair of vertices and may include self-loops. (See Figure 2.8 and Figure 2.9 for examples.)\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nB–C\n\n   \n\nC–A\n\n  \n\n\nFigure 2.8: Example of a simple graph.\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nA–A\n\n self-loop   \n\nB\n\n B   \n\nA–B\n\n   \n\nA–B\n\n e2   \n\nC\n\n C   \n\nB–C\n\n   \n\nC–A\n\n  \n\n\nFigure 2.9: Example of a multigraph.\n\n\n\n\n\nDegree: The degree of a vertex is the number of edges incident to it. In a directed graph, we can distinguish between in-degree (the number of edges directed towards the vertex) and out-degree (the number of edges directed away from the vertex). See Figure 2.10 for an example.\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n A (degree 3)   \n\nD\n\n D   \n\nA–D\n\n D (degree 1)   \n\nC\n\n C   \n\nB–C\n\n B (degree 3)   \n\nC–A\n\n C (degree 3)  \n\n\nFigure 2.10: Example graph with vertex degrees.\n\n\n\n\n\nPath: A path in a graph is a sequence of vertices connected by edges. See Figure 2.11 for an example.\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nB–C\n\n   \n\nD\n\n D   \n\nC–D\n\n  \n\n\nFigure 2.11: Example graph with a path from A to D.\n\n\n\n\n\nCycle: A cycle is a closed path, where the first and last vertices in the path are the same, and no vertex is visited more than once. See Figure 2.12 for an example.\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nB–C\n\n   \n\nC–A\n\n  \n\n\nFigure 2.12: Example graph with a cycle. (A-B-C)\n\n\n\n\n\nConnected vs Disconnected: A graph is connected if there is a path between every pair of vertices. If there is at least one pair of vertices with no path between them, the graph is disconnected. See Figure 2.13 and Figure 2.14 for examples.\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nB–C\n\n   \n\nC–A\n\n  \n\n\nFigure 2.13: Example of a connected graph.\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nB–C\n\n   \n\nD\n\n D   \n\nE\n\n E   \n\nD–E\n\n  \n\n\nFigure 2.14: Example of a disconnected graph.\n\n\n\n\n\n\n2.3.2 Graph Notation\nWe can use a notation like \\(G(V, E)\\), where \\(V\\) is the set of vertices and \\(E\\) is the set of edges, to represent a graph.\n\n\n2.3.3 Special Types of Graphs\n\nComplete Graph: A complete graph is a simple graph in which every pair of vertices is connected by a unique edge. See Figure 2.15 for an example.\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nA–C\n\n   \n\nD\n\n D   \n\nA–D\n\n   \n\nB–C\n\n   \n\nB–D\n\n   \n\nC–D\n\n  \n\n\nFigure 2.15: Example of a complete graph.\n\n\n\n\n\nBipartite Graph: A bipartite graph is a graph whose vertices can be divided into two disjoint sets such that all edges connect vertices from one set to the other, with no edges connecting vertices within the same set. See Figure 2.16 for an example.\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\n1\n\n 1   \n\nA–1\n\n   \n\n2\n\n 2   \n\nA–2\n\n   \n\nB\n\n B   \n\nB–1\n\n   \n\n3\n\n 3   \n\nB–3\n\n   \n\nC\n\n C   \n\nC–2\n\n   \n\nC–3\n\n  \n\n\nFigure 2.16: Example of a bipartite graph.\n\n\n\n\n\nTree: A tree is an undirected graph with no cycles, and all vertices are connected. It has a hierarchical structure, with one vertex acting as the root, and the other vertices connected in a parent-child relationship. See Figure 2.17 for an example.\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nA–C\n\n   \n\nD\n\n D   \n\nB–D\n\n   \n\nE\n\n E   \n\nB–E\n\n   \n\nF\n\n F   \n\nC–F\n\n   \n\nG\n\n G   \n\nC–G\n\n  \n\n\nFigure 2.17: Example of a tree."
  },
  {
    "objectID": "09_graphs.html#graph-representation",
    "href": "09_graphs.html#graph-representation",
    "title": "2  The Graph Data Structure",
    "section": "2.4 Graph Representation",
    "text": "2.4 Graph Representation\nIn order to work with graphs in code or store them in memory, we need efficient ways to represent them. There are multiple methods to represent graphs, and the choice of representation depends on factors such as the density of the graph, the operations to be performed, and memory constraints.\nIn this section, we will discuss two common methods to represent a graph: adjacency list and adjacency matrix.\n\n2.4.1 Adjacency List\nAn adjacency list represents a graph by storing a list of adjacent vertices for each vertex in the graph. This can be implemented using an array of lists or a hash table, where the index or key corresponds to a vertex, and the value is a list of adjacent vertices.\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nA–C\n\n   \n\nD\n\n D   \n\nB–D\n\n   \n\nC–D\n\n  \n\n\nFigure 2.18: See adjacency list for this example Listing 2.1\n\n\n\n\nAdjacency list representation for figure Figure 2.18:\n\nListing 2.1: Adjacency list representation.\nA: [B, C]\nB: [A, D]\nC: [A, D]\nD: [B, C]\n\n// or as an arraylist of arraylists -\n[[B, C], [A, D], [A, D], [B, C]]\n// here, the index of the outer arraylist represents the vertex.\n// in order for this to work, the order of the vertices must be \n// fixed, and stored separately.\n\nThe adjacency list representation is efficient for sparse graphs (graphs with relatively few edges) as it only stores the existing edges, reducing memory usage. This representation also allows for faster traversal of a vertex’s neighbors.\n\n\n2.4.2 Adjacency Matrix\nAn adjacency matrix is a two-dimensional array (or matrix) where the cell at the i-th row and j-th column represents the edge between vertex i and vertex j. For an undirected graph, the adjacency matrix is symmetric. For a weighted graph, the values in the cells represent the weights of the edges; for an unweighted graph, the cells contain either 1 (edge exists) or 0 (no edge).\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nA–C\n\n   \n\nD\n\n D   \n\nB–D\n\n   \n\nC–D\n\n  \n\n\nSee adjacency matrix for this example Listing 2.2\n\n\n\nAdjacency matrix representation (unweighted):\n\nListing 2.2: Adjacency matrix representation.\n  A B C D\nA 0 1 1 0\nB 1 0 0 1\nC 1 0 0 1\nD 0 1 1 0\n\nThe adjacency matrix representation is suitable for dense graphs (graphs with many edges) or when checking for the presence of an edge between two vertices needs to be fast. However, this representation can be inefficient in terms of memory usage, especially for large, sparse graphs, as it stores information for all possible edges, even if they do not exist.\n\n\n2.4.3 Converting Between Representations\nTo convert a graph diagram or notation into an adjacency list or an adjacency matrix, follow these steps:\n\nIdentify the vertices and edges in the graph.\nFor an adjacency list, create an empty list or hash table for each vertex. For each edge, add the adjacent vertices to the corresponding lists.\nFor an adjacency matrix, create a square matrix with dimensions equal to the number of vertices. For each edge, set the corresponding cells in the matrix to 1 (or the edge weight for weighted graphs).\n\nTo convert an adjacency list or an adjacency matrix back into a graph diagram or notation, follow these steps:\n\nIdentify the vertices based on the keys (for an adjacency list) or indices (for an adjacency matrix).\nFor an adjacency list, iterate through the lists and draw an edge for each adjacent vertex.\nFor an adjacency matrix, iterate through the matrix cells and draw an edge for each non-zero value (or the corresponding weight for weighted graphs)."
  },
  {
    "objectID": "09_graphs.html#graph-traversal",
    "href": "09_graphs.html#graph-traversal",
    "title": "2  The Graph Data Structure",
    "section": "2.5 Graph Traversal",
    "text": "2.5 Graph Traversal\nImagine you want to find the average age of all users on Facebook. With billions of users, it is infeasible to hold the entire graph of the friend network in memory. Ideally, we would want to find out information on each user one at a time, on a per-need basis. To achieve this, we can use graph traversal algorithms, which allow us to visit each user, add up their ages, and then calculate the average. A simple way to do this is to load information on a user, add all their friends to a stack, and then keep popping from the stack and requesting data from Facebook for each friend. When we receive the data, we mark that user as visited to avoid recounting their age if we reach the same user again. We then add friends of each loaded user to our stack and keep repeating until we run out of users in our stack.\nThis problem illustrates the importance of graph traversal, a fundamental operation in graph theory. Graphs are a powerful and versatile data structure that can model various kinds of relationships and networks, such as social networks, computer networks, transportation networks, web pages, games, and many other domains. Graph traversal allows us to explore and manipulate graphs in various ways, with applications in domains like searching for specific nodes, finding the shortest path between nodes, and analyzing the structure of a graph.\nGraph traversal algorithms typically begin with a start node and attempt to visit the remaining nodes from there. They must deal with several troublesome cases, such as unreachable nodes, revisited nodes, and choosing which node to visit next among several options. To handle these cases, graph traversal algorithms use different strategies and data structures to keep track of which nodes have been visited and which nodes are still pending. The most common graph traversal algorithms are breadth-first search (BFS) and depth-first search (DFS), which differ in the order in which they visit the nodes.\nIn some situations, we may not know the entire graph at once and instead only have access to a node object and its adjacent nodes. As demonstrated in the Facebook example, graph traversal algorithms can be used to solve problems that involve large and dynamic graphs by visiting each user and analyzing their information on a per-need basis.\nThere are two common methods to traverse a graph:\n\nBreadth-First Search (BFS)\nDepth-First Search (DFS)\n\nBy understanding and implementing these graph traversal methods, you can efficiently explore and manipulate complex graphs to solve a wide range of problems.\n\n2.5.1 Breadth-First Search (BFS)\nBreadth-First Search explores a graph by visiting all the neighbors of the starting vertex before moving on to their neighbors. BFS uses a queue data structure to keep track of the vertices to visit.\nHere’s a step-by-step example of BFS traversal (for the graph in example Figure 2.19):\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nA–C\n\n   \n\nD\n\n D   \n\nB–D\n\n   \n\nC–D\n\n   \n\nE\n\n E   \n\nC–E\n\n  \n\n\nFigure 2.19: Example graph for BFS traversal.\n\n\n\n\nBFS traversal starting from vertex A:\n\nVisit A and add its neighbors B and C to the queue: [B, C]\nVisit B and add its unvisited neighbor D to the queue: [C, D]\nVisit C and add its unvisited neighbor E to the queue: [D, E]\nVisit D: [E]\nVisit E: []\n\nBFS traversal order: A, B, C, D, E\nBFS pseudocode:\nBFS(graph, start):\n  Initialize an empty queue Q\n  Mark start as visited\n  Enqueue start into Q\n  \n  while Q is not empty:\n    vertex = Dequeue(Q)\n    Visit vertex\n    \n    for each neighbor of vertex:\n      if neighbor is not visited:\n        Mark neighbor as visited\n        Enqueue neighbor into Q\n\n\n2.5.2 Depth-First Search (DFS)\nDepth-First Search explores a graph by visiting a vertex and its neighbors as deeply as possible before backtracking. DFS can be implemented using recursion or an explicit stack data structure.\nHere’s a step-by-step example of DFS traversal (for the graph in example Figure 2.20):\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nA–C\n\n   \n\nD\n\n D   \n\nB–D\n\n   \n\nC–D\n\n   \n\nE\n\n E   \n\nC–E\n\n  \n\n\nFigure 2.20: Example graph for DFS traversal.\n\n\n\n\nDFS traversal starting from vertex A:\n\nVisit A and recurse on its first neighbor B\nVisit B and recurse on its first neighbor D\nVisit D and backtrack (no unvisited neighbors)\nBacktrack to A and recurse on its next neighbor C\nVisit C and recurse on its first neighbor E\nVisit E and backtrack (no unvisited neighbors)\n\nDFS traversal order: A, B, D, C, E\nDFS pseudocode (recursive):\nDFS(graph, vertex):\n  Mark vertex as visited\n  Visit vertex\n  \n  for each neighbor of vertex:\n    if neighbor is not visited:\n      DFS(graph, neighbor)\nDFS pseudocode (iterative with a stack):\nDFS(graph, start):\n  Initialize an empty stack S\n  Mark start as visited\n  Push start onto S\n  \n  while S is not empty:\n    vertex = Pop(S)\n    Visit vertex\n    \n    for each neighbor of vertex:\n      if neighbor is not visited:\n        Mark neighbor as visited\n        Push neighbor onto S\n\n\n2.5.3 Applications and Variations of BFS and DFS\nBoth BFS and DFS have numerous applications and can be adapted to solve various graph-related problems:\n\nShortest path: BFS can be used to find the shortest path between two vertices in an unweighted graph. The algorithm can be modified to keep track of the path length or the actual path itself.\nConnected components: Both BFS and DFS can be used to find connected components in an undirected graph. By running the traversal algorithm and marking visited vertices, we can identify the set of vertices reachable from a starting vertex. Repeating this process for all unvisited vertices will find all connected components in the graph.\nTopological sorting: DFS can be adapted to perform a topological sort on a directed acyclic graph (DAG). A topological ordering is a linear ordering of the vertices such that for every directed edge (u, v), vertex u comes before vertex v in the ordering. This can be useful in scheduling tasks with dependencies or determining the order of courses in a curriculum.\nBipartite graph check: BFS or DFS can be used to check if a graph is bipartite. The algorithm can be modified to color vertices while traversing the graph. If at any point during the traversal, two adjacent vertices have the same color, the graph is not bipartite.\nGraph cycle detection: DFS can be used to detect cycles in a graph. By keeping track of the recursion stack, we can determine if a vertex is visited more than once in the same path, indicating a cycle.\n\nIn summary, graph traversal is a fundamental operation in graph theory with various applications. Breadth-First Search (BFS) and Depth-First Search (DFS) are two common techniques to traverse a graph, each with its own advantages and use cases. Understanding these algorithms and their variations can help solve a wide range of graph-related problems."
  },
  {
    "objectID": "10_hashing.html#background-and-motivation",
    "href": "10_hashing.html#background-and-motivation",
    "title": "3  Hashing, Hash Tables, and Hash Maps",
    "section": "3.1 Background and Motivation",
    "text": "3.1 Background and Motivation\n\n3.1.1 Indexing\nIndexing refers to the idea of accessing a certain element of an array by referring to it using a specific number, called the index. Using the same index always returns the same element, as long as the array remains unchanged. For example, to access the 12th element of an array arr, we use arr[11].\nThe math for finding the address of an element in the array works out to be:\nbaseAddress + (index * sizeOfElement)\nNow, keeping that in mind, let’s explore the limitations of indexing.\n\n\n3.1.2 Limitations of Indexing\nSuppose we want to access an element in the array using a string as an index, such as arr[\"dhruv\"]. What is stopping us?\nThe problem is that we cannot calculate baseAddress + (\"dhruv\" * sizeOfElement) because the index, in this case, is a string, not a number. The operation is not defined, and therefore, we can’t directly use strings as indices in an array.\n\n\n3.1.3 Mapping Strings to Numbers\nLet’s consider an array of strings. We can use it to map a number to a string:\n0 -&gt; \"Alice\"\n1 -&gt; \"Bob\"\n2 -&gt; \"Charlie\"\nIf we can use an array to map a number to a string, can we also use it to map strings to numbers? Yes, we can!\nOne way to do this is by searching linearly for a string in the array to find its index. For example, we can find the string “Dhruv” at index 5:\n0 -&gt; \"Alice\"\n1 -&gt; \"Bob\"\n2 -&gt; \"Charlie\"\n...\n5 -&gt; \"Dhruv\"\nThe index at which we found the string “Dhruv” (in this case, 5) can be used as a key in a different array to find the data related to “Dhruv”. However, this method of linear searching can be quite slow for large datasets.\n\n\n3.1.4 Hashing: A Better Solution\nThis is where hashing comes into play. Hashing allows us to efficiently map strings (or any other non-numeric keys) to numbers. By using a hash function, we can convert a string into a number that represents the index in the array.\nA hash function takes a key as input and outputs an index in the hash table’s array. A good hash function has the following criteria:\n\nUniform distribution: The hash function should distribute keys evenly across the array to minimize collisions (when multiple keys map to the same index).\nMinimal collisions: A good hash function should minimize the chance of collisions.\nFast computation: The hash function should be fast to compute, allowing for quick insertion, deletion, and retrieval of data.\nDeterministic output: The hash function should produce the same output for the same input every time it is called.\n\nFor example, let’s consider a simple hash function that converts the first character of a string into its ASCII code:\nhash(\"dhruv\") = ASCII('d') = 100\nThe output of the hash function is 100, which we can use as an index in an array to store or retrieve data related to “dhruv”. This allows us to use strings (and other non-numeric keys) as indices, achieving our goal of efficient mapping."
  },
  {
    "objectID": "10_hashing.html#hash-functions",
    "href": "10_hashing.html#hash-functions",
    "title": "3  Hashing, Hash Tables, and Hash Maps",
    "section": "3.2 Hash Functions",
    "text": "3.2 Hash Functions\n\n3.2.1 Introduction\nPreviously, we managed to map a string “D” to some data, just like an index maps to some data in an array. The resulting data structure that can map any string to any data is called a hash table. The function used to map strings to data is called a hash function. The concept of mapping “D” to some data can be referred to as hashing “D” to an index 3, which is where we found the value corresponding to “D”.\nNow, we’ll learn about a way of mapping any object (called a key) to any other object (called the record, or the value). For instance, your student ID can be a key, and all the data about you on your ID can be stored in a record object.\n\n\n3.2.2 Hashing\nHashing can be thought of as a method for storing and retrieving records from a database. It lets you insert, delete, and search for records based on a search key value. When properly implemented, these operations can be performed in constant time. In fact, a properly tuned hash system typically looks at only one or two records for each search, insert, or delete operation. This is far better than the \\(O(log n)\\) average cost required to do a binary search on a sorted array of n records, or the \\(O(log n)\\) average cost required to do an operation on a binary search tree. However, even though hashing is based on a very simple idea, it is surprisingly difficult to implement properly. Designers need to pay careful attention to all of the details involved with implementing a hash system.\nA hash system stores records in an array called a hash table, which we will call HT. Hashing works by performing a computation on a search key K in a way that is intended to identify the position in HT that contains the record with key K. The function that does this calculation is called the hash function, and will be denoted by the letter h. Since hashing schemes place records in the table in whatever order satisfies the needs of the address calculation, records are not ordered by value. A position in the hash table is also known as a slot. The number of slots in hash table HT will be denoted by the variable M with slots numbered from 0 to M−1. The goal for a hashing system is to arrange things such that, for any key value K and some hash function h, \\(i=h(K)\\) is a slot in the table such that \\(0 \\leq i &lt; M\\), and we have the key of the record stored at HT[i] equal to K.\nHashing is not good for applications where multiple records with the same key value are permitted. Hashing is not a good method for answering range searches. In other words, we cannot easily find all records (if any) whose key values fall within a certain range. Nor can we easily find the record with the minimum or maximum key value or visit the records in key order. Hashing is most appropriate for answering the question, ‘What record, if any, has key value K?’ For applications where all search is done by exact-match queries, hashing is the search method of choice because it is extremely efficient when implemented correctly.\nHashing generally takes records whose key values come from a large range and stores those records in a table with a relatively small number of slots. Since keys have a large range and values have smaller, limited slots for storage – A hash function might sometimes end up hashing two keys to the same slot. We refer to such an event as a collision.\nTo illustrate, consider a classroom full of students. What is the probability that some pair of students shares the same birthday (i.e., the same day of the year, not necessarily the same year)? If there are 23 students, then it is unlikely that more than one student will share the same birthday. There are 365 “slots” or possible days a student can have a birthday on; but only 23 “keys”. As the number of students increases, the probability of a “collision” or two students sharing a birthday increases. To be practical, a database organized by hashing must store records in a hash table that is not so large that it wastes space.\nWe would like to pick a hash function that maps keys to slots in a way that makes each slot in the hash table have equal probability of being filled for the actual set keys being used. Unfortunately, we normally have no control over the distribution of key values for the actual records in a given database or collection. So how well any particular hash function does depends on the actual distribution of the keys used within the allowable key range. In some cases, incoming data are well distributed across their key range. For example, if the input is a set of random numbers selected uniformly from the key range, any hash function that assigns the key range so that each slot in the hash table receives an equal share of the range will likely also distribute the input records uniformly within the table.\nHowever, in many applications the incoming records are highly clustered or otherwise poorly distributed. When input records are not well distributed throughout the key range it can be difficult to devise a hash function that does a good job of distributing the records throughout the table, especially if the input distribution is not known in advance. For example, If the input is a collection of English words, the beginning letter will be poorly distributed. A dictionary of words mapped to their frequency is often used in rudimentary natural language processing algorithms.\nIn conclusion, anything can be a hash function (i.e., map a value to an index), but not everything can be a good hash function. A function that always returns the index 0 is a hash function that maps everything to 0. It’s no good but it’s still a hash function. An example of a commonly used hash function is the modulus operator! It is common for N-sized hash tables to use the modulus of N as a hash function. If N is \\(20\\), data for 113 will be hashed to index \\(113 \\% 20 = 13\\).\nBut if we use the modulo operator as a hash function, what do we do when multiple pieces of data map to the same index? \\(53 \\% 20 = 13\\), \\(73 \\% 20 = 13\\), etc. But if you think about it, we can store everything at \\(13\\)! By using nested data structures… More on this later.\n\n\n3.2.3 Simple Hash Functions\nLet’s apply a simple hash function to a set of keys and compute their indices. In this example, we’ll use the modulo operation as the hash function. Given a hash table with a size of 5, we can compute the indices for the keys as follows:\nHashTable size: 5\nHashFunction: key % size\n\nKeys: 15, 28, 47, 10, 33\n\nIndices:\n15 % 5 = 0\n28 % 5 = 3\n47 % 5 = 2\n10 % 5 = 0\n33 % 5 = 3\n\n\n3.2.4 Other Types of Hash Functions\n\n3.2.4.1 Direct Hashing\nA direct hash function uses the item’s key as the bucket index. For example, if the key is 937, the index is 937. A hash table with a direct hash function is called a direct access table. Given a key, a direct access table search algorithm returns the item at index key if the bucket is not empty, and returns null (indicating item not found) if empty.\nLimitations:\nA direct access table has the advantage of no collisions: Each key is unique (by definition of a key), and each gets a unique bucket, so no collisions can occur. However, a direct access table has two main limitations:\n\nAll keys must be non-negative integers, but for some applications, keys may be negative.\nThe hash table’s size equals the largest key value plus 1, which may be very large.\n\nSimilarly, there are other hash functions each with their own characteristics.\n\n\n3.2.4.2 Modulo Hash\nA modulo hash function computes the index by taking the remainder of the key divided by the table size M. This is a simple and effective way to convert a large key range into a smaller index range. The hash function can be defined as:\nh(K) = K % M\n\n\n3.2.4.3 Mid-Square Hash\nA mid-square hash function computes the index by first squaring the key, and then extracting a portion of the squared value as the index. This approach is especially useful when the keys are not uniformly distributed. The hash function can be defined as:\nh(K) = middle_digits(K^2)\n\n\n3.2.4.4 Mid-Square Hash with Base 2\nA mid-square hash function with base 2 is a variation of the mid-square hash function, where the key is first squared, and then the middle bits of the binary representation of the squared value are extracted as the index. This approach is especially useful for binary keys. The hash function can be defined as:\nh(K) = middle_bits(K^2)\n\n\n3.2.4.5 Multiplicative String Hashing\nA multiplicative string hashing function computes the index by treating the characters in the string as numbers and combining them using a multiplication and a constant. This approach can help achieve a good distribution of string keys in the hash table. The hash function can be defined as:\nh(K) = (c1 * a^(n-1) + c2 * a^(n-2) + ... + cn) % M\nwhere c1, c2, ..., cn are the character codes of the string, a is a constant, n is the length of the string, and M is the size of the hash table.\nHere’s the ASCII representation of the resulting hash table:\nIndex | Key\n-------------\n  0   | 15\n  1   | -\n  2   | 47\n  3   | 28\n  4   | -\nIn this example, we can see that the keys 15 and 10, as well as 28 and 33, have collided, as they both map to the same indices (0 and 3, respectively).\n\n\n\n3.2.5 Trade-offs Between Different Hash Functions\nThere are trade-offs between different hash functions in terms of performance and complexity:\n\nA simple hash function, like the modulo operation, is fast to compute but may not distribute keys uniformly, leading to more collisions and reduced performance.\nMore complex hash functions, such as cryptographic hash functions, can provide a better distribution of keys but may be slower to compute.\n\nIn practice, the choice of a hash function depends on the specific requirements of the application and the data being stored. The goal is to find a balance between uniform distribution, minimal collisions, fast computation, and deterministic output."
  },
  {
    "objectID": "10_hashing.html#hash-collisions",
    "href": "10_hashing.html#hash-collisions",
    "title": "3  Hashing, Hash Tables, and Hash Maps",
    "section": "3.3 Hash Collisions",
    "text": "3.3 Hash Collisions\nHash collisions occur when two or more keys map to the same index in the hash table. Due to the pigeonhole principle, hash collisions are inevitable, as there are typically more possible keys than available indices in the array. Collisions negatively impact the efficiency of hashing, as they can lead to longer access times for insertion, deletion, and retrieval of key-value pairs.\nThere are two primary methods to resolve hash collisions: chaining and open addressing."
  },
  {
    "objectID": "10_hashing.html#chaining",
    "href": "10_hashing.html#chaining",
    "title": "3  Hashing, Hash Tables, and Hash Maps",
    "section": "3.4 Chaining",
    "text": "3.4 Chaining\nChaining is a collision resolution technique that uses a linked list or another data structure to store multiple key-value pairs at the same index. When a collision occurs, the new key-value pair is simply added to the data structure at the index.\n\n3.4.1 Insertion, Search, and Deletion\nHere’s how to perform insertion, search, and deletion operations using chaining:\n\nInsertion: Calculate the index using the hash function. If the index is empty, create a new data structure (e.g., linked list) and insert the key-value pair. If the index is not empty, add the key-value pair to the existing data structure.\nSearch: Calculate the index using the hash function. If the index is empty, the key is not in the hash table. If the index is not empty, search the data structure at the index for the key.\nDeletion: Calculate the index using the hash function. If the index is empty, the key is not in the hash table. If the index is not empty, search the data structure at the index for the key and remove it if found.\n\n\n\n3.4.2 Advantages and Disadvantages of Chaining\nChaining has several advantages and disadvantages:\n\nAdvantages:\n\nEasy implementation: Chaining can be easily implemented using existing data structures like linked lists.\nDynamic size: The data structure at each index can grow or shrink as needed, allowing for efficient use of space.\n\nDisadvantages:\n\nExtra space: Chaining requires additional space for the data structure at each index, which can increase memory overhead.\nVariable access time: The access time for key-value pairs depends on the length of the data structure at the index, which can vary.\n\n\nChaining is a popular method for resolving hash collisions due to its simplicity and dynamic size. However, it may not be the most efficient option for all use cases, especially when memory overhead and variable access times are critical factors."
  },
  {
    "objectID": "10_hashing.html#open-addressing",
    "href": "10_hashing.html#open-addressing",
    "title": "3  Hashing, Hash Tables, and Hash Maps",
    "section": "3.5 Open Addressing",
    "text": "3.5 Open Addressing\nOpen addressing is a collision resolution technique that finds an alternative index for a key-value pair if the original index is occupied. When a collision occurs, the algorithm searches for the next available index using a probing technique. There are three common types of probing techniques: linear probing, quadratic probing, and double hashing.\n\n3.5.1 Probing Techniques\n\nLinear probing: When a collision occurs, search the hash table linearly (one index at a time) until an empty slot is found.\nQuadratic probing: When a collision occurs, search the hash table quadratically (by increasing the index by the square of the probe number) until an empty slot is found.\nDouble hashing: When a collision occurs, use a secondary hash function to compute a new index for the key-value pair, and repeat this process until an empty slot is found.\n\n\n\n3.5.2 Insertion, Search, and Deletion\nHere’s how to perform insertion, search, and deletion operations using open addressing:\n\nInsertion: Calculate the index using the hash function. If the index is empty, insert the key-value pair. If the index is occupied, use the chosen probing technique to find the next available index and insert the key-value pair there.\nSearch: Calculate the index using the hash function. If the index is empty, the key is not in the hash table. If the index is occupied, check if the key matches the stored key. If not, use the chosen probing technique to search for the next index until the key is found or an empty index is encountered.\nDeletion: Calculate the index using the hash function. If the index is empty, the key is not in the hash table. If the index is occupied and the key matches the stored key, remove the key-value pair and mark the index as deleted. Continue searching using the chosen probing technique to handle cases where the removed key-value pair was part of a cluster.\n\n\n\n3.5.3 Advantages and Disadvantages of Open Addressing\nOpen addressing has several advantages and disadvantages:\n\nAdvantages:\n\nNo extra space: Open addressing does not require additional space for data structures at each index, making it more memory-efficient.\nFixed size: The hash table has a fixed size, which can be useful when memory is limited.\n\nDisadvantages:\n\nClustering: Probing techniques can cause clusters of key-value pairs to form, leading to increased access times.\nDeletion issues: Deleting key-value pairs can create complications, as it may leave “holes” in clusters that need to be addressed.\n\n\nOpen addressing is an alternative method for resolving hash collisions that can be more memory-efficient than chaining. However, it may not be the best option for all use cases, especially when clustering and deletion issues are critical factors."
  },
  {
    "objectID": "10_hashing.html#complexity-and-load-factor",
    "href": "10_hashing.html#complexity-and-load-factor",
    "title": "3  Hashing, Hash Tables, and Hash Maps",
    "section": "3.6 Complexity and Load Factor",
    "text": "3.6 Complexity and Load Factor\nWhen analyzing the complexity of hash functions and hash tables, we need to consider the time taken for searching, inserting, or deleting an element. There are two main steps involved in these operations:\n\nComputing the hash function for the given key.\nTraversing the list of key-value pairs present at the computed index.\n\n\n3.6.1 Time Complexity of Hash Computation\nFor the first step, the time taken depends on the key and the hash function. For example, if the key is a string “abcd”, then its hash function may depend on the length of the string. But for very large values of n, the number of entries into the map, the length of the keys is almost negligible in comparison to n, so hash computation can be considered to take place in constant time, i.e., O(1).\n\n\n3.6.2 Time Complexity of List Traversal\nFor the second step, traversal of the list of key-value pairs present at that index needs to be done. In the worst case, all the n entries are at the same index, resulting in a time complexity of O(n). However, enough research has been done to make hash functions uniformly distribute the keys in the array, so this almost never happens.\n\n\n3.6.3 Load Factor\nOn average, if there are n entries and b is the size of the array, there would be n/b entries at each index. This value n/b is called the load factor, which represents the load on our map. The load factor is denoted by the symbol λ:\nλ = n/b\nThis load factor needs to be kept low so that the number of entries at one index is less, and the complexity remains almost constant, i.e., O(1).\n\n\n3.6.4 Balancing Load Factor and Complexity\nTo maintain the load factor at an acceptable level, the hash table can be resized when the load factor exceeds a certain threshold. This helps to keep the complexity of hash table operations near O(1) by redistributing the keys uniformly across a larger array.\nIn conclusion, understanding the complexity and load factor of hash functions is crucial for designing efficient hash tables. By carefully choosing a suitable hash function and managing the load factor, it’s possible to achieve near-constant time complexity for various hash table operations."
  },
  {
    "objectID": "10_hashing.html#rehashing",
    "href": "10_hashing.html#rehashing",
    "title": "3  Hashing, Hash Tables, and Hash Maps",
    "section": "3.7 Rehashing",
    "text": "3.7 Rehashing\nRehashing, as the name suggests, means hashing again. When the load factor increases to more than its pre-defined value (the default value of the load factor is 0.75), the complexity increases. To overcome this issue, the size of the array is increased (typically doubled) and all the values are hashed again and stored in the new, larger array. This helps maintain a low load factor and low complexity.\n\n3.7.1 Why?\nRehashing is done because whenever key-value pairs are inserted into the map, the load factor increases, which implies that the time complexity also increases, as explained earlier. This might not provide the desired time complexity of O(1). Hence, rehashing must be performed, increasing the size of the bucketArray to reduce the load factor and the time complexity.\n\n\n3.7.2 How?\nRehashing can be done as follows:\n\nFor each addition of a new entry to the map, check the load factor.\nIf the load factor is greater than its pre-defined value (or the default value of 0.75 if not given), then perform rehashing.\nTo rehash, create a new array of double the previous size and make it the new bucketArray.\nTraverse each element in the old bucketArray and call the insert() method for each, to insert it into the new larger bucketArray.\n\nThe following diagram illustrates the rehashing process:\nInitial bucketArray (size = 4):\n+---+---+---+---+\n|   | K1|   | K2|\n+---+---+---+---+\n\nAfter inserting a new key K3 (load factor &gt; 0.75):\n\nNew bucketArray (size = 8):\n+---+---+---+---+---+---+---+---+\n|   | K1|   | K2|   |   |   | K3|\n+---+---+---+---+---+---+---+---+\nBy rehashing, the hash table maintains its desired time complexity of O(1) even as the number of elements increases. It is important to note that rehashing can be a costly operation, especially if the number of elements in the hash table is large. However, since rehashing is done infrequently and only when the load factor surpasses a certain threshold, the amortized cost of rehashing remains low, allowing the hash table operations to maintain near-constant time complexity."
  },
  {
    "objectID": "10_hashing.html#hash-tables-vs-hash-maps",
    "href": "10_hashing.html#hash-tables-vs-hash-maps",
    "title": "3  Hashing, Hash Tables, and Hash Maps",
    "section": "3.8 Hash Tables vs Hash Maps",
    "text": "3.8 Hash Tables vs Hash Maps\nHash tables and hash maps differ in their implementation and functionality.\n\nHash tables use direct hashing, where the key is an integer or can be directly converted to an integer (e.g., a string of digits). The integer is then used to compute the index in the hash table.\nHash maps use indirect hashing, where the key can be any data type. A separate hash function is needed to convert the key into an index in the hash table.\n\nWhen deciding whether to use a hash table or a hash map, consider the problem domain and the data type of the keys:\n\nIf the keys are integers or can be directly converted to integers, a hash table may be a more suitable choice. For example, if you’re working with student IDs as keys, a hash table would be a good fit.\nIf the keys are of any other data type or cannot be directly converted to integers, a hash map would be more appropriate. For example, if you’re working with strings, such as usernames or URLs, a hash map would be a better choice."
  },
  {
    "objectID": "10_hashing.html#hashmaps-in-java",
    "href": "10_hashing.html#hashmaps-in-java",
    "title": "3  Hashing, Hash Tables, and Hash Maps",
    "section": "3.9 HashMaps in Java",
    "text": "3.9 HashMaps in Java\nA HashMap is a collection in Java that implements the Map interface and uses a hash table for storage. It stores key-value pairs, where each key is unique, and the keys are not ordered.\nHere’s how to use a HashMap in Java:\n\nImport the HashMap class: To use the HashMap class in your Java code, you’ll need to import it from the java.util package:\nimport java.util.HashMap;\nCreate a HashMap: To create a new HashMap, use the following syntax:\nHashMap&lt;String, Integer&gt; myMap = new HashMap&lt;String, Integer&gt;();\nAdd elements: To add key-value pairs to the HashMap, use the put() method:\nmyMap.put(\"apple\", 3);\nmyMap.put(\"banana\", 5);\nmyMap.put(\"orange\", 2);\nAccess elements: To access the value associated with a key, use the get() method:\nint apples = myMap.get(\"apple\"); // 3\nint oranges = myMap.get(\"orange\"); // 2\nRemove elements: To remove a key-value pair from the HashMap, use the remove() method:\nmyMap.remove(\"banana\");\nCheck if a key exists: To check if a key is in the HashMap, use the containsKey() method:\nboolean hasApple = myMap.containsKey(\"apple\"); // true\nboolean hasGrape = myMap.containsKey(\"grape\"); // false\nIterate over keys: To iterate over the keys in a HashMap, you can use a for-each loop with the keySet() method:\nfor (String fruit : myMap.keySet()) {\n    System.out.println(fruit + \": \" + myMap.get(fruit));\n}\nIterate over values: To iterate over the values in a HashMap, you can use a for-each loop with the values() method:\nfor (Integer count : myMap.values()) {\n    System.out.println(count);\n}\nIterate over key-value pairs: To iterate over the key-value pairs in a HashMap, you can use a for-each loop with the entrySet() method:\nfor (HashMap.Entry&lt;String, Integer&gt; entry : myMap.entrySet()) {\n    System.out.println(entry.getKey() + \": \" + entry.getValue());\n}\n\nA HashMap can be a useful data structure when you need to store key-value pairs efficiently. It provides constant-time performance for common operations like put, get, and remove, making it an ideal choice for various applications."
  },
  {
    "objectID": "10_hashing.html#hashtables-in-java",
    "href": "10_hashing.html#hashtables-in-java",
    "title": "3  Hashing, Hash Tables, and Hash Maps",
    "section": "3.10 HashTables in Java",
    "text": "3.10 HashTables in Java\nA HashTable is a collection in Java that implements the Map interface and uses a hash table for storage. It is similar to a HashMap but with some differences, such as being synchronized, which makes it thread-safe. HashTable stores key-value pairs, where each key is unique, and the keys are not ordered.\nHere’s how to use a HashTable in Java:\n\nImport the HashTable class: To use the HashTable class in your Java code, you’ll need to import it from the java.util package:\nimport java.util.Hashtable;\nCreate a HashTable: To create a new HashTable, use the following syntax:\nHashtable&lt;String, Integer&gt; myTable = new Hashtable&lt;String, Integer&gt;();\nAdd elements: To add key-value pairs to the HashTable, use the put() method:\nmyTable.put(\"apple\", 3);\nmyTable.put(\"banana\", 5);\nmyTable.put(\"orange\", 2);\nAccess elements: To access the value associated with a key, use the get() method:\nint apples = myTable.get(\"apple\"); // 3\nint oranges = myTable.get(\"orange\"); // 2\nRemove elements: To remove a key-value pair from the HashTable, use the remove() method:\nmyTable.remove(\"banana\");\nCheck if a key exists: To check if a key is in the HashTable, use the containsKey() method:\nboolean hasApple = myTable.containsKey(\"apple\"); // true\nboolean hasGrape = myTable.containsKey(\"grape\"); // false\nIterate over keys: To iterate over the keys in a HashTable, you can use a for-each loop with the keySet() method:\nfor (String fruit : myTable.keySet()) {\n    System.out.println(fruit + \": \" + myTable.get(fruit));\n}\nIterate over values: To iterate over the values in a HashTable, you can use a for-each loop with the values() method:\nfor (Integer count : myTable.values()) {\n    System.out.println(count);\n}\nIterate over key-value pairs: To iterate over the key-value pairs in a HashTable, you can use a for-each loop with the entrySet() method:\nfor (Hashtable.Entry&lt;String, Integer&gt; entry : myTable.entrySet()) {\n    System.out.println(entry.getKey() + \": \" + entry.getValue());\n}\n\nA HashTable can be a useful data structure when you need to store key-value pairs and require thread-safe operations. However, it has some performance overhead due to synchronization, so if thread safety is not a concern, a HashMap is generally a more efficient choice."
  },
  {
    "objectID": "10_hashing.html#hashsets-in-java",
    "href": "10_hashing.html#hashsets-in-java",
    "title": "3  Hashing, Hash Tables, and Hash Maps",
    "section": "3.11 HashSets in Java",
    "text": "3.11 HashSets in Java\nA HashSet is a collection in Java that implements the Set interface and uses a hash table for storage. It does not store key-value pairs like hash tables or hash maps, but instead stores unique elements. The elements in a HashSet are not ordered, and duplicate values are not allowed.\nHere’s how to use a HashSet in Java:\n\nImport the HashSet class: To use the HashSet class in your Java code, you’ll need to import it from the java.util package:\nimport java.util.HashSet;\nCreate a HashSet: To create a new HashSet, use the following syntax:\nHashSet&lt;String&gt; mySet = new HashSet&lt;String&gt;();\nAdd elements: To add elements to the HashSet, use the add() method:\nmySet.add(\"apple\");\nmySet.add(\"banana\");\nmySet.add(\"orange\");\nRemove elements: To remove elements from the HashSet, use the remove() method:\nmySet.remove(\"banana\");\nCheck if an element exists: To check if an element is in the HashSet, use the contains() method:\nboolean hasApple = mySet.contains(\"apple\"); // true\nboolean hasGrape = mySet.contains(\"grape\"); // false\nIterate over elements: To iterate over the elements in a HashSet, you can use a for-each loop:\nfor (String fruit : mySet) {\n    System.out.println(fruit);\n}\n\nA HashSet can be a useful data structure when you need to store a collection of unique elements without any specific order. It provides constant-time performance for common operations like add, remove, and contains, making it an efficient choice for many applications."
  },
  {
    "objectID": "10_hashing.html#hascode-and-equals-in-java",
    "href": "10_hashing.html#hascode-and-equals-in-java",
    "title": "3  Hashing, Hash Tables, and Hash Maps",
    "section": "3.12 hasCode and equals in Java",
    "text": "3.12 hasCode and equals in Java\nIn Java, the hashCode method is part of the Object class, which is the superclass of all Java classes. The purpose of the hashCode method is to provide a default implementation for generating hash codes, which are integer values that represent the memory address of an object.\n\n3.12.1 The hashCode Method\nThe hashCode method has the following signature:\npublic int hashCode()\nThis method returns an integer hash code for the object on which it is called. By default, it returns a hash code that is based on the object’s memory address, but this behavior can be overridden in subclasses to provide custom hash code generation.\nA well-implemented hashCode method should follow these general rules:\n\nIf two objects are equal according to their equals() method, they must have the same hash code.\nIf two objects have the same hash code, they are not necessarily equal according to their equals() method.\nThe hash code of an object should not change over time unless the information used in the equals() method also changes.\n\n\n\n3.12.2 Overriding the hashCode Method\nWhen creating custom classes, it is important to override the hashCode method if the equals() method is also overridden. This ensures that the general contract of the hashCode method is maintained, which is essential for the correct functioning of hash-based data structures like HashSet and HashMap.\nHere’s an example of a custom Person class that overrides both the equals() and hashCode() methods:\npublic class Person {\n    private String name;\n    private int age;\n\n    // Constructor, getters, and setters\n\n    @Override\n    public boolean equals(Object obj) {\n        if (this == obj) {\n            return true;\n        }\n        if (obj == null || getClass() != obj.getClass()) {\n            return false;\n        }\n        Person person = (Person) obj;\n        return age == person.age && Objects.equals(name, person.name);\n    }\n\n    @Override\n    public int hashCode() {\n        return Objects.hash(name, age);\n    }\n}\nIn this example, the equals() method checks if two Person objects have the same name and age. The hashCode() method uses the Objects.hash() utility method, which generates a hash code based on the name and age fields.\n\n\n3.12.3 Using hashCode with Java Collections\nThe hashCode method plays a crucial role in the performance of Java’s hash-based data structures, such as HashSet, HashMap, and HashTable. These data structures rely on the hashCode method to efficiently store and retrieve objects based on their hash codes.\nWhen working with these collections, it is important to ensure that the hashCode method is correctly implemented for the objects being stored. Failing to do so can lead to poor performance or incorrect behavior.\nIn summary, the hashCode method in Java is a critical part of the Object class that provides a default implementation for generating hash codes. When creating custom classes, it is essential to override the hashCode method if the equals() method is also overridden, ensuring the correct functioning of hash-based data structures like HashSet and HashMap."
  }
]