[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ITSC 2214 - Data Structures and Algorithms - Summer 2023",
    "section": "",
    "text": "Preface\nThis website contains a set of readings for ITSC 2214 - Data Structures and Algorithms."
  },
  {
    "objectID": "01_intro_to_cs_dsa.html#computing-capabilities",
    "href": "01_intro_to_cs_dsa.html#computing-capabilities",
    "title": "1  Background on Computer Science and Data Structures",
    "section": "1.1 Computing Capabilities",
    "text": "1.1 Computing Capabilities\nComputers, in the broadest sense, are devices that can perform calculations or manipulate information. Throughout history, humans have invented and used various types of computers, each with increasing capabilities and complexity. Here are some examples of how computing capabilities have evolved over time:\n\nTally stick - One of the earliest forms of computers, dating back to prehistoric times. A tally stick is a piece of wood or bone with notches carved into it to record numbers or events. The computations it could do were incrementing and retrieving one piece of information. For example, a shepherd could use a tally stick to keep track of his sheep by making a notch for each one.\nAbacus - A manual device used for calculations by sliding counters along rods or in grooves. The abacus was invented in ancient times and is still used today in some parts of the world. It could store one set of numbers, add, subtract, multiply, divide, and perform other arithmetic operations to the stored information which can later be retrieved. For example, a merchant could use an abacus to keep track of his transactions and profits.\nAstrolabe - A sophisticated instrument used for astronomy and navigation by measuring the positions and movements of celestial bodies. The astrolabe was developed in ancient Greece and reached its peak in the Islamic Golden Age. It could perform complex calculations such as determining the time, latitude, longitude, and direction based on the observation of stars and planets. For example, a sailor could use an astrolabe to find his way across the sea by aligning it with the sun or the pole star.\nAntikythera mechanism - A mechanical device that simulated the motions of the sun, moon, and planets according to a geocentric model. The Antikythera mechanism was discovered in a shipwreck near the Greek island of Antikythera in 1901. It is estimated to date back to the 2nd century BC and is considered one of the first analog computers. It could predict astronomical phenomena such as eclipses, phases of the moon, and positions of the zodiac signs. For example, a priest could use the Antikythera mechanism to plan religious ceremonies and festivals based on the celestial calendar.\nDifference engine - A mechanical calculator that could compute polynomial functions using the method of finite differences. The difference engine was designed by Charles Babbage in the early 19th century but was never fully completed due to technical and financial difficulties. It could generate accurate tables of values for various mathematical functions such as logarithms, trigonometry, and navigation. For example, a mathematician could use the difference engine to check his calculations and avoid errors.\nAnalytical engine - A proposed mechanical computer that could perform any calculation given a set of instructions or a program. The analytical engine was also designed by Charles Babbage in the mid-19th century but was never built due to his death and lack of funding. It is considered the first general-purpose computer and the precursor of modern computers. It could store data in memory, process data using arithmetic and logical operations, control the flow of execution using conditional branching and looping, and output data using a printer or a punch card. For example, Ada Lovelace, who wrote the first algorithm for the analytical engine, envisioned that it could compose music based on mathematical rules.\n\nHere is a summary of the computing capabilities of some of these devices -\n\nTally stick: Store and retrieve one piece of data\nAbacus: Store and retrieve one piece of data, and perform basic arithmetical operations on them with another operand.\nSystem of Gears: Store and retrieve one piece of data. Each gear can use stored data and scale it up or down (multiply or divide) by a fixed constant determined by the gear ratio.​\nDifference engine: Can perform complex, but non-programmable computations. Produced tables of input-output pairs.\nAnalytical engine: Can perform complex, programmable computations. Can store data in memory, process data using arithmetic and logical operations, control the flow of execution using conditional branching and looping, and output data using a printer or a punch card."
  },
  {
    "objectID": "01_intro_to_cs_dsa.html#what-is-computer-science-then",
    "href": "01_intro_to_cs_dsa.html#what-is-computer-science-then",
    "title": "1  Background on Computer Science and Data Structures",
    "section": "1.2 What is Computer Science then?",
    "text": "1.2 What is Computer Science then?\nThe science of computers, or Computer Science, seeks to answer fundamental questions like: What are the essential parts of a computer? What can be computed, and what cannot? What determines the ease and speed of computation? This field provides the foundation for understanding the principles that drive computational systems."
  },
  {
    "objectID": "01_intro_to_cs_dsa.html#data-structures-and-algorithms",
    "href": "01_intro_to_cs_dsa.html#data-structures-and-algorithms",
    "title": "1  Background on Computer Science and Data Structures",
    "section": "1.3 Data Structures and Algorithms",
    "text": "1.3 Data Structures and Algorithms\nThe field of Data Structures and Algorithms expands upon the principles of Computer Science. It determines the efficiency of computation by answering the question: How easily or quickly can something be computed, and what factors influence these metrics? It outlines the necessary building blocks or tools required for programming, and explores the common patterns and problems in programs, offering known ways to enhance their speed. It is this branch of Computer Science that gives us the skills to design, write and analyze the efficiency of our programs. This understanding is crucial to becoming proficient in the broader field of Computer Science.\nIn essence, the intertwined journey of computers and computing science is a testament to human ingenuity and the relentless pursuit of understanding and harnessing the principles that underlie our world. As we delve deeper into the concepts of data structures and algorithms, we continue to contribute to this exciting journey."
  },
  {
    "objectID": "01_intro_to_cs_dsa.html#review",
    "href": "01_intro_to_cs_dsa.html#review",
    "title": "1  Background on Computer Science and Data Structures",
    "section": "1.4 Review",
    "text": "1.4 Review\nWe talked a little about what computers are, and what capabilities a device needs to have to be able to compute certain types of problems.​ The next chapter will be about “general purpose, programmable computers”, and how they work.\nHere’s a fun exercise for you -\n\nAre analog wristwatches computers? What do they compute?\nCan you design a system of gears that can convert Fahrenheit to Celcius?"
  },
  {
    "objectID": "02_general_purpose_computers.html#the-working-of-a-processor",
    "href": "02_general_purpose_computers.html#the-working-of-a-processor",
    "title": "2  Modern General-Purpose, Programmable Computers",
    "section": "2.1 The Working of a Processor",
    "text": "2.1 The Working of a Processor\nThe way Cory and Jamal communicate parallels how a computer’s processor operates. A processor has a set of operations it can perform - like copying, moving, adding, subtracting data, jumping to another instruction, conditional branching, and more.\nTo tell a computer to perform a certain operation, we need several indicators, just like Cory and Jamal’s system. Computers use tiny wires that may or may not have electricity running through them. The presence or absence of electricity indicates what the processor should do.\nWe refer to the absence as a 0 and the presence of electricity as a 1. Each wire conveying either a 0 or a 1 is said to convey one bit of information.\nThis is a fixed group of wires going into the processor to convey such information on what needs to be done. This grouping is known as “Instruction.” The number of bits in an instruction tells us the `size`` of the instruction.\nA processor usually has a fixed instruction size (64 for a 64-bit processor, 32 for a 32-bit processor, etc.).\nSometimes even more information is needed. For example, for operations like jumping to a different instruction, the processor needs to know where to jump to. For operations like addition, two numbers are needed. Sometimes, along with the operation, the processor also needs to be instructed about where to store the result. We call these pieces of data on which we perform instructions as operands. We often refer to instructions as operations.\nWe cannot have an extra set of wires for each time we need to share another operand with the processor, so we share this data in sequence.\nSay 0010 1010 is the code for the add operation, 0000 0000`` is the number 0, and0000 0001` is the number 1. If we want an 8-bit processor to add these together, we might share the following data through the wires -\n...\n0000 0001 // third cycle - second operand\n0000 0000 // second cycle - first operand\n0010 1010 // first cycle - add operation\nThere is a “clock” mechanism within the processor to signal the start and end of “cycles”. Generally the processor “pipelines” its operation such that it can complete one instruction in each cycle. However, many complex instructions like division or square roots take &gt;20 cycles to complete.\nA computer processor can be compared to a complex Rube Goldberg machine, with wires that take in the presence or absence of electricity similar to how many Rube Goldberg machines’ mechanism is kicked off by a rolling ball or marbles. The “marbles” of electricity roll through the processor, along each wire, triggering complex chain reactions or side effects that are designed to elicit the intended operation. Consider that a processor, as small as it is, frequently contains billions of transistors - use this information to imagine how complex these “Rube Goldberg machines” are."
  },
  {
    "objectID": "02_general_purpose_computers.html#memory",
    "href": "02_general_purpose_computers.html#memory",
    "title": "2  Modern General-Purpose, Programmable Computers",
    "section": "2.2 Memory",
    "text": "2.2 Memory\nLet’s compare the computer’s memory to a vast grid filled with tiny cells. These cells can each store some electricity - and again the presence or absence of electricity conveys a bit of information. If we continue the previous analogy, it can be compared to a large grid storing balls (data).\nThe processor has instructions that allow it to select which cells to “read” and transfer those bits to the processor - to be interpreted either as data or instructions. The processor also has instructions that allow it to write to memory.\nIt is this interaction of a processor being able to “write” instructions and data to memory, and then conditionally “read” and execute them, and then write new instructions or data that makes a processor “programmable”.\nWhat we describe in this section is Active memory or RAM. It is the data in RAM that is readily available for the processor to read and write."
  },
  {
    "objectID": "02_general_purpose_computers.html#programs",
    "href": "02_general_purpose_computers.html#programs",
    "title": "2  Modern General-Purpose, Programmable Computers",
    "section": "2.3 Programs",
    "text": "2.3 Programs\nPrograms are essentially detailed sets of instructions that command a computer to execute specific operations. They can be likened to a recipe that a computer follows to achieve a particular task. Programs dictate what steps the computer must take and in what sequence to reach the desired outcome.\nThe program is stored on disk in a format that is standard for that operating system, and where the first instruction is stored in this format is always known - say, the first instruction in a “main” section. When a program is executed, it is loaded from the computer’s storage into RAM, and then the processor reads and executes the first instruction; and so the execution begins.\nThe complexity of programs can vary greatly, from a simple one that performs basic arithmetic to an intricate operating system like Windows or Linux that manages every aspect of a computer. However, the core characteristic of all programs is the same: they are sequences of instructions that the computer follows."
  },
  {
    "objectID": "02_general_purpose_computers.html#the-java-compiler",
    "href": "02_general_purpose_computers.html#the-java-compiler",
    "title": "2  Modern General-Purpose, Programmable Computers",
    "section": "2.4 The Java Compiler",
    "text": "2.4 The Java Compiler\nAs instructions are hard to write directly, we make use of programs called compilers that take in “code” in human-readable text format and output a list of instructions. The program is designed in such a way that the produced list of valid instructions always carries out the task described in “code” faithfully.\nThe Java compiler is such a program for Java code. When a Java program is compiled, the compiler reviews the code for syntax errors and then translates it into bytecode, a type of intermediate language closer to machine language. The Java Virtual Machine (JVM) then interprets this bytecode into machine code that your computer’s processor can execute.\nSyntax errors or other kinds of errors essentially refer to situations where the compiler doesn’t know how to, or cannot produce a valid set of instructions that can carry out what the code is describing.\nCompilers like the Java compiler don’t just translate code. They also optimize it, making it more efficient so that the resulting program runs faster and consumes less memory. For example, they may look at your for loop that is adding up the first N natural numbers and decide to replace the loop with the formula for this computation instead."
  },
  {
    "objectID": "02_general_purpose_computers.html#operating-systems",
    "href": "02_general_purpose_computers.html#operating-systems",
    "title": "2  Modern General-Purpose, Programmable Computers",
    "section": "2.5 Operating Systems",
    "text": "2.5 Operating Systems\nWith a sound understanding of the fundamental components of modern computing, it’s important to highlight the role of the operating system.\nAn operating system (OS) is a type of system software that manages computer hardware and software resources and provides various services for computer programs. It acts as a mediator between users and the computer hardware. Users interact with the operating system through user interfaces such as a command-line interface (CLI) or a graphical user interface (GUI).\nOperating systems bear the responsibility of managing the computer’s resources, including the processor, memory, disk space, and input/output devices. They coordinate tasks, ensuring that the processor’s time is used judiciously, and manage the memory, keeping track of which parts are in use and which are available.\nIn essence, the operating system provides the platform on which all other software runs. It is the environment in which programs, written in languages like Java and then compiled, operate.\nThis fundamental understanding of modern computing components helps elucidate the intricate operations that are continuously happening within our laptops, desktops, and even our smartphones. These fundamental aspects form the backbone of the digital age we live in."
  },
  {
    "objectID": "03_algorithmic_analysis.html#problems-algorithms-and-programs",
    "href": "03_algorithmic_analysis.html#problems-algorithms-and-programs",
    "title": "3  Algorithmic Analysis",
    "section": "3.1 Problems, Algorithms, and Programs",
    "text": "3.1 Problems, Algorithms, and Programs\nBefore understanding algorithmic analysis, it’s essential to differentiate between problems, algorithms, and computer programs. These are three distinct concepts that are interrelated.\n\n3.1.1 Problems\nA problem in computer science refers to a specific task that needs to be solved. It can be thought of in terms of inputs and matching outputs. For instance, to solve the problem of finding the youngest student in our class, the input would be the names and ages of all students in the class. The output would be the name of the youngest student.\nIt’s helpful to perceive problems as functions in a mathematical sense. In mathematics, a function is a relationship or correspondence between two sets — the input set (domain) and the output set (range).\n\n\n3.1.2 Algorithms\nAn algorithm, on the other hand, is a method or a process followed to solve a problem. If we perceive the problem as a function, then an algorithm can be seen as an implementation of this function that transforms an input into the corresponding output.\nSince there are typically numerous ways to solve a problem, there could be many different algorithms for the same problem. Having multiple solutions is advantageous because a specific solution might be more efficient than others for certain variations of the problem or specific types of inputs.\nFor instance, one sorting algorithm might be best suited for sorting a small collection of integers. Another might excel in sorting a large collection of integers, while a third might be ideal for sorting a collection of variable-length strings.\nBy definition, a sequence of steps can only be called an algorithm if it fulfills the following properties:\n\nIt must be correct.\nIt consists of a series of concrete steps.\nThere is no ambiguity about the step to be performed next.\nIt must comprise a finite number of steps.\nIt must terminate.\n\n\n\n3.1.3 Programs and Their Building Blocks\nBefore discussing programs in detail, let’s briefly review two essential components that make a program run: the CPU (Central Processing Unit) and memory.\nA CPU is the electronic circuitry within a computer that has the ability to execute certain instructions. Its primary function is to fetch, decode, and execute instructions. It has slots to store data, referred to as registers. Communicating with a CPU involves telling it what operation you want to perform and on which data. For instance, you might instruct the CPU to add two numbers stored in registers A and B and store the result in register C.\nCPU instructions are binary codes that specify which operation the CPU should perform. Here’s an example of what they look like:\n10010011001100111110000111011111\nSome bits in the instruction form the opcode, the operation code. The opcode is a unique identifier for an operation, like adding integers. Other bits form the operand(s), the data on which to operate. The operand can be where the data is stored (the name of a register or an address in memory), or where to store the result of the operation (again, the name of a register or an address in memory).\nFor human readability, there are notations to represent these binary instructions. Here is an example of a set of instructions in a human-readable form:\n.global main\nmain:\naddi   sp, sp, -16\nsd     t0, 0(sp)\nsd     t1, 8(sp)\ncall   some_function\nld     t0, 0(sp)\nld     t1, 8(sp)\n# Use t0 and t1 here as if nothing happened.\naddi   sp, sp, 16\nPrograms are structured into sections. They include code sections, which contain a list of instructions, and data sections, which hold binary data such as text, images, or numbers that the program needs to use. Typically, there is a designated “main” section that contains the instructions to be executed first.\nWhen you initiate an executable (with the exception of Mac “applications”), the binary data (“bits”) are read from the hard disk and transferred to the main memory (RAM). The execution of the program begins when the first instruction from the “main” section is transferred to the CPU.\nIn this context, a computer program’s code can be seen as an instance, or concrete representation, of an algorithm. Although the terms “algorithm” and “program” are distinct, they are often used interchangeably for simplification."
  },
  {
    "objectID": "03_algorithmic_analysis.html#comparing-the-performance-of-programs",
    "href": "03_algorithmic_analysis.html#comparing-the-performance-of-programs",
    "title": "3  Algorithmic Analysis",
    "section": "3.2 Comparing the Performance of Programs",
    "text": "3.2 Comparing the Performance of Programs\nWhen you compile or build a program, its code is converted into a series of instructions and data in memory. However, the execution time of the same program can vary across different machines due to differences in the processor’s capabilities.\nAs each machine can potentially have a different processor, comparing the speed of programs can be a complex task, and is only meaningful when the processor is the same or standardized. Even when the processor is standardized, many factors affect performance -\n\nBackground tasks on one machine can interfere with performance measurements.\nEven if you have the exact same processor, differences in manufacturing mean each can run at a different clock frequency.\nSmall differences in ambient temperature affect how high a processor can clock.\nThe mounting pressure of a cooler can affect heat transfer and in turn how high a processor can clock.\nMany cooling solutions involve vapor chambers. The orientation of vapor chambers can affect heat transfer and in turn how high a processor can clock.\nEven cosmic radiation can affect processors and memory.\n\nTherefore, we usually prefer to compare algorithms instead. The methods of comparing algorithms will be discussed in the following sections of this chapter."
  },
  {
    "objectID": "03_algorithmic_analysis.html#analyzing-algorithms",
    "href": "03_algorithmic_analysis.html#analyzing-algorithms",
    "title": "3  Algorithmic Analysis",
    "section": "3.3 Analyzing Algorithms",
    "text": "3.3 Analyzing Algorithms\nOne of the key components of this course is to provide a framework for predicting the performance of algorithms just by inspecting their structure. Let’s dive into the process of analyzing an algorithm’s time complexity.\n\n3.3.1 Predicting Execution Time\nConsider a simple method that adds two numbers:\npublic int add(int lhs, int rhs)\nSuppose calling add(2, 4) takes 1 second. How long would add(40, 50) or add(343245634, 32432423) take? As you might expect, all these operations, despite the difference in magnitude of the numbers involved, take approximately the same time. That’s because the time complexity of an addition operation does not depend on the values of the numbers but on the number of operations involved, which, in this case, is a single addition.\n\n\n3.3.2 Impact of Input Size\nNow, let’s examine a slightly more complex method that sums up a list of numbers:\npublic int sumOfList(List&lt;int&gt; l)\nAssuming that adding two numbers takes one second, how long would summing a list of 10 numbers take? We can infer that the time taken by sumOfList depends on the size of the list we provide. For instance, summing up a list of 10 numbers would take about half the time needed to sum up a list of 20 numbers.\nHere’s an implementation of sumOfList:\nimport java.util.ArrayList;\n\nclass Square {\n  static int sumOfList(ArrayList&lt;Integer&gt; l) {\n    int sum = 0;\n    for (int i : l) {\n      sum += i;\n    }\n    return sum;\n  }\n}\nThe key insight is that the execution time of this method depends on the number of elements in the list - which is the size of the input. The time complexity is directly proportional to the number of times the addition statement is executed, which is equal to the size of the list.\n\n\n3.3.3 Iterations and Input Size\nLet’s take it a step further. If you’re summing a list of N items, each of which is another list of M items, the operation would take N * M addition statements. Here, the time complexity is a function of both N and M.\n\n\n3.3.4 Gauging Relative Execution Time\nThe crux of analyzing an algorithm’s performance lies in understanding how many times statements in the program run as a function of the size of the input. This approach enables us to estimate how long two invocations of the same method will take relative to each other, given the size of the input for each.\nUnderstanding this concept will allow you to better predict the performance of algorithms, which is a crucial skill in efficient programming and system design."
  },
  {
    "objectID": "03_algorithmic_analysis.html#algorithm-complexities",
    "href": "03_algorithmic_analysis.html#algorithm-complexities",
    "title": "3  Algorithmic Analysis",
    "section": "3.4 Algorithm Complexities",
    "text": "3.4 Algorithm Complexities\nIn order to evaluate an algorithm’s efficiency, we analyze its time complexity and space complexity, both of which describe how the algorithm’s performance scales with the size of the input.\n\n3.4.1 Time Complexity\nTime complexity measures how the execution time of an algorithm increases with the size of the input. For instance, counting how many times the number \\(5\\) appears in a list requires checking each number in the list. Hence, the time complexity is directly proportional to the size of the list.\n\n\n3.4.2 Space Complexity\nSpace complexity quantifies the amount of memory an algorithm requires relative to the size of the input. Using the previous example, we would need enough space to store the list and an additional space to store the counter. Thus, the space complexity is proportional to the size of the list, or more precisely, \\(N + 1\\).\n\n\n3.4.3 Conditional Statements and Complexity\nIn cases where conditional statements are present, the number of executed statements depends on which branch the program takes. One branch may contain more statements than the other. To handle such scenarios, we introduce the concept of Big-O, Big-Ω, and Big-Θ notations.\n\n3.4.3.1 Big-O Notation\nThe Big-O notation describes the worst-case time complexity of an algorithm, essentially providing an upper bound on the time taken. This notation considers the scenario where the program consistently takes the path with the most statements. While Big-O is commonly used for time complexity, it can also describe space complexity.\nThe notation comprises two parts: the function itself and the variable representing the input size. Generally, ‘n’ is used to represent the input size, and constants and coefficients are typically ignored. For instance, the following functions are all \\(O(n)\\):\n\n\\(n + 1\\)\n\\(2n\\)\n\\(103n + 124\\)\nOnly the highest degree of ‘n’ is considered when determining Big-O notation. Therefore, functions such as n², n/2, or √n are not considered O(n).\n\n\n\n3.4.3.2 Big-Ω Notation\nThe Big-Ω notation represents the best-case complexity of an algorithm. It follows the same format as Big-O notation but focuses on the scenario where the program consistently takes the path with the fewest statements.\n\n\n3.4.3.3 Big-Θ Notation\nThe Big-Θ notation is used to denote the average-case complexity of an algorithm. It again follows the same structure as Big-O, but it considers both the best and worst-case scenarios to provide an average estimate of the algorithm’s performance.\nRemember, these notations and complexities are pivotal in estimating the performance of an algorithm based on the size or other properties of the input, correlating to the number of steps taken by the algorithm. This understanding is crucial when designing efficient and effective solutions in computer science."
  },
  {
    "objectID": "03_algorithmic_analysis.html#common-big-o-complexities",
    "href": "03_algorithmic_analysis.html#common-big-o-complexities",
    "title": "3  Algorithmic Analysis",
    "section": "3.5 Common Big-O Complexities",
    "text": "3.5 Common Big-O Complexities\nBig-O notation is a way of expressing the worst-case time complexity of an algorithm. It describes how the running time of an algorithm changes as the size of its input grows. The most common Big-O complexities are:\n\n3.5.1 Constant Complexity and Growth\nConstant complexity, often represented as \\(O(1)\\), occurs when the running time of an algorithm or the amount of work needed does not change with the size of the input (\\(N\\)). This means the algorithm takes a fixed amount of time, regardless of how many elements it is processing.\nFor example, accessing an element in an array by its index is an operation of constant complexity. This is because it takes roughly the same amount of time, regardless of the size of the array:\n\n\\(f(n) = 1\\) for all \\(n\\)\n\nIn this case, no matter how large or small our input size is, the amount of work we have to do remains the same. This is the most efficient complexity an algorithm can have.\n\n\n3.5.2 Linear Complexity and Growth\nLinear complexity, often represented as \\(O(n)\\), occurs when the running time of an algorithm or the amount of work needed scales proportionally with the size of the input (\\(N\\)). For every additional element in the input, a fixed amount of work is added.\nFor example, finding an element in an unsorted list is an operation of linear complexity, as the algorithm might need to look at every element once:\n\n\\(f(n) = n\\)\n\nIn this case, if we add one more element to our input size, we add one more unit of work. This is because every additional element requires the same amount of work.\n\n\n3.5.3 Quadratic Complexity and Growth\nQuadratic complexity, often represented as \\(O(n^2)\\), occurs when the running time of an algorithm or the amount of work needed scales with the square of the size of the input (\\(N\\)). For every additional element in the input, the work increases by a factor of \\(n\\).\nFor example, a simple nested loop for comparing pairs of elements in a list has quadratic complexity. This is because each element is compared to every other element:\n\n\\(f(n) = n^2\\)\n\nIn this case, if we add one more element to our input size, we add \\(n\\) units of work, as we have to compare this new element with every other element already in the list.\n\n\n3.5.4 Exponential Functions and Growth\nAn exponential function is one that includes a variable in the exponent. To illustrate, the function \\(2^n\\) is an exponential function. Here, with each unit increase in the input, the output is multiplied (or scaled up) by a factor of 2. For instance:\n\n\\(f(1) = 2^1 = 2\\)\n\\(f(2) = 2^2 = 4\\)\n\\(f(n+1) = f(n) * 2\\)\n\nExponential growth is characterized by a constant factor scaling up the running time of the algorithm, or the amount of work needed, for each unit increase in the size of the input (often denoted as \\(N\\)). This constant factor is the base of the exponent. This means that for an algorithm with a time complexity of \\(2^N\\), adding one more element to the input could potentially double the amount of work required. Examples of algorithms exhibiting exponential growth include certain solutions to problems like the Towers of Hanoi and calculations of the Fibonacci sequence.\n\n\n3.5.5 Logarithmic Functions and Growth\nLogarithmic functions serve as the inverse of exponential functions. For example, \\(log_2(n)\\) is a logarithmic function. Here, the output is decremented by 1 each time the input is divided (or scaled down) by a factor of 2. For instance:\n\n\\(f(16) = log_2(16) = 4\\)\n\\(f(8) = log_2(8) = 3\\)\n\\(f(n/2) = f(n) - 1\\)\n\nLogarithmic growth is characterized by a decrement of 1 in the running time of the algorithm or the amount of work needed, each time the size of the input (denoted as \\(N\\)) is divided by a constant factor. This constant factor is the base of the logarithm. For an algorithm with a time complexity of \\(log_2(n)\\), if we have an input size of 16, doubling the input size will only increase the amount of work by 1 unit. Similarly, halving the input size will decrease the work by 1 unit.\nAlgorithms often display complexities such as \\(N*log_2(N)\\), which represents a combination of linear and logarithmic growth. Examples of such algorithms that include logarithmic growth are binary search and certain sorting algorithms.\n\n\n3.5.6 Examples\nLet’s look at some examples of algorithms and their Big-O complexities.\nstatic int findMin(x, y) {\n  if (x &lt; y) {\n    return x;\n  } else {\n    return y;\n  }\n}\nThis algorithm finds the minimum of two numbers x and y. It does not depend on the input size, since it only performs one comparison and one return statement. Therefore, its worst-case complexity is \\(O(1)\\). Its best case and average case are also \\(O(1)\\) since they are the same as the worst case.\nstatic int linearSearch(numbers[], target)\n  for (int i = 0; i &lt; numbers.length; i++) {\n    if (numbers[i] == target) {\n      return i;\n    }\n  }\n  return -1;\n}\nThis algorithm performs a linear search on an array of numbers to find a target value. It iterates through each element of the array until it finds the target or reaches the end of the array. In the worst case, it has to check every element of the array, which means its worst-case complexity is \\(O(n)\\), where n is the length of the array. In the best case, it finds the target in the first element, which means its best-case complexity is \\(O(1)\\). In the average case, it finds the target somewhere in the middle of the array, which means its average-case complexity is also \\(O(n)\\).\nWe will talk about more examples of other common worst-case complexities throughout this course."
  },
  {
    "objectID": "03_algorithmic_analysis.html#growth-rate",
    "href": "03_algorithmic_analysis.html#growth-rate",
    "title": "3  Algorithmic Analysis",
    "section": "3.6 Growth Rate",
    "text": "3.6 Growth Rate\nThe growth rate for an algorithm is the rate at which the cost of the algorithm grows as the size of its input grows. The cost can be measured in terms of time, space, or other resources. The worst-case complexity notation essentially denotes the growth rate of the time complexity with respect to the size of a worst-case input.\nThe table below summarizes how different Big-O complexities compare in terms of their growth rates.\n\n\n\nComplexity\nGrowth Rate\n\n\n\n\nO(1)\nConstant\n\n\nO(\\(log(n)\\))\nLogarithmic\n\n\nO(n)\nLinear\n\n\nO(n\\(^2\\))\nQuadratic\n\n\nO(\\(2^n\\))\nExponential\n\n\n\nAs we can see, constant and logarithmic complexities have very low growth rates, meaning that they are very efficient and scalable algorithms. Linear complexity has a moderate growth rate, meaning that it can handle reasonably large inputs but may become slow for very large inputs. Quadratic and exponential complexities have very high growth rates, meaning that they are very inefficient and unscalable algorithms that can only handle small inputs."
  },
  {
    "objectID": "03_algorithmic_analysis.html#summary-review",
    "href": "03_algorithmic_analysis.html#summary-review",
    "title": "3  Algorithmic Analysis",
    "section": "3.7 Summary / Review",
    "text": "3.7 Summary / Review\nIn this section, we learned how to analyze the performance of algorithms using Big-O notation. We saw that measuring the actual running time of an algorithm on real hardware is difficult and impractical because it depends on many factors such as the hardware specifications, the programming language, the compiler, the input size, and the input distribution. Therefore, we need a way to simplify and standardize the performance analysis across different hardware and software platforms.\nWe learned that one way to simplify the performance analysis is to count the number of steps or instructions that an algorithm needs to execute before finishing. We saw that these steps are analogous to the instructions generated by a compiler when it translates our code into machine code. We also learned that different steps may have different costs depending on their complexity, but we can ignore these differences for simplicity and focus on the overall number of steps.\nWe learned that Big-O notation is a mathematical tool that allows us to express the worst-case complexity of an algorithm. It describes how the number of steps grows as a function of the input size in the worst possible scenario. It gives us an upper bound on the performance of an algorithm, meaning that it tells us how slow an algorithm can get in the worst case. We also learned that Big-O notation ignores constant factors and lower-order terms because they become insignificant as the input size grows.\nWe learned about some common Big-O complexities and their growth rates, such as constant, logarithmic, linear, quadratic, and exponential. We saw some examples of algorithms and their Big-O complexities, and how to analyze them using simple rules such as adding complexities for sequential steps, multiplying complexities for nested steps, and taking the maximum complexity for conditional steps.\nWe learned that Big-O notation helps us compare different algorithms and choose the most efficient one for a given problem. It also helps us estimate how well an algorithm can scale to larger inputs and how it can affect the performance of our applications."
  },
  {
    "objectID": "04_unit_testing.html#background",
    "href": "04_unit_testing.html#background",
    "title": "4  Unit Testing and Test-Driven Development",
    "section": "4.1 Background",
    "text": "4.1 Background\nImagine you are an architect and you’ve just designed a large, complex building - let’s say a skyscraper. This skyscraper is not merely a single entity; it’s composed of thousands of individual components - the plumbing, the electrical wiring, the elevators, the heating system, and the building’s structural elements, among many others. Now, how would you ensure that the skyscraper works as intended? Would you wait until the entire building is constructed and then start testing every possible scenario? Obviously, this approach is time-consuming, and it exposes you to significant risk.\nA similar challenge exists in the world of software development. Take, for example, a web browser. This is a complex piece of software with hundreds of classes interacting in intricate ways. These classes and methods perform various tasks such as rendering HTML and CSS, processing JavaScript, managing cookies, implementing security features, and many others. Ensuring the correct functionality of this software is a daunting task, given the vast range of potential inputs. After all, there are billions of web pages on the internet, each with its unique combination of technologies, designs, and user interactions. How can you guarantee that your browser works flawlessly with all of them? A naive approach would be to load each web page and observe the output, but this process is not only time-consuming but also practically impossible.\nThis conundrum begs the question: How can we validate the correct functionality of a software product efficiently? The direction points towards automation - the ability to conduct tests without manual intervention. But how can we achieve this, especially given the enormous application surface area?\nThis is where unit testing and Test-Driven Development (TDD) come in."
  },
  {
    "objectID": "04_unit_testing.html#the-power-of-unit-testing",
    "href": "04_unit_testing.html#the-power-of-unit-testing",
    "title": "4  Unit Testing and Test-Driven Development",
    "section": "4.2 The Power of Unit Testing",
    "text": "4.2 The Power of Unit Testing\nWhile the surface area of an entire application is vast, the surface area of individual classes and units of code within the software project is significantly smaller. If we can write tests to verify that each method within each class functions correctly for all possible inputs, we reduce the complexity of the problem.\nAt first glance, it might seem like an overwhelming task. Even a moderately complex software project can have thousands of methods spread across hundreds of classes. Writing tests for all of them could result in thousands of test cases. But this is precisely where automation proves its worth. By automating these tests, we can execute them each time we modify our code, ensuring the functionality remains intact. This method gives us confidence that our changes have not inadvertently introduced bugs into existing functionality.\nThe key principle here is that by ensuring each individual unit of our software behaves correctly, we can be reasonably confident that the application as a whole operates as expected, provided the software architecture is sound. In this manner, unit testing allows us to break down the monumental task of verifying a complex software system’s functionality into manageable, automated tasks.\nIn the following sections, we will delve deeper into the concept of unit testing, its implementation in Java, and the practice of Test-Driven Development, where tests actually guide and shape the development of the software. Buckle up, for we’re about to embark on an exciting journey that will fundamentally change how you approach software development!"
  },
  {
    "objectID": "04_unit_testing.html#a-basic-approach-to-unit-testing",
    "href": "04_unit_testing.html#a-basic-approach-to-unit-testing",
    "title": "4  Unit Testing and Test-Driven Development",
    "section": "4.3 A Basic Approach to Unit Testing",
    "text": "4.3 A Basic Approach to Unit Testing\nIn order to illustrate the process of unit testing in Java, let’s consider a simple utility class named MathUtil. This class defines basic arithmetic operations such as add, subtract, etc.\npublic class MathUtil {\n    public int add(int a, int b) {\n        return a + b;\n    }\n    \n    // More methods for subtract, multiply, etc.\n}\nAs we discussed in the previous section, to ensure our MathUtil functions correctly, we associate it with a MathUtilTest class. This class contains multiple test methods, each designed to verify a different scenario of the operations provided by MathUtil.\npublic class MathUtilTest {\n\n    public boolean testAdd1() {\n        MathUtil m = new MathUtil();\n        int lhs = 5;\n        int rhs = 7;\n\n        if (m.add(lhs, rhs) == lhs + rhs) {\n            return true;\n        } else {\n            return false;\n        }\n    }\n    \n    // More test methods for other cases...\n}\nIn the above example, the testAdd1 method tests the addition of two positive numbers. We could also add methods like testAdd2 to test adding a positive and a negative number, testAdd3 to test adding two negative numbers, and so forth. Each of these methods tests a specific scenario and validates that the result is as expected."
  },
  {
    "objectID": "04_unit_testing.html#recognizing-the-inefficiencies-and-redundancies",
    "href": "04_unit_testing.html#recognizing-the-inefficiencies-and-redundancies",
    "title": "4  Unit Testing and Test-Driven Development",
    "section": "4.4 Recognizing the Inefficiencies and Redundancies",
    "text": "4.4 Recognizing the Inefficiencies and Redundancies\nWhile the above approach accomplishes our objective of validating the methods in our MathUtil class, you might have already noticed that it’s far from optimal. There are several glaring issues that can make this method tedious and inefficient:\n\n4.4.1 Redundancy\nEvery test method follows a similar pattern - we perform an operation and then verify if the result matches the expected outcome. This redundancy suggests we could abstract out the verification part into a separate method.\nIn the following expanded MathUtilTest class, you can observe that testAdd2 and testAdd3 follow the exact same pattern as testAdd1. They create an instance of MathUtil, perform an operation, and then compare the result with the expected outcome. This repetitive pattern across multiple tests highlights the redundancy and inefficiency of this approach.\npublic class MathUtilTest {\n\n    public boolean testAdd1() {\n        MathUtil m = new MathUtil();\n        int lhs = 5;\n        int rhs = 7;\n\n        if (m.add(lhs, rhs) == lhs + rhs) {\n            return true;\n        } else {\n            return false;\n        }\n    }\n    \n    public boolean testAdd2() {\n        MathUtil m = new MathUtil();\n        int lhs = -5;\n        int rhs = 7;\n\n        if (m.add(lhs, rhs) == lhs + rhs) {\n            return true;\n        } else {\n            return false;\n        }\n    }\n\n    public boolean testAdd3() {\n        MathUtil m = new MathUtil();\n        int lhs = -5;\n        int rhs = -7;\n\n        if (m.add(lhs, rhs) == lhs + rhs) {\n            return true;\n        } else {\n            return false;\n        }\n    }\n    \n    // More test methods for other cases...\n}\n\n\n4.4.2 Lack of Automation\nLet’s see how we need to currently run the tests we’ve written.\npublic class MathUtilTest {\n\n    // testAdd1, testAdd2, testAdd3, etc. test methods...\n\n    public static void main(String[] args) {\n        MathUtilTest test = new MathUtilTest();\n\n        System.out.println(\"testAdd1 result: \" + (test.testAdd1() ? \"PASS\" : \"FAIL\"));\n        System.out.println(\"testAdd2 result: \" + (test.testAdd2() ? \"PASS\" : \"FAIL\"));\n        System.out.println(\"testAdd3 result: \" + (test.testAdd3() ? \"PASS\" : \"FAIL\"));\n\n        // add more prints for other test cases\n    }\n}\nWith this main method, you can now run the MathUtilTest class, and it will execute each of the testAdd methods and print whether each test passed or failed. This method is a basic way to manually execute the tests and check their results.\nCurrently, we need to call each test method manually to run our tests. An automated system that could execute all tests for us would save time and reduce the chances of human error. However, as we will see later, there are better approaches to automation than the one we’ve used here."
  },
  {
    "objectID": "04_unit_testing.html#a-slightly-more-sophisticated-approach-to-unit-testing",
    "href": "04_unit_testing.html#a-slightly-more-sophisticated-approach-to-unit-testing",
    "title": "4  Unit Testing and Test-Driven Development",
    "section": "4.5 A slightly more sophisticated approach to Unit Testing",
    "text": "4.5 A slightly more sophisticated approach to Unit Testing\nTo alleviate the redundancy, we can create a method that compares the expected and actual results and raises an error if they do not match. This method, which we can call assertEquals, would look something like this:\npublic static void assertEquals(String testCaseName, int expected, int actual) {\n    if (expected != actual) {\n        System.out.println(testCaseName + \" result: FAIL\");\n    } else {\n        System.out.println(testCaseName + \" result: PASS\");\n    }\n}\nThen we can simplify our test methods by using assertEquals:\npublic void testAdd() {\n    MathUtil m = new MathUtil();\n\n    assertEquals(\"testAddTwoPositive\", m.add(5, 7), 13);\n    assertEquals(\"testAddTwoNegative\", m.add(-5, -7), -13);\n    assertEquals(\"testAddNegPos\", m.add(-5, 7), 2);\n}\nNow, our test case looks cleaner and easier to understand. The assertEquals method abstracts away the comparison details, leaving only the test logic in the test case. We can apply this simplification to all our test methods.\nThis approach significantly reduces the redundancy in our test code, making it easier to write and maintain our tests. However, we are still manually running each test method from the main method. What if we could also automate the execution of all test methods?"
  },
  {
    "objectID": "04_unit_testing.html#automating-test-execution",
    "href": "04_unit_testing.html#automating-test-execution",
    "title": "4  Unit Testing and Test-Driven Development",
    "section": "4.6 Automating Test Execution",
    "text": "4.6 Automating Test Execution\nWhat if we could just call a single method that runs all our test methods? Let’s define a simple runTests method that does exactly that.\npublic void runTests() {\n    testAdd();\n    // Call all other test methods here...\n}\nAnd then you can simply call the runTests method to execute all your tests:\npublic static void main(String[] args) {\n    MathUtilTest test = new MathUtilTest();\n    test.runTests();\n}\nThis approach is an improvement over manually running each test. However, it still has some drawbacks. For instance, when you add a new test method, you need to remember to add a call to this method in the runTests method. It would be better if our test framework could automatically detect and run all test methods without requiring any modifications to the runTests method. As we’ll see later, this is precisely what test frameworks like JUnit offer."
  },
  {
    "objectID": "04_unit_testing.html#summary",
    "href": "04_unit_testing.html#summary",
    "title": "4  Unit Testing and Test-Driven Development",
    "section": "4.7 Summary",
    "text": "4.7 Summary\nSo far, we have seen how unit testing can be a powerful tool in ensuring that individual units of code within a larger software application function as expected. We have also discussed and implemented a basic system for automating unit tests in Java, gradually refining this system to make it more efficient and less redundant.\nIn the following sections, we will discuss JUnit, a popular unit testing framework in Java that takes automation and convenience to the next level. We will also explore the practice of Test-Driven Development, where we let our tests guide the development of our software, helping us to write cleaner, more robust code. Stay tuned!"
  },
  {
    "objectID": "05_writing_junit_tests.html#introduction-to-junit",
    "href": "05_writing_junit_tests.html#introduction-to-junit",
    "title": "5  Writing JUnit Tests and Test-Driven Development",
    "section": "5.1 Introduction to JUnit",
    "text": "5.1 Introduction to JUnit\nJUnit is a widely used testing framework in the Java world. It automates the process of running tests and provides us with a wide range of assertion methods to validate our code. JUnit helps to simplify our test code, making it easier to read and maintain.\nSo, why is JUnit so popular?\n\nSimplicity: JUnit simplifies the process of writing and running tests. The framework handles the boilerplate code, allowing us to focus solely on writing the test cases.\nAssertion Library: JUnit provides a comprehensive set of assertion methods that help us validate our code against a wide range of conditions.\nAnnotations: JUnit uses annotations to define test methods and setup methods, making our test code easier to read and understand.\nAutomatic Test Discovery: JUnit automatically finds and runs all test methods, so we don’t have to manually list them in our code.\nIDE Integration: Most modern IDEs provide first-class support for JUnit, including features such as generating test cases and displaying test results in a friendly format.\n\nNow that you understand what JUnit is and why it’s beneficial let’s see how to use it in our MathUtil class."
  },
  {
    "objectID": "05_writing_junit_tests.html#useful-junit-assertions",
    "href": "05_writing_junit_tests.html#useful-junit-assertions",
    "title": "5  Writing JUnit Tests and Test-Driven Development",
    "section": "5.2 Useful JUnit Assertions",
    "text": "5.2 Useful JUnit Assertions\nJUnit provides a set of methods called assertions that are used to test the expected output of your code. These assertions help verify that your code behaves as expected under different conditions.\nLet’s take a look at some commonly used assertions:\n\n5.2.1 assertEquals\nThis assertion checks if two values are equal:\nassertEquals(expected, actual);\nIf actual is not equal to expected, the assertion fails, and the test method will terminate immediately.\nLet’s rewrite our addTest1 method using JUnit’s assertEquals:\n@Test\npublic void addTest1() {\n    MathUtil m = new MathUtil();\n    int lhs = 5;\n    int rhs = 7;\n\n    assertEquals(lhs + rhs, m.add(lhs, rhs));\n}\n\n\n5.2.2 assertTrue and assertFalse\nThese assertions verify if a condition is true or false, respectively:\nassertTrue(condition);\nassertFalse(condition);\nIf the condition does not meet the expectation (i.e., true for assertTrue and false for assertFalse), the assertion fails, and the test method will terminate immediately.\n\n\n5.2.3 assertNotNull and assertNull\nThese assertions check if an object is null or not:\nassertNotNull(object);\nassertNull(object);\nIf the object does not meet the expectation (i.e., not null for assertNotNull and null for assertNull), the assertion fails, and the test method will terminate immediately.\nThese are just a few examples. JUnit provides a comprehensive set of assertions to cover almost any condition you might want to verify."
  },
  {
    "objectID": "05_writing_junit_tests.html#setting-up-junit",
    "href": "05_writing_junit_tests.html#setting-up-junit",
    "title": "5  Writing JUnit Tests and Test-Driven Development",
    "section": "5.3 Setting Up JUnit",
    "text": "5.3 Setting Up JUnit\nBefore you can use JUnit, you need to make sure the library is on your classpath. This process can vary depending on the IDE and build system you’re using.\nFor our labs, we will ensure the JUnit library is on our classpath by pre-configuring the project and IDE for you. However, if you’re working on your own project, you’ll need to add the JUnit library to your project’s classpath.\nWhen working on your own projects, you might be interested in using a build system like Maven or Gradle to manage your dependencies. These build systems make it easy to add and manage dependencies in your project. For example, if you’re using Maven, you can add the following dependency to your pom.xml file:\n&lt;dependency&gt;\n    &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt;\n    &lt;artifactId&gt;junit-jupiter-api&lt;/artifactId&gt;\n   \n\n &lt;version&gt;5.7.0&lt;/version&gt;\n    &lt;scope&gt;test&lt;/scope&gt;\n&lt;/dependency&gt;\nThis will automatically download the JUnit library and add it to your project’s classpath.\nRegardless of how you added the JUnit library to your project, next, we need to import the necessary classes and annotations from JUnit. At the top of our MathUtilTest class, we add the following import statements:\nimport static org.junit.jupiter.api.Assertions.*;\nimport org.junit.jupiter.api.Test;\nimport org.junit.jupiter.api.BeforeEach;\nThe first import statement statically imports all assertion methods from Assertions, allowing us to use them directly in our code. The second import statement imports the Test annotation, which we use to denote our test methods. The third import statement imports the BeforeEach annotation, which we’ll discuss in a moment."
  },
  {
    "objectID": "05_writing_junit_tests.html#utilizing-test-and-beforeeach-annotations",
    "href": "05_writing_junit_tests.html#utilizing-test-and-beforeeach-annotations",
    "title": "5  Writing JUnit Tests and Test-Driven Development",
    "section": "5.4 Utilizing @Test and @BeforeEach Annotations",
    "text": "5.4 Utilizing @Test and @BeforeEach Annotations\nIn JUnit, we use the @Test annotation to indicate that a method is a test method. This allows JUnit to automatically discover and run this method as a test.\n@Test\npublic void addTest1() {\n    // test code...\n}\nHowever, what if we have some setup code that we want to run before each test? This is where the @BeforeEach annotation comes in. Any method annotated with @BeforeEach will be run before each @Test method.\nLet’s say we want to create a new MathUtil instance before each test:\nMathUtil m;\n\n@BeforeEach\npublic void setup() {\n    m = new MathUtil();\n}\nNow, before each test method is run, JUnit will first execute the setup method, ensuring that we have a fresh MathUtil instance for each test."
  },
  {
    "objectID": "05_writing_junit_tests.html#interpreting-junit-test-runner-output",
    "href": "05_writing_junit_tests.html#interpreting-junit-test-runner-output",
    "title": "5  Writing JUnit Tests and Test-Driven Development",
    "section": "5.5 Interpreting JUnit Test Runner Output",
    "text": "5.5 Interpreting JUnit Test Runner Output\nUnderstanding the output of the JUnit test runner is crucial for interpreting the results of your tests. This helps you diagnose issues in your code and identify exactly what went wrong. Let’s analyze the output of the junit-platform-console-standalone test runner to get a feel for how this works.\n╷\n├─ JUnit Jupiter ✔\n├─ JUnit Vintage ✔\n│  └─ BinarySearchTreeHiddenTest ✔\n│     ├─ testInsertAndSearch ✔\n│     ├─ testDeleteSingleNode ✔\n│     ├─ testTreeTraversals ✘ expected:&lt;[2, 3, 4, 5, [6, 7], 8]&gt; but was:&lt;[2, 3, 4, 5, [7, 6], 8]&gt;\n│     ├─ testContainsElementNotInTree ✔\n│     ├─ testContainsEmptyTree ✔\n│     ├─ testDeleteEmptyTree ✔\n│     ├─ testContainsElementInTree ✔\n│     ├─ testSearchElementInTree ✔\n│     ├─ testInsertMultipleElements ✔\n│     ├─ testDeleteDuplicateElements ✔\n│     ├─ testDeleteElementNotInTree ✔\n│     ├─ testInsertNegativeNumbers ✔\n│     ├─ testInsertAndSize ✔\n│     ├─ testSearchEmptyTree ✔\n│     ├─ testDeleteNodeWithMultipleElements ✔\n│     ├─ testInsertDuplicatesAndRemove ✔\n│     ├─ testInsertSingleElement ✔\n│     └─ testSearchElementNotInTree ✔\n└─ JUnit Platform Suite ✔\n\nFailures (1):\n  JUnit Vintage:BinarySearchTreeHiddenTest:testTreeTraversals\n    =&gt; org.junit.ComparisonFailure: expected:&lt;[2, 3, 4, 5, [6, 7], 8]&gt; but was:&lt;[2, 3, 4, 5, [7, 6], 8]&gt;\n       DataStructures.BinarySearchTreeHiddenTest.testTreeTraversals(BinarySearchTreeHiddenTest.java:220)\n       [...]\nThe JUnit console output provides a tree structure representing the test execution. The topmost nodes represent the test engines used, in this case, JUnit Jupiter and JUnit Vintage. Underneath each engine are the individual test classes, such as BinarySearchTreeHiddenTest.\nWithin each test class node, there are child nodes representing each test method, such as testInsertAndSearch or testDeleteSingleNode. These methods are marked with a ✔ symbol if they passed, and with a ✘ symbol if they failed. In this case, we see that testTreeTraversals has failed.\nAccompanying the failure symbol is a brief description of the failure, which is the assertion message from the test method. In this example, the test expected the array [2, 3, 4, 5, 6, 7, 8], but received the array [2, 3, 4, 5, 7, 6, 8]. This discrepancy caused the test to fail.\nAfter the tree structure, there is a section titled Failures which provides more detailed information about each failure. For each failure, it lists:\n\nThe test class and method that failed.\nThe type of assertion failure that occurred, which is org.junit.ComparisonFailure in this case.\nThe detailed assertion failure message, which is the same as what’s shown in the tree structure.\nThe location in the code where the failure occurred, which can be very useful (and is often the first thing you should look at when debugging a test failure). In this case, the failure occurred on line 220 of BinarySearchTreeHiddenTest.java (see the DataStructures.BinarySearchTreeHiddenTest.testTreeTraversals(BinarySearchTreeHiddenTest.java:220) line)."
  },
  {
    "objectID": "05_writing_junit_tests.html#conclusion",
    "href": "05_writing_junit_tests.html#conclusion",
    "title": "5  Writing JUnit Tests and Test-Driven Development",
    "section": "5.6 Conclusion",
    "text": "5.6 Conclusion\nIn conclusion, JUnit simplifies the process of writing and managing tests. It provides a comprehensive set of assertion methods to verify our code and uses annotations to define and organize our tests, making them easier to read and understand. By taking advantage of these features, we can write more effective and maintainable tests. In the next section, we’ll dive deeper into Test-Driven Development, a methodology that leverages the power of testing to guide and improve the development process."
  },
  {
    "objectID": "05_writing_junit_tests.html#test-driven-development",
    "href": "05_writing_junit_tests.html#test-driven-development",
    "title": "5  Writing JUnit Tests and Test-Driven Development",
    "section": "5.7 Test-Driven Development",
    "text": "5.7 Test-Driven Development\nTest-Driven Development (TDD) is a software development methodology that is centered around the idea of writing tests before writing the actual code. It is a highly disciplined process that follows a strict order of operations: red, green, refactor. This method has profound implications on the design, quality, and reliability of the software.\nLet’s dive into what these steps entail.\n\nRed: Write a test that covers a specific functionality you want to implement. This test should fail initially because you haven’t written the actual code yet. This stage helps you think about the functionality in detail, ponder on the inputs and expected outputs, and outline the structure of your code.\nGreen: Write the minimal amount of code needed to pass the test. At this stage, don’t worry about the elegance of your code. Your primary focus is on functionality. Run your test, and it should pass this time.\nRefactor: Refactor the code you just wrote in the green stage to eliminate duplication, improve readability, and ensure the code adheres to the best practices. After refactoring, all tests should still pass. If a test fails, it means the refactoring broke the functionality, and you need to revise your changes.\n\nThis cycle repeats for every small chunk of functionality you add to your software. With this approach, you are incrementally building your software with the assurance that at each step, the implemented functionality is working as expected.\n\n5.7.1 The Motivation Behind Test-Driven Development\nYou might be wondering, why would you want to put in the extra effort to write tests before writing the actual code? Here are a few motivating factors:\n\nConfidence: With TDD, you can be confident that your code works because you have tests that prove it. This confidence is especially important when you need to modify your code later. Changes can break existing functionality, but with a robust set of tests, you can quickly catch and fix these regressions.\nBetter Design: Writing tests first forces you to think about your code from a user’s perspective. This shift in viewpoint often results in better code organization and modularity because you design your code to be easy to test, which typically means it is also easy to use and modify.\nDocumentation: Tests act as a form of documentation that shows how the code is supposed to work. New team members can look at the tests to understand what each function is supposed to do and what edge cases it handles.\nDevelopment Speed: While TDD might seem to slow you down at the beginning, it typically results in faster development in the long run. With TDD, you spend less time debugging and fixing bugs because you catch them early in the development process, before they become entangled with other parts of the code.\n\nIn conclusion, TDD is a powerful methodology that can significantly improve the quality of your code and your efficiency as a developer. While it might seem difficult at first, with practice, it becomes a natural part of the development process."
  },
  {
    "objectID": "06_mutation_testing.html#unit-testing-a-recap",
    "href": "06_mutation_testing.html#unit-testing-a-recap",
    "title": "6  Introduction to Mutation Testing",
    "section": "6.1 Unit Testing, a Recap",
    "text": "6.1 Unit Testing, a Recap\nWriting correct code is challenging. Owning your code, maintaining it, and ensuring it always performs as expected can be even more difficult. After all, we are human, and mistakes are a part of our nature. Therefore, we can’t always rely solely on our ability to catch these mistakes. Instead, automated processes that can help us identify and correct these errors are truly invaluable.\nUnit testing is one such automated process that tests your code. These tests target individual units of your code and are designed to verify their behavior. In programming, a method can be thought of as the smallest unit of code, and Classes, Packages, and Modules are larger units of code. Every unit test is designed to validate a single behavior of a single unit of code.\nUnit testing is so crucial that they are a mandatory requirement for many software development projects. For instance, if you are contributing a Java source class to most projects, you are also required to provide unit tests for that class. In this course, where we deal with Data Structures and Algorithms, learning how to implement them in Java, etc., submitting your implementations without Unit tests would be like submitting a paper without a bibliography. It might make us question, “Sure, you wrote this and it looks convincing, but where’s your evidence? Why should I believe you?”\nAnd that’s why you will be required to write unit tests for your Data Structures and Algorithms implementations. But then, how do you know that your unit tests are correct?\nConsider this example:\n@Test\npublic void testAdd() {\n   assertTrue(1 == 1);\n}\nThe test passes, but does it mean it’s correct? Let’s examine the test."
  },
  {
    "objectID": "06_mutation_testing.html#writing-correct-unit-tests",
    "href": "06_mutation_testing.html#writing-correct-unit-tests",
    "title": "6  Introduction to Mutation Testing",
    "section": "6.2 Writing Correct Unit Tests",
    "text": "6.2 Writing Correct Unit Tests\nTaking a closer look at the unit test above, two problems become evident:\n\nThe add method isn’t called in the test.\nThe test can’t fail.\n\nTo address the first issue, we can leverage tools that detect when a unit test doesn’t cover a particular line of code. Here “cover” means “execute”.\nSo what about a test like this?\n@Test\npublic void testAdd() {\n   assertTrue(add(1, 1) == add(1, 1));\n}\nThe add method is called, and test coverage is at 100%. However, this test can’t fail, meaning it is not correct."
  },
  {
    "objectID": "06_mutation_testing.html#mutation-testing-a-solution-to-test-the-correctness-of-your-unit-tests",
    "href": "06_mutation_testing.html#mutation-testing-a-solution-to-test-the-correctness-of-your-unit-tests",
    "title": "6  Introduction to Mutation Testing",
    "section": "6.3 Mutation Testing: A Solution to Test the Correctness of Your Unit Tests",
    "text": "6.3 Mutation Testing: A Solution to Test the Correctness of Your Unit Tests\nMutation testing provides a way to test the correctness of your unit tests. It can help you find tests that can’t fail, tests that need more test cases, and even logic errors in your code.\nThe basic idea of mutation testing is as follows: if you claim your source code is correct, and that the tests prove it, mutation testing will challenge that claim. Mutation testing introduces small changes, called mutations, to your source code. If your tests still pass after these changes, it suggests that your test or source code may not be correct.\nThese mutations are created by algorithms called mutators or mutation operators. Each time a mutator runs, it receives a fresh copy of your source code and makes only one change. The result of a mutator is a mutant - a mutated version of your source code."
  },
  {
    "objectID": "06_mutation_testing.html#mutation-testing-in-practice",
    "href": "06_mutation_testing.html#mutation-testing-in-practice",
    "title": "6  Introduction to Mutation Testing",
    "section": "6.4 Mutation Testing in Practice",
    "text": "6.4 Mutation Testing in Practice\nHere’s how mutation testing works in practice:\n\nApply a set of mutators to your source code to produce a collection of mutants.\nFor each mutant, run your unit tests.\n\nIf the unit tests pass, the mutant has survived.\nIf the unit tests fail, the mutant was killed\n\n\n.\nThe aim is to kill as many mutants as possible. If too many mutants survive, it indicates that your unit tests are not sufficient to prove that your source code is correct. If you manage to kill all mutants, you can use your unit tests to argue that your source code is indeed correct.\nIn our course, only unit tests that leave no surviving mutants will be accepted as a valid submission.\n\n6.4.1 What are these mutators?\nMutators introduce specific types of changes to your code. For instance, a primitive returns mutator (PRIMITIVE_RETURNS) replaces int, short, long, char, float, and double return values with 0. So, for example, this method:\npublic int add(int lhs, int rhs) {\n    return lhs + rhs;\n}\nbecomes:\npublic int add(int lhs, int rhs) {\n    return 0;\n}\nThis mutant would survive in two cases: if the add method is never tested or if the only test cases for add are ones where the result is 0. To fix this, you can add more test cases that test add with non-zero results.\nAnother example of a mutator is the Remove Conditionals Mutator (REMOVE_CONDITIONALS), which removes all conditional statements such that the guarded statements always execute.\nFor example, this code:\nif (a == b) {\n  // do something\n}\nbecomes:\nif (true) {\n  // do something\n}\nThis mutant would survive if the source method is never tested, if all test cases for the source method only ever test the true case, or if both branches have the exact same code or equivalent code, to begin with. To fix this, you can add more test cases that test the false case, and ensure that the true and false branches are not equivalent.\nThrough mutation testing, you will get valuable feedback to improve your code and tests, making them more robust and reliable.\nRemember, a well-tested code base is not just about coverage—it’s about the quality of tests, their ability to catch mistakes, and their resilience against possible errors. Mutation testing is an invaluable tool in achieving this.\nCertainly, let’s decipher the feedback from the Coding Rooms."
  },
  {
    "objectID": "06_mutation_testing.html#understanding-codingrooms-feedback",
    "href": "06_mutation_testing.html#understanding-codingrooms-feedback",
    "title": "6  Introduction to Mutation Testing",
    "section": "6.5 Understanding CodingRooms Feedback",
    "text": "6.5 Understanding CodingRooms Feedback\nIn the feedback provided by the autograder in Coding Rooms, you can find two main parts. The first part lists the mutation tests that have been run, and the second part provides a grading overview.\n\n6.5.1 Understanding Mutation Test Feedback\nEach mutation test feedback starts with “Running Mutation tests”, followed by details about each mutation test performed.\nRunning Mutation tests -\nRan mutation tests for Calculator.CalculatorTest -\n-[ RECORD 0 ]---------+------------------------------------\nMutation type         | RemoveConditionalMutator_EQUAL_ELSE\nSource method mutated | divide\nLine no. of mutation  | 56\nTest examined         | None\nResult                | SURVIVED\nIn the example above, the autograder has run a mutation test on the “divide” method of your “Calculator” class. Here is what each line in the record means:\n\nMutation type: The type of mutation that was made to your code. In this case, a RemoveConditionalMutator_EQUAL_ELSE mutation was made. This mutation type removes a conditional (==) operator and always takes the else branch.\nSource method mutated: The method in your code that was mutated for the test. In this case, it’s the “divide” method.\nLine no. of mutation: The line number in your code where the mutation was applied.\nTest examined: This line indicates which test case was run against the mutant. “None” means no specific test case was chosen, and all available tests were run.\nResult: The outcome of the mutation test. If your tests fail against the mutated code (which is a good thing!), this line will read “KILLED”. If your tests pass (which implies your tests didn’t catch the error introduced by the mutation), this line will read “SURVIVED”. In this case, the mutant has survived, indicating your tests didn’t catch the error.\n\nIf any mutants survive, the autograder lists those under the line, “Problematic mutation test failures printed about.”\n\n\n6.5.2 Understanding the Grading Overview\nThe grading overview gives you a quick summary of how your code has performed in the tests. It breaks down the grading by requirements.\n┌──────────────────────────────────────────────────────┬\n│                   Grading Overview                   │\n├──────────────────────────────────────────────────────┼\n│ Requirement │    Grade    │          Reason          │\n├─────────────┼─────────────┼──────────────────────────┤\n│      1      │ 10.00/10.00 │   - 4/4 tests passing.   │\n├─────────────┼─────────────┼──────────────────────────┤\n│      2      │ 32.00/40.00 │ -8 Penalty due to surviv │\n│             │             │       ing muations       │\n├─────────────┼─────────────┼──────────────────────────┤\n│                  Total: 42.00/50.00                  │\n└──────────────────────────────────────────────────────┴\nIn the example above, we have:\n\nRequirement 1: This section corresponds to the first requirement of the assignment. The student received a full score (10.00/10.00) because all 4 test cases associated with this requirement passed.\nRequirement 2: This section corresponds to the second requirement of the assignment. The student lost 8 points because of surviving mutations, resulting in a score of 32.00/40.00 for this requirement.\n\nThe total grade of the assignment is\n42.00/50.00, indicating the need for further improvement.\nRemember, each surviving mutation indicates a flaw in your test cases—they didn’t catch an erroneous mutation. To improve your grade, aim to update your test cases to ensure that they effectively detect the mutations."
  },
  {
    "objectID": "07_list_of_mutators.html#conditionals-boundary-mutator-conditionals_boundary",
    "href": "07_list_of_mutators.html#conditionals-boundary-mutator-conditionals_boundary",
    "title": "7  List of Mutators",
    "section": "7.1 Conditionals Boundary Mutator (CONDITIONALS_BOUNDARY)",
    "text": "7.1 Conditionals Boundary Mutator (CONDITIONALS_BOUNDARY)\nThe conditionals boundary mutator replaces the relational operators &lt;, &lt;=, &gt;, &gt;=\nwith their boundary counterpart as per the table below.\n\n\n\nOriginal conditional\nMutated conditional\n\n\n\n\n&lt;\n&lt;=\n\n\n&lt;=\n&lt;\n\n\n&gt;\n&gt;=\n\n\n&gt;=\n&gt;\n\n\n\n{:.table }\nFor example\nif (a &lt; b) {\n  // do something\n}\nwill be mutated to\nif (a &lt;= b) {\n  // do something\n}"
  },
  {
    "objectID": "07_list_of_mutators.html#increments-mutator-increments",
    "href": "07_list_of_mutators.html#increments-mutator-increments",
    "title": "7  List of Mutators",
    "section": "7.2 Increments Mutator (INCREMENTS)",
    "text": "7.2 Increments Mutator (INCREMENTS)\nThe increments mutator will mutate increments, decrements and assignment increments and decrements of local variables (stack variables). It will replace increments with decrements and vice versa.\nFor example\npublic int method(int i) {\n  i++;\n  return i;\n}\nwill be mutated to\npublic int method(int i) {\n  i--;\n  return i;\n}\nPlease note that the increments mutator will be applied to increments of local variables only. Increments and decrements of member variables will be covered by the Math Mutator."
  },
  {
    "objectID": "07_list_of_mutators.html#invert-negatives-mutator-invert_negs",
    "href": "07_list_of_mutators.html#invert-negatives-mutator-invert_negs",
    "title": "7  List of Mutators",
    "section": "7.3 Invert Negatives Mutator (INVERT_NEGS)",
    "text": "7.3 Invert Negatives Mutator (INVERT_NEGS)\nThe invert negatives mutator inverts negation of integer and floating point numbers. For example\npublic float negate(final float i) {\n  return -i;\n}\nwill be mutated to\npublic float negate(final float i) {\n  return i;\n}"
  },
  {
    "objectID": "07_list_of_mutators.html#math-mutator-math",
    "href": "07_list_of_mutators.html#math-mutator-math",
    "title": "7  List of Mutators",
    "section": "7.4 Math Mutator (MATH)",
    "text": "7.4 Math Mutator (MATH)\nThe math mutator replaces binary arithmetic operations for either integer or floating-point arithmetic with another operation. The replacements will be selected according to the table below.\n\n\n\nOriginal conditional\nMutated conditional\n\n\n\n\n+\n-\n\n\n-\n+\n\n\n*\n/\n\n\n/\n*\n\n\n%\n*\n\n\n&\n|\n\n\n|\n&\n\n\n^\n&\n\n\n&lt;&lt;\n&gt;&gt;\n\n\n&gt;&gt;\n&lt;&lt;\n\n\n&gt;&gt;&gt;\n&lt;&lt;\n\n\n\n{:.table}\nFor example\nint a = b + c;\nwill be mutated to\nint a = b - c;\nKeep in mind that the + operator on Strings as in\nString a = \"foo\" + \"bar\";\nis not a mathematical operator but a string concatenation and will be replaced by the compiler with something like\nString a = new StringBuilder(\"foo\").append(\"bar\").toString();\nPlease note that the compiler will also use binary arithmetic operations for increments, decrements and assignment increments and decrements of non-local variables (member variables) although a special iinc opcode for increments exists. This special opcode is restricted to local variables (also called stack variables) and cannot be used for member variables. That means the math mutator will also mutate\npublic class A {\n  private int i;\n\n  public void foo() {\n    this.i++;\n  }\n}\nto\npublic class A {\n  private int i;\n\n  public void foo() {\n    this.i = this.i - 1;\n  }\n}\nSee the Increments Mutator for details."
  },
  {
    "objectID": "07_list_of_mutators.html#negate-conditionals-mutator-negate_conditionals",
    "href": "07_list_of_mutators.html#negate-conditionals-mutator-negate_conditionals",
    "title": "7  List of Mutators",
    "section": "7.5 Negate Conditionals Mutator (NEGATE_CONDITIONALS)",
    "text": "7.5 Negate Conditionals Mutator (NEGATE_CONDITIONALS)\nThe negate conditionals mutator will mutate all conditionals found according to the replacement table below.\n\n\n\nOriginal conditional\nMutated conditional\n\n\n\n\n==\n!=\n\n\n!=\n==\n\n\n&lt;=\n&gt;\n\n\n&gt;=\n&lt;\n\n\n&lt;\n&gt;=\n\n\n&gt;\n&lt;=\n\n\n\n{:.table }\nFor example\nif (a == b) {\n  // do something\n}\nwill be mutated to\nif (a != b) {\n  // do something\n}\nThis mutator overlaps to a degree with the conditionals boundary mutator, but is less stable i.e these mutations are generally easier for a test suite to detect."
  },
  {
    "objectID": "07_list_of_mutators.html#return-values-mutator-return_vals",
    "href": "07_list_of_mutators.html#return-values-mutator-return_vals",
    "title": "7  List of Mutators",
    "section": "7.6 Return Values Mutator (RETURN_VALS)",
    "text": "7.6 Return Values Mutator (RETURN_VALS)\nThis mutator has been superseded by the new returns mutator set. See Empty returns, False returns, True returns, Null returns and Primitive returns.\nThe return values mutator mutates the return values of method calls. Depending on the return type of the method another mutation is used.4\n\n\n\n\n\n\n\nReturn Type\nMutation\n\n\n\n\nboolean\nreplace the unmutated return value true with false and replace the unmutated return value false with true\n\n\nint byte short\nif the unmutated return value is 0 return 1, otherwise mutate to return value 0\n\n\nlong\nreplace the unmutated return value x with the result of x+1\n\n\nfloat double\nreplace the unmutated return value x with the result of -(x+1.0) if x is not NAN and replace NAN with 0\n\n\nObject\nreplace non-null return values with null and throw a java.lang.RuntimeException if the unmutated method would return null\n\n\n\n{:.table}\nFor example\npublic Object foo() {\n  return new Object();\n}\nwill be mutated to\npublic Object foo() {\n  new Object();\n  return null;\n}\nPlease note that constructor calls are not considered void method calls. See the Constructor Call Mutator for mutations of constructors or the Non Void Method Call Mutator for mutations of non void methods."
  },
  {
    "objectID": "07_list_of_mutators.html#void-method-call-mutator-void_method_calls",
    "href": "07_list_of_mutators.html#void-method-call-mutator-void_method_calls",
    "title": "7  List of Mutators",
    "section": "7.7 Void Method Call Mutator (VOID_METHOD_CALLS)",
    "text": "7.7 Void Method Call Mutator (VOID_METHOD_CALLS)\nThe void method call mutator removes method calls to void methods. For example\npublic void someVoidMethod(int i) {\n  // does something\n}\n\npublic int foo() {\n  int i = 5;\n  someVoidMethod(i);\n  return i;\n}\nwill be mutated to\npublic void someVoidMethod(int i) {\n  // does something\n}\n\npublic int foo() {\n  int i = 5;\n  return i;\n}"
  },
  {
    "objectID": "07_list_of_mutators.html#empty-returns-mutator-empty_returns",
    "href": "07_list_of_mutators.html#empty-returns-mutator-empty_returns",
    "title": "7  List of Mutators",
    "section": "7.8 Empty returns Mutator (EMPTY_RETURNS)",
    "text": "7.8 Empty returns Mutator (EMPTY_RETURNS)\nReplaces return values with an ‘empty’ value for that type as follows\n\njava.lang.String -&gt; “”\njava.util.Optional -&gt; Optional.empty()\njava.util.List -&gt; Collections.emptyList()\njava.util.Collection -&gt; Collections.emptyList()\njava.util.Set -&gt; Collections.emptySet()\njava.lang.Integer -&gt; 0\njava.lang.Short -&gt; 0\njava.lang.Long -&gt; 0\njava.lang.Character -&gt; 0\njava.lang.Float -&gt; 0\njava.lang.Double -&gt; 0\n\nPitest will filter out equivalent mutations to methods that are already hard coded to return the empty value."
  },
  {
    "objectID": "07_list_of_mutators.html#false-returns-mutator-false_returns",
    "href": "07_list_of_mutators.html#false-returns-mutator-false_returns",
    "title": "7  List of Mutators",
    "section": "7.9 False returns Mutator (FALSE_RETURNS)",
    "text": "7.9 False returns Mutator (FALSE_RETURNS)\nReplaces primitive and boxed boolean return values with false.\nPitest will filter out equivalent mutations to methods that are already hard coded to return false."
  },
  {
    "objectID": "07_list_of_mutators.html#true-returns-mutator-true_returns",
    "href": "07_list_of_mutators.html#true-returns-mutator-true_returns",
    "title": "7  List of Mutators",
    "section": "7.10 True returns Mutator (TRUE_RETURNS)",
    "text": "7.10 True returns Mutator (TRUE_RETURNS)\nReplaces primitive and boxed boolean return values with true.\nPitest will filter out equivalent mutations to methods that are already hard coded to return true."
  },
  {
    "objectID": "07_list_of_mutators.html#null-returns-mutator-null_returns",
    "href": "07_list_of_mutators.html#null-returns-mutator-null_returns",
    "title": "7  List of Mutators",
    "section": "7.11 Null returns Mutator (NULL_RETURNS)",
    "text": "7.11 Null returns Mutator (NULL_RETURNS)\nReplaces return values with null. Methods that can be mutated by the EMPTY_RETURNS mutator or that are directly annotated with NotNull will not be mutated.\nPitest will filter out equivalent mutations to methods that are already hard coded to return null."
  },
  {
    "objectID": "07_list_of_mutators.html#primitive-returns-mutator-primitive_returns",
    "href": "07_list_of_mutators.html#primitive-returns-mutator-primitive_returns",
    "title": "7  List of Mutators",
    "section": "7.12 Primitive returns Mutator (PRIMITIVE_RETURNS)",
    "text": "7.12 Primitive returns Mutator (PRIMITIVE_RETURNS)\nReplaces int, short, long, char, float and double return values with 0.\nPitest will filter out equivalent mutations to methods that are already hard coded to return 0."
  },
  {
    "objectID": "07_list_of_mutators.html#constructor-call-mutator-constructor_calls",
    "href": "07_list_of_mutators.html#constructor-call-mutator-constructor_calls",
    "title": "7  List of Mutators",
    "section": "7.13 Constructor Call Mutator (CONSTRUCTOR_CALLS)",
    "text": "7.13 Constructor Call Mutator (CONSTRUCTOR_CALLS)\nOptional mutator that replaces constructor calls with null values. For example\npublic Object foo() {\n  Object o = new Object();\n  return o;\n}\nwill be mutated to\npublic Object foo() {\n  Object o = null;\n  return o;\n}\nPlease note that this mutation is fairly unstable and likely to cause NullPointerExceptions even with weak test suites.\nThis mutator does not affect non constructor method calls. See Void Method Call Mutator for mutations of void methods and Non Void Method Call Mutator for mutations of non void methods."
  },
  {
    "objectID": "08_sorting_intro.html#understanding-the-sorting-problem",
    "href": "08_sorting_intro.html#understanding-the-sorting-problem",
    "title": "8  Introduction to Sorting",
    "section": "8.1 Understanding the Sorting Problem",
    "text": "8.1 Understanding the Sorting Problem\nSorting is an essential problem in computer science and software engineering. Simply put, sorting is the task of rearranging a set of items, such as an array, in a specific order. For instance, an array of integers might be sorted in ascending or descending order based on their numeric value.\nOne might wonder why sorting is so ubiquitous in the field of software engineering. The answer lies in the multitude of applications that it has. Sorting allows for more efficient searching, data compression, and organization. It is a crucial step in numerous algorithms and computational tasks. The efficiency of a sorting algorithm can significantly affect the performance of these applications."
  },
  {
    "objectID": "08_sorting_intro.html#developing-a-sorting-algorithm-from-scratch",
    "href": "08_sorting_intro.html#developing-a-sorting-algorithm-from-scratch",
    "title": "8  Introduction to Sorting",
    "section": "8.2 Developing a Sorting Algorithm from Scratch",
    "text": "8.2 Developing a Sorting Algorithm from Scratch\nOne might find the task of developing a sorting algorithm from scratch quite daunting. Sorting algorithms, like many computational problems, have an abstract and often complex nature that can make it difficult to understand, let alone create. If shown the solution, one might understand how it works and even memorize it, but the thought process behind creating such a solution can be elusive.\nA common approach to overcoming such challenges is to simplify the problem. By breaking down a complex problem into simpler subproblems, we can gain insights into the problem’s structure and ultimately develop a solution. This approach will be demonstrated as we develop a sorting algorithm."
  },
  {
    "objectID": "08_sorting_intro.html#starting-simple-sorting-an-array-of-size-1",
    "href": "08_sorting_intro.html#starting-simple-sorting-an-array-of-size-1",
    "title": "8  Introduction to Sorting",
    "section": "8.3 Starting Simple: Sorting an Array of Size 1",
    "text": "8.3 Starting Simple: Sorting an Array of Size 1\nThe simplest possible sorting problem involves an array of size one. But, of course, this array is already sorted as it only contains a single element. Thus, we need to do nothing to sort this array. Here’s how this can be implemented in Java:\npublic static void sort1(int[] arr) {\n  // do nothing\n}"
  },
  {
    "objectID": "08_sorting_intro.html#sorting-an-array-of-size-2",
    "href": "08_sorting_intro.html#sorting-an-array-of-size-2",
    "title": "8  Introduction to Sorting",
    "section": "8.4 Sorting an Array of Size 2",
    "text": "8.4 Sorting an Array of Size 2\nWhat about an array of size two? In this case, we simply need to compare the two elements and swap them if they are out of order:\npublic static void sort2(int[] arr) {\n    if (arr[0] &gt; arr[1]) {\n      int temp = arr[0];\n      arr[0] = arr[1];\n      arr[1] = temp;\n    }\n}"
  },
  {
    "objectID": "08_sorting_intro.html#expanding-the-problem-sorting-an-array-of-size-3",
    "href": "08_sorting_intro.html#expanding-the-problem-sorting-an-array-of-size-3",
    "title": "8  Introduction to Sorting",
    "section": "8.5 Expanding the Problem: Sorting an Array of Size 3",
    "text": "8.5 Expanding the Problem: Sorting an Array of Size 3\nWhen sorting an array of size three, we perform similar steps as we did for the array of size two. First, we compare and potentially swap the first two numbers. Then, we do the same for the second and third numbers. Finally, we compare the first two numbers once again:\npublic static void sort3(int[] arr) {\n    if (arr[0] &gt; arr[1]) {\n      int temp = arr[0];\n      arr[0] = arr[1];\n      arr[1] = temp;\n    }\n    if (arr[1] &gt; arr[2]) {\n      int temp = arr[1];\n      arr[1] = arr[2];\n      arr[2] = temp;\n    }\n    if (arr[0] &gt; arr[1]) {\n      int temp = arr[0];\n      arr[0] = arr[1];\n      arr[1] = temp;\n    }\n}\nBy following this process, we ensure that the array is sorted, regardless of the initial order of the elements."
  },
  {
    "objectID": "08_sorting_intro.html#scaling-up-sorting-an-array-of-size-4",
    "href": "08_sorting_intro.html#scaling-up-sorting-an-array-of-size-4",
    "title": "8  Introduction to Sorting",
    "section": "8.6 Scaling Up: Sorting an Array of Size 4",
    "text": "8.6 Scaling Up: Sorting an Array of Size 4\nWe can extend the same approach to sort an array of size four. However, notice that once we have the largest number at the last position, the problem\nreduces to sorting an array of size three. This realization will be crucial as we move forward.\npublic static void sort4(int[] arr) {\n    for(int i = 0; i &lt; 3; i++) {\n      if (arr[i] &gt; arr[i+1]) {\n        int temp = arr[i];\n        arr[i] = arr[i+1];\n        arr[i+1] = temp;\n      }\n    }\n    sort3(arr);\n}"
  },
  {
    "objectID": "08_sorting_intro.html#recognizing-the-pattern",
    "href": "08_sorting_intro.html#recognizing-the-pattern",
    "title": "8  Introduction to Sorting",
    "section": "8.7 Recognizing the Pattern",
    "text": "8.7 Recognizing the Pattern\nAs we incrementally solve larger versions of our sorting problem, we start to notice a pattern. For every array size, we first ensure the largest number ends up at the end of the array. Then, we apply the same sorting logic to the remaining elements in the array.\nFor instance, when sorting an array of size 5, we first find and move the largest number to the end of the array, then apply the sorting process for an array of size 4 to the remaining numbers:\npublic static void sort5(int[] arr) {\n  for(int i = 0; i &lt; 4; i++) {\n    if (arr[i] &gt; arr[i+1]) {\n      int temp = arr[i];\n      arr[i] = arr[i+1];\n      arr[i+1] = temp;\n    }\n  }\n  sort4(arr);\n}"
  },
  {
    "objectID": "08_sorting_intro.html#generalizing-the-algorithm",
    "href": "08_sorting_intro.html#generalizing-the-algorithm",
    "title": "8  Introduction to Sorting",
    "section": "8.8 Generalizing the Algorithm",
    "text": "8.8 Generalizing the Algorithm\nThis pattern suggests a way to generalize our algorithm to sort an array of any size. For an array of size n, we first ensure the largest number ends up at the end, then recursively apply the same process to the remaining n-1 elements. By repeatedly applying this process, we ensure that the entire array ends up sorted.\npublic static void sort(int[] arr) {\n  for(int i = arr.length - 1; i &gt; 0; i--) {\n    for(int j = 0; j &lt; i; j++) {\n      if (arr[j] &gt; arr[j+1]) {\n        int temp = arr[j];\n        arr[j] = arr[j+1];\n        arr[j+1] = temp;\n      }\n    }\n  }\n}\nThis algorithm, known as Bubble Sort, represents a simple yet effective solution to the sorting problem. It demonstrates how complex problems can often be broken down into simpler subproblems, providing a valuable lesson for problem-solving in computer science and beyond."
  },
  {
    "objectID": "09_comparable_comparator.html#sorting-complex-objects",
    "href": "09_comparable_comparator.html#sorting-complex-objects",
    "title": "9  Comparable and Comparator",
    "section": "9.1 Sorting Complex Objects",
    "text": "9.1 Sorting Complex Objects\nIn our previous lessons, we have primarily dealt with arrays of basic data types, such as integers, which are inherently easy to compare and sort. However, software development often involves sorting collections of complex objects, such as Employee or Student objects. These user-defined types lack a natural order for comparison, necessitating a custom solution."
  },
  {
    "objectID": "09_comparable_comparator.html#defining-a-comparison-method",
    "href": "09_comparable_comparator.html#defining-a-comparison-method",
    "title": "9  Comparable and Comparator",
    "section": "9.2 Defining a Comparison Method",
    "text": "9.2 Defining a Comparison Method\nA rudimentary solution could involve appending a comparison method to our Employee or Student classes, like compareSalary or compareGPA. However, this design lacks versatility - each new attribute requires a fresh comparison method.\nA superior solution employs the Comparable interface, comprising a single method, compareTo. This method, accepting an object of the same type, returns an integer indicating if the current object is lesser than, equal to, or greater than the input object.\nThe Comparable interface in Java is defined as follows:\npublic interface Comparable&lt;T&gt; {\n    int compareTo(T o);\n}"
  },
  {
    "objectID": "09_comparable_comparator.html#implementing-comparable",
    "href": "09_comparable_comparator.html#implementing-comparable",
    "title": "9  Comparable and Comparator",
    "section": "9.3 Implementing Comparable",
    "text": "9.3 Implementing Comparable\nImplementing the Comparable interface in any class endows it with sortable attributes. For example, sorting Student objects by GPA could be achieved by implementing Comparable&lt;Student&gt; in the Student class, defining compareTo to compare GPAs:\npublic class Student implements Comparable&lt;Student&gt; {\n    private double gpa;\n    // other fields and methods...\n\n    @Override\n    public int compareTo(Student other) {\n        return Double.compare(this.gpa, other.gpa);\n    }\n}\nThe array of Student objects can be sorted with our new comparison logic, arr[j].compareTo(arr[j+1]) &gt; 0:\npublic static &lt;T extends Comparable&lt;T&gt;&gt; void sort(T[] arr) {\n    for(int i = arr.length - 1; i &gt; 0; i--) {\n        for(int j = 0; j &lt; i; j++) {\n            if (arr[j].compareTo(arr[j+1]) &gt; 0) {\n                T temp = arr[j];\n                arr[j] = arr[j+1];\n                arr[j+1] = temp;\n            }\n        }\n    }\n}"
  },
  {
    "objectID": "09_comparable_comparator.html#comparable-in-the-standard-library",
    "href": "09_comparable_comparator.html#comparable-in-the-standard-library",
    "title": "9  Comparable and Comparator",
    "section": "9.4 Comparable in the Standard Library",
    "text": "9.4 Comparable in the Standard Library\nThe Comparable interface facilitates the creation of versatile sorting functions, capable of sorting arrays of any type implementing Comparable. This applies to user-defined types, as well as many built-in types in the Java standard library.\nImplementing the Comparable interface communicates that a class is able to be compared with other instances of its type. This standard behavior empowers methods to interact with objects more abstractly and flexibly.\nThe Java standard library provides a Comparable interface similar to the one defined earlier. Many built-in classes such as Integer, Double, and String already implement this interface, enabling their comparison and sorting with no additional code. Our Student class can be modified to use java.lang.Comparable in place of our custom interface, maintaining the same compareTo method for use in our generic sort function."
  },
  {
    "objectID": "09_comparable_comparator.html#sorting-flexibility-with-comparator",
    "href": "09_comparable_comparator.html#sorting-flexibility-with-comparator",
    "title": "9  Comparable and Comparator",
    "section": "9.5 Sorting Flexibility with Comparator",
    "text": "9.5 Sorting Flexibility with Comparator\nThe Comparable interface is limited to a single compareTo method per class. When multiple sorting methods are required, Java’s Comparator interface comes into play. Comparator represents different orderings for a specific type, allowing multiple comparators per class.\nFor instance, a Comparator for Student objects that orders by GPA could look like this:\nimport java\n\n.util.Comparator;\n\npublic class StudentGPAComparator implements Comparator&lt;Student&gt; {\n    @Override\n    public int compare(Student s1, Student s2) {\n        return Double.compare(s1.getGPA(), s2.getGPA());\n    }\n}\nA modified version of our generic sort function, now accepting a Comparator, can compare elements of the array:\npublic static &lt;T&gt; void sort(T[] arr, Comparator&lt;? super T&gt; comparator) {\n    for(int i = arr.length - 1; i &gt; 0; i--) {\n        for(int j = 0; j &lt; i; j++) {\n            if (comparator.compare(arr[j], arr[j+1]) &gt; 0) {\n                T temp = arr[j];\n                arr[j] = arr[j+1];\n                arr[j+1] = temp;\n            }\n        }\n    }\n}"
  },
  {
    "objectID": "09_comparable_comparator.html#understanding-natural-and-total-orderings",
    "href": "09_comparable_comparator.html#understanding-natural-and-total-orderings",
    "title": "9  Comparable and Comparator",
    "section": "9.6 Understanding Natural and Total Orderings",
    "text": "9.6 Understanding Natural and Total Orderings\nImplementing Comparable endows a class with a natural ordering, a default ordering used in operations like sorting. For example, String objects follow lexicographic order and Integer objects adhere to numerical order. When Comparable&lt;Student&gt; is implemented to compare GPAs, we establish GPA as the natural ordering for Student objects.\nJava’s Comparator interface allows the definition of additional orderings, known as total orderings. While the natural ordering is default, total orderings, which also follow rules of completeness, transitivity, and antisymmetry, provide flexibility to define any suitable ordering.\nThe Comparable and Comparator interfaces epitomize the power of abstraction in computer science. By concentrating on the core concept of order and essential operations involving order, we can create general, flexible, and reusable code to work with a broad spectrum of data types and orderings.\nIn the next chapter, we will delve into how these concepts are employed in data structures like trees and heaps to manage data for efficient searching and sorting. For now, contemplate the elegance and versatility of the Comparable and Comparator interfaces, and the compelling concept of order they embody."
  },
  {
    "objectID": "10_bubble_selection_insertion_sort.html#bubble-sort",
    "href": "10_bubble_selection_insertion_sort.html#bubble-sort",
    "title": "10  Bubble, Selection, and Insertion Sort",
    "section": "10.1 Bubble Sort",
    "text": "10.1 Bubble Sort\nBubble sort is a simple sorting algorithm that works by repeatedly comparing and swapping adjacent elements in an array until the array is sorted. Bubble sort is easy to implement and understand, but it is not very efficient for large arrays. The name bubble sort comes from the idea that the larger elements “bubble up” to the end of the array after each iteration.\n\n10.1.1 Pseudocode\nThe pseudocode for bubble sort is as follows:\nprocedure bubbleSort(A : list of sortable items)\n    n = length(A)\n    repeat\n        swapped = false\n        for i = 1 to n-1 inclusive do\n            if A[i-1] &gt; A[i] then\n                swap(A[i-1], A[i])\n                swapped = true\n            end if\n        end for\n        n = n - 1\n    until not swapped\nend procedure\nThe procedure takes an array A of sortable items and sorts it in ascending order. The procedure uses a variable n to keep track of the unsorted part of the array, and a variable swapped to indicate whether any swaps occurred in the current iteration. The procedure repeats until no swaps are made, which means the array is sorted.\n\n\n10.1.2 An Example\n\nLet us see an example of bubble sort on the following array:\n[5, 1, 4, 2, 8]\nWe start with n = 5 and swapped = false. We compare the first and second elements, 5 and 1, and swap them since 5 &gt; 1. We set swapped to true to indicate that a swap occurred. We compare the second and third elements, 5 and 4, and swap them as well. We continue to compare and swap the adjacent elements until we reach the end of the array. The array after the first iteration is:\n[1, 4, 2, 5, 8]\nWe decrement n by 1, since the last element 8 is now in its correct position. We repeat the process with n = 4 and swapped = false. We compare and swap the first and second elements, 1 and 4, since 1 &lt; 4. We compare and swap the second and third elements, 4 and 2, since 4 &gt; 2. We compare the third and fourth elements, 5 and 8, but do not swap them since 5 &lt; 8. The array after the second iteration is:\n[1, 2, 4, 5, 8]\nWe decrement n by 1, since the last element 5 is now in its correct position. We repeat the process with n = 3 and swapped = false. We compare and swap the first and second elements, 1 and 2, since 1 &lt; 2. We compare and swap the second and third elements, 2 and 4, since 2 &lt; 4. The array after the third iteration is:\n[1, 2, 4, 5, 8]\nWe decrement n by 1, since the last element 4 is now in its correct position. We repeat the process with n = 2 and swapped = false. We compare and swap the first and second elements, 1 and 2, since 1 &lt; 2. The array after the fourth iteration is:\n[1, 2, 4, 5, 8]\nWe decrement n by 1, since the last element 2 is now in its correct position. We repeat the process with n = 1 and swapped = false. We do not compare any elements, since there is only one element left. The array after the fifth iteration is:\n[1, 2, 4, 5, 8]\nWe exit the loop, since swapped is false, which means the array is sorted.\n\n\n\n10.1.3 Lab Instructions\n\nThe template uses Java generics to create a generic class Main that can sort arrays of any type that implements the Comparable interface. Generics are a way of implementing generic programming in Java, which allows you to write code that can work with different types of objects without casting or risking ClassCastException.\nThe constructor of the Main class takes an array of type T as a parameter and assigns it to the data field. The data field is also of type T, which means it can store any type of object that implements Comparable.\nThe BubbleSort method returns an array of type T that is sorted in ascending order using the bubble sort algorithm. The method uses a local variable sorted to store a copy of the data array and then modifies it using the pseudocode provided in the assignment instructions.\nTo compare and swap the elements of the sorted array, you need to use the compareTo method of the Comparable interface. The compareTo method returns a negative integer, zero, or a positive integer if the current object is less than, equal to, or greater than the specified object. For example, if you want to compare the elements at index i-1 and i, you can write:\nif (sorted[i-1].compareTo(sorted[i]) &gt; 0) {\n    // swap the elements\n}\nTo swap the elements of the sorted array, you can use a temporary variable of type T to store one of the elements, and then assign the other element to its place. For example, if you want to swap the elements at index i-1 and i, you can write:\nT temp = sorted[i-1];\nsorted[i-1] = sorted[i];\nsorted[i] = temp;\nAfter performing the bubble sort algorithm, the method returns the sorted array.\nTo test your code, you can create an object of the Main class with different types of arrays, such as Integer, String, or Double, and call the BubbleSort method on them. You can print the original and sorted arrays to check the output. For example, you can write:\nInteger[] intArray = {5, 1, 4, 2, 8};\nMain&lt;Integer&gt; intMain = new Main&lt;&gt;(intArray);\nInteger[] intSorted = intMain.BubbleSort();\nSystem.out.println(\"Original array: \" + Arrays.toString(intArray));\nSystem.out.println(\"Sorted array: \" + Arrays.toString(intSorted));\nThe output should be:\nOriginal array: [5, 1, 4, 2, 8]\nSorted array: [1, 2, 4, 5, 8]"
  },
  {
    "objectID": "10_bubble_selection_insertion_sort.html#selection-sort",
    "href": "10_bubble_selection_insertion_sort.html#selection-sort",
    "title": "10  Bubble, Selection, and Insertion Sort",
    "section": "10.2 Selection Sort",
    "text": "10.2 Selection Sort\n\n10.2.1 Introduction\nSelection sort is a sorting algorithm that repeatedly finds the smallest element in the unsorted part of the array and swaps it with the first element of the unsorted part. This way, the array is divided into two subarrays: one that contains the sorted elements and one that contains the unsorted elements. The process is repeated until the entire array is sorted. Selection sort is a simple and efficient algorithm that works well for small arrays, but it has a high time complexity of O(n^2) for large arrays.\n\n\n10.2.2 Pseudocode\nThe pseudocode for selection sort is as follows:\nProcedure selection_sort(array, N)\n  array - array of items to be sorted\n  N - size of array\nBegin\n  For i = 1 to N-1\n    Begin\n      Set min = i\n      For j = i+1 to N\n        Begin\n          If array[j] &lt; array[min] Then\n            min = j\n          End If\n        End For\n      // Swap the minimum element with the current element\n      If min != i Then\n        Swap array[min] and array[i]\n      End If\n    End For\nEnd Procedure\n\n\n10.2.3 An Example\n\nLet us see an example of how selection sort works on the following array:\n[64, 25, 12, 22, 11]\nIn the first iteration, the algorithm finds the smallest element (11) in the unsorted part of the array and swaps it with the first element of the unsorted part (64). The array becomes:\n[11, 25, 12, 22, 64]\nIn the second iteration, the algorithm finds the smallest element (12) in the remaining unsorted part of the array and swaps it with the second element of the unsorted part (25). The array becomes:\n[11, 12, 25, 22, 64]\nIn the third iteration, the algorithm finds the smallest element (22) in the remaining unsorted part of the array and swaps it with the third element of the unsorted part (25). The array becomes:\n[11, 12, 22, 25, 64]\nIn the fourth iteration, the algorithm finds the smallest element (25) in the remaining unsorted part of the array and swaps it with the fourth element of the unsorted part (25). The array becomes:\n[11, 12, 22, 25, 64]\nIn the fifth iteration, the algorithm finds the smallest element (64) in the remaining unsorted part of the array and swaps it with the fifth element of the unsorted part (64). The array becomes:\n[11, 12, 22, 25, 64]\nThe array is now sorted and the algorithm terminates.\n\n\n\n10.2.4 Instructions\n\nThe template is a generic class that can sort any type of data that implements the Comparable interface. This means that the data type T must have a method compareTo(T other) that returns a negative, zero, or positive integer depending on whether the current object is less than, equal to, or greater than the other object.\nThe constructor of the class takes an array of type T as a parameter and assigns it to the data field. The data field is the array that needs to be sorted.\nThe SelectionSort() method is where the students need\n\nto write the code for the selection sort algorithm. The method should return a sorted array of type T.\n\nThe students can use the pseudocode provided in the previous section as a guide for writing the code. They need to use a for loop to iterate over the unsorted part of the array, find the minimum element, and swap it with the first element of the unsorted part. They can use the compareTo() method to compare the elements of the array. They can use a temporary variable to store the value of the element to be swapped.\nThe students can test their code by creating an object of the Main class with different types of data, such as integers, strings, or custom objects, and calling the SelectionSort() method on it. They can print the original and sorted arrays to check the output. They can also use different sizes of arrays to see how the algorithm performs."
  },
  {
    "objectID": "10_bubble_selection_insertion_sort.html#insertion-sort",
    "href": "10_bubble_selection_insertion_sort.html#insertion-sort",
    "title": "10  Bubble, Selection, and Insertion Sort",
    "section": "10.3 Insertion Sort",
    "text": "10.3 Insertion Sort\n\n10.3.1 Introduction\nInsertion sort is a simple and adaptive sorting technique that works by inserting each element into its correct position in a sorted subarray. The sorted subarray starts with the first element and grows by one element in each iteration. The element to be inserted is compared with the previous elements in the sorted subarray and swapped if they are larger. This process continues until the element finds its correct position or reaches the beginning of the array.\n\n\n10.3.2 Pseudocode\nThe pseudocode for insertion sort is as follows:\nprocedure insertionSort (A: list of sortable items)\n    n = length (A)\n    for i = 1 to n - 1 do\n        j = i\n        while j &gt; 0 and A [j-1] &gt; A [j] do\n            swap (A [j], A [j-1])\n            j = j - 1\n        end while\n    end for\nend procedure\n\n\n10.3.3 An Example\nConsider the array: [3, 5, 7, 11, 13, 2, 9, 6]\nBelow is a detailed walkthrough of each iteration:\n\nIteration 1: [3, 5, 7, 11, 13, 2, 9, 6]\nThe sorted subarray is [3] and the element to be inserted is 5. Since 5 is larger than 3, no swap is needed.\nIteration 2: [3, 5, 7, 11, 13, 2, 9, 6]\nThe sorted subarray is [3, 5] and the element to be inserted is 7. Since 7 is larger than 5, no swap is needed.\nIteration 3: [3, 5, 7, 11, 13, 2, 9, 6]\nThe sorted subarray is [3, 5, 7] and the element to be inserted is 11. Since 11 is larger than 7, no swap is needed.\nIteration 4: [3, 5, 7, 11, 13, 2, 9, 6]\nThe sorted subarray is [3, 5, 7, 11] and the element to be inserted is 13. Since 13 is larger than 11, no swap is needed.\nIteration 5: [3, 5, 7, 11, 13, 2, 9, 6]\nThe sorted subarray is [3, 5, 7, 11, 13] and the element to be inserted is 2. Since 2 is smaller than 13, 13 and 2 are swapped.\nAfter swapping: [3, 5, 7, 11, 2, 13, 9, 6]\nIteration 6: [3, 5, 7, 11, 2, 13, 9, 6]\nThe sorted subarray is [3, 5, 7, 11, 2, 13] and the element to be inserted is 2. Since 2 is smaller than 11, 11 and 2 are swapped.\nAfter swapping: [3, 5, 7, 2, 11, 13, 9, 6]\nIteration 7: `[3, 5, 7,\n\n2, 11, 13, 9, 6]`\nThe sorted subarray is [3, 5, 7, 2, 11, 13] and the element to be inserted is 2. Since 2 is smaller than 7, 7 and 2 are swapped.\nAfter swapping: [3, 5, 2, 7, 11, 13, 9, 6]\n\nIteration 8: [3, 5, 2, 7, 11, 13, 9, 6]\nThe sorted subarray is [3, 5, 2, 7, 11, 13] and the element to be inserted is 2. Since 2 is smaller than 5, 5 and 2 are swapped.\nAfter swapping: [3, 2, 5, 7, 11, 13, 9, 6]\nIteration 9: [3, 2, 5, 7, 11, 13, 9, 6]\nThe sorted subarray is [3, 2, 5, 7, 11, 13] and the element to be inserted is 2. Since 2 is smaller than 3, 3 and 2 are swapped.\nAfter swapping: [2, 3, 5, 7, 11, 13, 9, 6]\nIteration 10: [2, 3, 5, 7, 11, 13, 9, 6]\nThe sorted subarray is [2, 3, 5, 7, 11, 13] and the element to be inserted is 9. Since 9 is smaller than 13, 13 and 9 are swapped.\nAfter swapping: [2, 3, 5, 7, 11, 9, 13, 6]\nIteration 11: [2, 3, 5, 7, 11, 9, 13, 6]\nThe sorted subarray is [2, 3, 5, 7, 11, 9, 13] and the element to be inserted is 9. Since 9 is smaller than 11, 11 and 9 are swapped.\nAfter swapping: [2, 3, 5, 7, 9, 11, 13, 6]\nIteration 12: [2, 3, 5, 7, 9, 11, 13, 6]\nThe sorted subarray is [2, 3, 5, 7, 9, 11, 13] and the element to be inserted is 9. Since 9 is larger than 7, no swap is needed.\nIteration 13: [2, 3, 5, 7, 9, 11, 13, 6]\nThe sorted subarray is [2, 3, 5, 7, 9, 11, 13] and the element to be inserted is 6. Since 6 is smaller than 13, 13 and 6 are swapped.\nAfter swapping: [2, 3, 5, 7, 9, 11, 6, 13]\n**Iteration\n\n14**: [2, 3, 5, 7, 9, 11, 6, 13]\nThe sorted subarray is [2, 3, 5, 7, 9, 11, 6, 13] and the element to be inserted is 6. Since 6 is smaller than 11, 11 and 6 are swapped.\n*After swapping*: `[2, 3, 5, 7, 9, 6, 11, 13]` \n\nIteration 15: [2, 3, 5, 7, 9, 6, 11, 13]\nThe sorted subarray is [2, 3, 5, 7, 9, 6, 11, 13] and the element to be inserted is 6. Since 6 is smaller than 9, 9 and 6 are swapped.\nAfter swapping: [2, 3, 5, 7, 6, 9, 11, 13]\nIteration 16: [2, 3, 5, 7, 6, 9, 11, 13]\nThe sorted subarray is [2, 3, 5, 7, 6, 9, 11, 13] and the element to be inserted is 6. Since 6 is smaller than 7, 7 and 6 are swapped.\nAfter swapping: [2, 3, 5, 6, 7, 9, 11, 13]\nIteration 17: [2, 3, 5, 6, 7, 9, 11, 13]\nThe sorted subarray is [2, 3, 5, 6, 7, 9, 11, 13] and the element to be inserted is 6. Since 6 is larger than 5, no swap is needed.\n\nFinal array: [2, 3, 5, 6, 7, 9, 11, 13]\nThe sorted subarray is [2, 3, 5, 6, 7, 9, 11, 13] and the array is fully sorted.\n\n\n10.3.4 Instructions\n\nStart by understanding how insertion sort works and what it does to sort an array.\nStudy the given template for the Main class and understand how it uses Java generics to sort an array of any type that implements the Comparable interface.\nImplement the InsertionSort method by following the pseudocode provided above. Students should create a copy of the input array and modify the copy in-place to produce the sorted array.\nTest their implementation using a variety of input arrays and verify that the output is correct.\nEncourage students to optimize their implementation by considering edge cases and identifying areas for improvement.\nRemind students to comment their code and explain their thought process as they work through the assignment. This will help them develop their coding skills and become better problem solvers."
  },
  {
    "objectID": "11_linear_search.html#pseudocode",
    "href": "11_linear_search.html#pseudocode",
    "title": "11  Linear Search",
    "section": "11.1 Pseudocode",
    "text": "11.1 Pseudocode\nThe pseudocode for linear search is straightforward:\nprocedure LinearSearch(A, key)\n    for i from 1 to length(A)\n        if A[i] equals key\n            return i\n    return -1\nThis pseudocode represents a function called LinearSearch that takes two parameters: a list (or array) A and a key to search for in A. The function returns the index of key if found, otherwise it returns -1."
  },
  {
    "objectID": "11_linear_search.html#an-example",
    "href": "11_linear_search.html#an-example",
    "title": "11  Linear Search",
    "section": "11.2 An Example",
    "text": "11.2 An Example\nLet’s consider the following example: We have an array [10, 15, 20, 25, 30, 35] and we want to find the number 20.\n\nIteration 1: The first element is 10. 10 is not equal to 20, so we move on to the next element.\nIteration 2: The second element is 15. 15 is not equal to 20, so we move on to the next element.\nIteration 3: The third element is 20. 20 is equal to 20, so we stop searching. The index of the number 20 is 2 (considering the first index as 0).\n\nTherefore, the linear search algorithm would return 2 as the index of the number 20 in the given array."
  },
  {
    "objectID": "11_linear_search.html#instructions",
    "href": "11_linear_search.html#instructions",
    "title": "11  Linear Search",
    "section": "11.3 Instructions",
    "text": "11.3 Instructions\nHere are some instructions to help you start working on the assignment:\n\nThe template uses Java generics to create a generic class Main that can search for a key element in an array of any type that implements the Comparable interface. Generics are a way of implementing generic programming in Java, which allows you to write code that can work with different types of objects without casting or risking ClassCastException.\nThe constructor of the Main class takes an array of type T as a parameter and assigns it to the array field. The array field is also of type T, which means it can store any type of object that implements Comparable.\nThe LinearSearch method returns an int that is the index of the key element in the array, or -1 if the key element is not found. The method uses a for loop to iterate over the array and compare each element with the key using the compareTo method of the Comparable interface. The compareTo method returns a negative integer, zero, or a positive integer if the current object is less than, equal to, or greater than the specified object. For example, if you want to compare the element at index i with the key, you can write:\nif (array[i].compareTo(key) == 0) {\n    // return the index\n}\nThe LinearSearch method has a time complexity of O(n), where\n\nn is the size of the array, because it may have to scan the entire array to find the key element. 4. To test your code, you can create an object of the Main class with different types of arrays, such as Integer, String, or Double, and call the LinearSearch method on them. You can print the key element and the index returned by the methods to check the output. For example, you can write:\n```java\nInteger[] intArray = {1, 2, 4, 5, 8};\nMain&lt;Integer&gt; intMain = new Main&lt;&gt;(intArray);\nInteger key = 4;\nint linearIndex = intMain.LinearSearch(key);\nSystem.out.println(\"Key element: \" + key);\nSystem.out.println(\"Linear search index: \" + linearIndex);\n```\n\nThe output of the above code should be:\n\n```\nKey element: 4\nLinear search index: 2\n```"
  },
  {
    "objectID": "12_collections_of_data_intro.html#active-memory-in-computers",
    "href": "12_collections_of_data_intro.html#active-memory-in-computers",
    "title": "12  Collections of Data",
    "section": "12.1 (Active) Memory in Computers",
    "text": "12.1 (Active) Memory in Computers\nTo fully grasp the concept of data collections in Java, we must first lay a foundational understanding of memory in computing systems. It is helpful to visualize the computer’s memory as a vast grid, with each cell in the grid being capable of storing a certain number of bits. This simple grid becomes the underpinning infrastructure for data storage, holding variables that serve a multitude of functions in our programming ventures.\n\n\n\n\n\n\n\n\nG\n\n  \n\na\n\n 1  0  1  1  0  1  0  1  0  1  0  1  0  1  0  1  1  0  1  0  1  0  1  0  1  0  1  1  0  1  0  1  0  1  0  1  0  1  0  1  1  0  1  0  1  0  1  0  \n\n\nFigure 12.1: A diagram showing a grid of memory cells, each capable of storing a single bit."
  },
  {
    "objectID": "12_collections_of_data_intro.html#the-concept-of-data-collections",
    "href": "12_collections_of_data_intro.html#the-concept-of-data-collections",
    "title": "12  Collections of Data",
    "section": "12.2 The Concept of Data Collections",
    "text": "12.2 The Concept of Data Collections\nIn programming, the need often arises to handle groups of similar variables. These groups are what we refer to as data collections. A data collection is essentially an assembly of bits in the memory, representing a group of variables. For instance, you could have a collection of four 2-bit variables, which together would take up eight bits of memory. But the behavior of these collections can vary, particularly when considering size constraints."
  },
  {
    "objectID": "12_collections_of_data_intro.html#types-of-collections-fixed-and-dynamic",
    "href": "12_collections_of_data_intro.html#types-of-collections-fixed-and-dynamic",
    "title": "12  Collections of Data",
    "section": "12.3 Types of Collections: Fixed and Dynamic",
    "text": "12.3 Types of Collections: Fixed and Dynamic\nWhen analyzing the size of data collections, we encounter two main types: fixed (or static) and dynamic collections.\nFixed or Static Collections: As the name suggests, in a static collection, the number of variables is constant. This implies that the collection’s size is immutable, and the total memory occupancy remains consistent. An array of 10 integers is a perfect example of a static collection - regardless of the values stored in it, the array always contains ten integers.\n\n\n\n\n\n\n\n\nG\n\n  \n\na\n\n  Data   Data   Data    1   0   0   1   1   1   0   0   1   1   0   1   1   1   1   0   1   0   0   1   0   0   0   0  \n\n\nFigure 12.2: A diagram showing a static collection of data where the five 4-bit variables are placed sequentially in memory.\n\n\n\n\nDynamic Collections: Dynamic collections differ from their static counterparts in that their sizes are subject to change over time. Consider, for example, a collection storing the names of your favorite musicians. As your taste in music evolves, so does this list, reflecting an expansion or contraction based on your preferences. Thus, it is a dynamic collection."
  },
  {
    "objectID": "12_collections_of_data_intro.html#memory-layout-of-collections",
    "href": "12_collections_of_data_intro.html#memory-layout-of-collections",
    "title": "12  Collections of Data",
    "section": "12.4 Memory Layout of Collections",
    "text": "12.4 Memory Layout of Collections\nLet’s take a deeper look into how these collections are stored in memory. Note that the memory allocation of these collections can be influenced by factors such as the variables’ data type and size.\nA common approach to store a collection of two 4-bit numbers is sequentially, positioning them back-to-back. However, this is not the only strategy. Depending on the system’s memory management and data alignment methodologies, ‘padding’ could be introduced, which involves inserting extra bits between variables to align them properly in memory.\nRegardless of these variations, a crucial consideration when working with collections is devising a mechanism to locate the next variable in the sequence. This task is straightforward for static collections due to the known size of each variable. But how about dynamic collections?"
  },
  {
    "objectID": "12_collections_of_data_intro.html#tackling-dynamic-collections",
    "href": "12_collections_of_data_intro.html#tackling-dynamic-collections",
    "title": "12  Collections of Data",
    "section": "12.5 Tackling Dynamic Collections",
    "text": "12.5 Tackling Dynamic Collections\nDynamic collections present an interesting challenge due to their mutable size. One way to navigate this issue is by earmarking a portion of the memory to store the length of the collection. For instance, the first four bits could be utilized to denote the size of the collection.\nBy adopting this method, we can efficiently locate subsequent data elements in our collection and discern the collection’s end point. Bear in mind that the data in collections doesn’t have to be stored adjacently. We can opt to pad each element with a fixed number of bits. As long as the padding size remains consistent, we can still locate the next piece of data.\n\n\n\n\n\n\n\n\nG\n\n  \n\na\n\n  Length   Data   Pad   Data   Pad    0   0   1   0   0   1   0   0   1   1   0   0   1   0   0   0   0   1   0   0   1   1   0   0  0  0  0  0  0  0  0  0  0  0  0  0  \n\n\nFigure 12.3: A diagram showing a dynamic collection of data where the first 4 bits represent the length of the collection, and the next few bits are the collection of variables. Each variable will be separated by 2-bit padding."
  },
  {
    "objectID": "12_collections_of_data_intro.html#memory-layout-and-program-performance",
    "href": "12_collections_of_data_intro.html#memory-layout-and-program-performance",
    "title": "12  Collections of Data",
    "section": "12.6 Memory Layout and Program Performance",
    "text": "12.6 Memory Layout and Program Performance\nThe manner in which data collections are stored in memory can have a significant impact on the performance of a program. For instance, accessing memory sequentially (in a pattern that matches the memory layout of the collection) allows the system to leverage cache lines, which are blocks of memory that are read into the CPU’s cache. Due to the way modern CPUs are designed, once a part of the memory is read, an entire cache line (usually 64 or 128 bytes) is loaded into the cache. Thus, sequential access often results in fewer cache misses, which improves the performance of your program.\nIn contrast, random access to memory can lead to frequent cache misses, as each access may require loading a different cache line into the CPU cache, slowing down your program. Therefore, understanding the memory layout of your data collections and designing your program to access data in a manner that complements this layout can lead to significant performance improvements.\nSo, as you delve deeper into the world of data collections, remember that your approach to handling memory can either elevate or hinder your program’s efficiency. The choice, as always, is in your hands!"
  },
  {
    "objectID": "12_collections_of_data_intro.html#summary-and-conclusion",
    "href": "12_collections_of_data_intro.html#summary-and-conclusion",
    "title": "12  Collections of Data",
    "section": "12.7 Summary and Conclusion",
    "text": "12.7 Summary and Conclusion\nThis chapter provided a comprehensive overview of data collections in programming, focusing mainly on their storage in computer memory. It began by explaining the fundamental understanding of memory in computing systems, portraying it as a vast grid where each cell stores a certain number of bits. This concept helped set the groundwork for understanding how data collections, which are groups of similar variables, are stored in memory.\nThe chapter then discussed two main types of data collections: fixed (static) and dynamic collections. While fixed collections have a constant number of variables and thus a constant memory size, dynamic collections have a variable size that can change over time. An illustration was given of both types to aid understanding.\nThe memory layout of these collections was then delved into, noting that how these collections are stored in memory can be influenced by the variables’ data type and size. The chapter also explained the strategies for accessing variables in both static and dynamic collections.\nMoreover, the chapter emphasized the significant impact of the memory layout of data collections on program performance. Sequential memory access in line with the memory layout could leverage cache lines and enhance the program’s performance, while random access could lead to frequent cache misses and slow down the program.\nIn conclusion, understanding data collections, how they are stored in memory, and how to access this data efficiently are critical aspects of programming. By considering these factors, programmers can significantly influence the performance of their programs. Thus, the mastery of data collections is not only essential for writing efficient code but is also a vital skill for optimizing the overall performance of a program."
  },
  {
    "objectID": "13_operations_on_lists.html#adding-data-to-a-list",
    "href": "13_operations_on_lists.html#adding-data-to-a-list",
    "title": "13  Operations on Lists",
    "section": "13.1 Adding Data to a List",
    "text": "13.1 Adding Data to a List\nAs you may recall, a list is a collection of data elements. A key operation, therefore, is the ability to add data to this collection. Let’s visualize a simple list:\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\nThere are multiple ways to insert new data into this list. For instance, you might want to add a new element, say “X”, at the end:\n\n\n\n1\n2\n3\n4\n5\n6\nX\n\n\n\n\n\nAlternatively, you could place “X” at the beginning:\n\n\n\nX\n1\n2\n3\n4\n5\n6\n\n\n\n\n\nOr perhaps insert “X” somewhere in the middle:\n\n\n\n1\n2\n3\nX\n4\n5\n6\n\n\n\n\n\nThere are three distinct methods to add data to our list, and each method may serve different needs in your program. Let’s discuss how these operations are implemented in the List ADT provided by the Java standard library, java.util.List.\n\n13.1.1 Appending an Element\nAppending an element to the end of a list is perhaps the most straightforward method of adding data. The add method provided by java.util.List is used for this operation. Here is how you might use it:\nlistObject.add(itemToAdd);\n\n\n13.1.2 Prepending an Element\nInserting an element at the beginning of a list is also a common operation, often called “prepending”. Though the java.util.List interface doesn’t provide a prepend method, we can use the overloaded add method, which accepts an index and an element. To prepend, we simply pass 0 as the index:\nlistObject.add(0, itemToAdd);\n\n\n13.1.3 Inserting an Element at a Specific Index\nAs hinted in the prepend operation, the add method in java.util.List allows for adding an element at any index in the list:\nlistObject.add(index, itemToAdd);\n\n\n13.1.4 Adding All Elements from Another List\nSometimes, you may want to merge one list into another. The addAll method allows you to add all the elements from another list to the end of the current list:\nlistObject.addAll(anotherList);"
  },
  {
    "objectID": "13_operations_on_lists.html#the-role-of-abstract-data-types-adts",
    "href": "13_operations_on_lists.html#the-role-of-abstract-data-types-adts",
    "title": "13  Operations on Lists",
    "section": "13.2 The Role of Abstract Data Types (ADTs)",
    "text": "13.2 The Role of Abstract Data Types (ADTs)\nWe’ve mentioned the term “Abstract Data Type” (ADT) several times so far, but what does it really mean? An ADT is a high-level description of a collection of data and the operations that can be performed on that data. It is “abstract” in that it describes what operations are to be done but not how these operations will be implemented.\nIn the context of lists,\nthe List ADT defines a list’s behavior. For instance, we can append, prepend, or insert an element at a specific index, but we don’t care about how these operations are performed.\nWhy do we need ADTs? ADTs allow us to abstract away the details of the data structure’s implementation. They encapsulate the data and provide a well-defined interface to interact with it, ensuring that data remains consistent and operations on the data are predictable. Moreover, by using ADTs, different implementations of the same type of data structure can be swapped seamlessly, without changing the code that uses the data structure.\nIn your journey with Java so far, you’ve only used one kind of list – the ArrayList. As you progress, you’ll encounter other types of lists, like LinkedLists and Stacks, each with its own strengths, weaknesses, and suitable use cases. All these list types adhere to the same List ADT, allowing us to switch from one type to another depending on our requirements, while our code remains largely the same.\nIn the next sections of this course, we will dive deeper into different types of lists and their unique characteristics. For now, the important takeaway is that ADTs allow us to focus on what operations we want to perform without worrying about how they are implemented.\n\n13.2.1 Summary of Addition Operations\nHere is a summary of the adding operations available in the java.util.List ADT:\n\n\n\nModifier and Type\nMethod and Description\n\n\n\n\nboolean\nadd(E e) Appends the specified element to the end of this list (optional operation).\n\n\nvoid\nadd(int index, E element) Inserts the specified element at the specified position in this list (optional operation).\n\n\nboolean\naddAll(Collection&lt;? extends E&gt; c) Appends all of the elements in the specified collection to the end of this list, in the order that they are returned by the specified collection’s iterator (optional operation).\n\n\nboolean\naddAll(int index, Collection&lt;? extends E&gt; c) Inserts all of the elements in the specified collection into this list at the specified position (optional operation)."
  },
  {
    "objectID": "13_operations_on_lists.html#removing-data-from-our-list",
    "href": "13_operations_on_lists.html#removing-data-from-our-list",
    "title": "13  Operations on Lists",
    "section": "13.3 Removing data from our List",
    "text": "13.3 Removing data from our List"
  },
  {
    "objectID": "13_operations_on_lists.html#removing-data-from-a-list",
    "href": "13_operations_on_lists.html#removing-data-from-a-list",
    "title": "13  Operations on Lists",
    "section": "13.4 Removing Data from a List",
    "text": "13.4 Removing Data from a List\nHaving explored the addition of data to a list, let’s now shift our attention to its counterpart: removing data from the list. Remember our list from the previous section?\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\nLet’s consider different ways to remove data from this list.\n\n13.4.1 Removing an Element at a Specific Index\nSometimes, you might need to remove an item from a specific position within the list. Java’s List ADT provides the remove(int index) method for this purpose. This method removes the element at the specified position:\nlistObject.remove(index);\n\n\n13.4.2 Removing the First Occurrence of an Element\nAt times, you might not know (or care about) the index of the element you want to remove, but you know the value of the element. In such cases, you can use the remove(Object o) method, which removes the first occurrence of the specified element from the list:\nlistObject.remove(objectToRemove);\n\n\n13.4.3 Removing All Elements from Another List\nConsider that you have two lists, and you want to remove all elements in the second list from the first one. You can use the removeAll(Collection&lt;?&gt; c) method, which removes from the current list all of its elements that are contained in the specified collection:\nlistObject.removeAll(anotherList);\n\n\n13.4.4 Removing All Elements from the List\nIn some scenarios, you might want to clear your list entirely. The clear() method comes in handy for this, as it removes all elements from the list:\nlistObject.clear();\nLike the addition operations, these removal operations offer different ways to manage the data in our list, providing flexibility based on the specific needs of our program.\n\n\n13.4.5 Summary of Removal Operations\nHere is a summary of the removal operations available in the java.util.List ADT:\n\n\n\nModifier and Type\nMethod and Description\n\n\n\n\nE\nremove(int index) Removes the element at the specified position in this list (optional operation).\n\n\nboolean\nremove(Object o) Removes the first occurrence of the specified element from this list, if it is present (optional operation).\n\n\nboolean\nremoveAll(Collection&lt;?&gt; c) Removes from this list all of its elements that are contained in the specified collection (optional operation).\n\n\nvoid\nclear() Removes all of the elements from this list (optional operation)."
  },
  {
    "objectID": "13_operations_on_lists.html#searching-for-data-in-our-list",
    "href": "13_operations_on_lists.html#searching-for-data-in-our-list",
    "title": "13  Operations on Lists",
    "section": "13.5 Searching for data in our List",
    "text": "13.5 Searching for data in our List"
  },
  {
    "objectID": "13_operations_on_lists.html#searching-in-a-list",
    "href": "13_operations_on_lists.html#searching-in-a-list",
    "title": "13  Operations on Lists",
    "section": "13.6 Searching in a List",
    "text": "13.6 Searching in a List\nBeyond just adding and removing data from our list, we often need to find or check for the existence of certain elements in our list. Java’s List ADT provides several methods for such search operations. Let’s dive into these operations using our familiar list:\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\n\n13.6.1 Checking If an Element Exists in the List\nSometimes, all we need to know is whether a certain element exists in our list. The contains(Object o) method serves this purpose, returning true if this list contains the specified element, and false otherwise:\nboolean contains = listObject.contains(objectToCheck);\n\n\n13.6.2 Checking If All Elements of Another Collection Exist in the List\nIf you have a collection of elements and you want to check whether all these elements exist in your list, you can use the containsAll(Collection&lt;?&gt; c) method. It returns true if the list contains all of the elements of the specified collection:\nboolean containsAll = listObject.containsAll(anotherCollection);\n\n\n13.6.3 Finding the Index of an Element\nIf you want to find the position of a certain element in your list, Java provides the indexOf(Object o) method. This method returns the index of the first occurrence of the specified element in this list, or -1 if this list does not contain the element:\nint index = listObject.indexOf(objectToFind);\n\n\n13.6.4 Finding the Last Index of an Element\nIn a list with duplicate elements, you might be interested in finding the last occurrence of an element. In this case, you can use the lastIndexOf(Object o) method, which returns the index of the last occurrence of the specified element in this list, or -1 if this list does not contain the element:\nint lastIndex = listObject.lastIndexOf(objectToFind);\n\n\n13.6.5 Summary of Search Operations\nHere is a summary of the search operations available in the java.util.List ADT:\n\n\n\nModifier and Type\nMethod and Description\n\n\n\n\nboolean\ncontains(Object o) Returns true if this list contains the specified element.\n\n\nboolean\ncontainsAll(Collection&lt;?&gt; c) Returns true if this list contains all of the elements of the specified collection.\n\n\nint\nindexOf(Object o) Returns the index of the first occurrence of the specified element in this list, or -1 if this list does not contain the element.\n\n\nint\nlastIndexOf(Object o) Returns the index of the last occurrence of the specified element in this list, or -1 if this list does not contain the element.\n\n\n\nSearch operations are a fundamental part of manipulating lists. They allow us to locate and verify the presence of data, enhancing our ability to interact with our list and derive meaningful results."
  },
  {
    "objectID": "13_operations_on_lists.html#miscellaneous-operations-on-a-list",
    "href": "13_operations_on_lists.html#miscellaneous-operations-on-a-list",
    "title": "13  Operations on Lists",
    "section": "13.7 Miscellaneous Operations on a List",
    "text": "13.7 Miscellaneous Operations on a List\nIn addition to adding, removing, and searching elements in our list, Java’s List ADT provides a variety of other useful operations. These allow us to manipulate and inquire our list in a more sophisticated manner.\n\n13.7.1 Accessing an Element\nSometimes, we need to retrieve the element at a specific position in our list without removing it. The get(int index) method provides this functionality, returning the element at the specified position:\nE element = listObject.get(index);\n\n\n13.7.2 Modifying an Element\nWhat if we need to change an element at a specific position? Java provides the set(int index, E element) method. This method replaces the element at the specified position in this list with the specified element:\nlistObject.set(index, newElement);\n\n\n13.7.3 Determining the Size of the List\nWhen working with lists, it’s often necessary to know the number of elements present. The size() method returns the number of elements in this list:\nint size = listObject.size();\n\n\n13.7.4 Converting the List to an Array\nOn occasion, we may need to convert our list into an array. The toArray() method fulfills this purpose, returning an array containing all the elements in this list in proper sequence:\nObject[] array = listObject.toArray();\n\n\n13.7.5 Summary of Miscellaneous Operations\nHere is a summary of the miscellaneous operations available in the java.util.List ADT:\n\n\n\nModifier and Type\nMethod and Description\n\n\n\n\nE\nget(int index) Returns the element at the specified position in this list.\n\n\nE\nset(int index, E element) Replaces the element at the specified position in this list with the specified element (optional operation).\n\n\nint\nsize() Returns the number of elements in this list.\n\n\nObject[]\ntoArray() Returns an array containing all of the elements in this list in proper sequence (from first to last element).\n\n\n\nThese miscellaneous operations on a list help us to manipulate and utilize our lists effectively. In the subsequent sections, we will delve deeper into different types of lists and how these operations can be applied differently. Stay tuned!"
  },
  {
    "objectID": "13_operations_on_lists.html#summary-and-conclusion",
    "href": "13_operations_on_lists.html#summary-and-conclusion",
    "title": "13  Operations on Lists",
    "section": "13.8 Summary and Conclusion",
    "text": "13.8 Summary and Conclusion\nThis chapter aimed to offer a comprehensive understanding of operations that can be performed on lists using the Java programming language. It introduced key manipulations including adding, removing, and searching for elements in a list, as well as several other useful list operations.\nWe began by delving into the process of adding elements to a list, where we considered various insertion methods such as appending, prepending, and inserting at a specific index. We also explored the ability to add all elements from another list into the current list. These addition operations provide flexibility and adaptability in managing data in our list as required by the program.\nNext, we moved on to the concept of removing elements from a list. We investigated removing elements at specific indices, removing the first occurrence of an element, and removing all elements from a list or all elements present in another list. These removal operations are just as vital in data management, providing diverse ways to regulate the contents of our list.\nSubsequently, we analyzed search operations within a list. These operations are crucial in data retrieval, determining the existence and position of data elements in our list. Methods like contains(Object o), containsAll(Collection&lt;?&gt; c), indexOf(Object o), and lastIndexOf(Object o) were discussed in depth.\nFurthermore, we examined miscellaneous operations that provide additional functionality in our list. These include accessing and modifying elements at specific indices, determining the size of the list, and converting the list into an array.\nThroughout our exploration, we highlighted the significant role of Abstract Data Types (ADTs). By standardizing the operations that can be performed on a list, the List ADT allows us to focus on what we want to accomplish without concerning ourselves with the underlying implementation details. This abstraction brings forth code consistency, reliability, and the potential for seamless integration of different list implementations.\nIn conclusion, understanding and utilizing list operations in Java is fundamental for effective data management in programming. The breadth of operations available affords us immense flexibility in manipulating lists to align with our program’s requirements. As we proceed in this course, we will delve deeper into the different types of lists and their unique characteristics, all the while, leveraging the power and versatility of the List ADT."
  },
  {
    "objectID": "15_implementing_linkedlists.html#understanding-references-in-java",
    "href": "15_implementing_linkedlists.html#understanding-references-in-java",
    "title": "15  Implementing Linked Lists",
    "section": "15.1 Understanding References in Java",
    "text": "15.1 Understanding References in Java\nIn Java programming, the statement that “all objects are references” is a cornerstone. Understanding what this statement means is a fundamental step in mastering Java.\nIn Java, when we say an object is a reference, it means that the object acts as an address pointer, referencing a specific location in memory. This location is where the actual data associated with that object is stored.\nConsider your computer’s memory as a large grid, as illustrated in Table 6.1. Each cell can store some data.\n\nRepresentation of computer’s memory\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\nFor any object in Java, it’s not the data of the object that is directly stored in the variable, but rather the reference to the data."
  },
  {
    "objectID": "15_implementing_linkedlists.html#memory-allocation-for-java-objects",
    "href": "15_implementing_linkedlists.html#memory-allocation-for-java-objects",
    "title": "15  Implementing Linked Lists",
    "section": "15.2 Memory Allocation for Java Objects",
    "text": "15.2 Memory Allocation for Java Objects\nConsider the following example, in which we have a Dog class.\npublic class Dog {\n    String name;\n    int age;\n}\nInitially, the Dog class doesn’t occupy any memory space. It simply serves as a blueprint for creating Dog objects.\nNow, let’s create a new instance of Dog and assign it to the variable myDog:\nDog myDog = new Dog();\nThis operation performs two main actions:\n\nMemory is allocated for a new Dog object.\nThe address of that memory is stored in the myDog variable.\n\n\n\n\nDog Object Memory Allocation\n\n\nSo, myDog is essentially a pointer that points to the location in memory where the Dog object’s data is stored."
  },
  {
    "objectID": "15_implementing_linkedlists.html#objects-interacting-through-references",
    "href": "15_implementing_linkedlists.html#objects-interacting-through-references",
    "title": "15  Implementing Linked Lists",
    "section": "15.3 6.3 Objects Interacting Through References",
    "text": "15.3 6.3 Objects Interacting Through References\nNow, let’s consider a scenario where we have two classes, Dog and HomeOwner. The Dog class is the same as before, but the HomeOwner class is a bit more complex.\npublic class HomeOwner\n\n {\n    String name;\n    Dog pet;\n}\nHere, the HomeOwner class has a pet field of type Dog. This means a HomeOwner can own a Dog.\nNow, suppose we have a constructor in the Dog class that accepts name and age as parameters, and a default constructor in the HomeOwner class:\npublic class Dog {\n    String name;\n    int age;\n    \n    public Dog(String name, int age) {\n        this.name = name;\n        this.age = age;\n    }\n}\n\npublic class HomeOwner {\n    String name;\n    Dog pet;\n\n    public HomeOwner() {\n        this.name = \"Timothy\";\n        this.pet = new Dog(\"Timothy Jr.\", 5);\n    }\n}\nWhen we create an instance of HomeOwner:\nHomeOwner owner = new HomeOwner();\n\n\n\nHomeOwner Object Creation\n\n\nA new Dog object is created and assigned to the pet field of the HomeOwner object. Importantly, the pet field stores the reference to the Dog object, not the actual Dog object data.\n\n\n\nDog Object Creation and Assignment\n\n\nThis creates a chain of references in memory, with the owner variable pointing to the HomeOwner object, which in turn has a reference to the Dog object.\n\n\n\nChain of References\n\n\nThis understanding of references and memory allocation sets the foundation for how linked lists work in Java, which we will explore in the upcoming sections."
  },
  {
    "objectID": "15_implementing_linkedlists.html#implementing-linked-lists-in-java",
    "href": "15_implementing_linkedlists.html#implementing-linked-lists-in-java",
    "title": "15  Implementing Linked Lists",
    "section": "15.4 Implementing Linked Lists in Java",
    "text": "15.4 Implementing Linked Lists in Java\nAbsolutely! Last time, we used arrays to construct lists, but we can indeed build a list using the concept of references we have just discussed.\nWe’ll start by creating a class where we’ll store a reference to the first piece of data in our List. Then, our first piece of data will contain a reference to the next piece, and the next piece will point to the one following it, and so on.\n\n\n\nLinked List\n\n\nThis gives us the following structure:\nclass LinkedList {\n  firstPieceOfData head;\n}\n\nclass firstPieceOfData {\n  nextPieceOfData next;\n}\nHowever, there’s no actual data in this structure yet.\nSo, how about this?\nclass LinkedList {\n  firstPieceOfData head;\n}\n\nclass firstPieceOfData {\n  nextPieceOfData next;\n  actualData data;\n}\n\nclass nextPieceOfData {\n  nextPieceOfData next;\n  actualData data;\n}\nTo ensure our list is flexible and can store any type of data, we should make it generic:\nclass LinkedList&lt;T&gt; {\n  firstPieceOfData&lt;T&gt; head;\n}\n\nclass firstPieceOfData&lt;T&gt; {\n  nextPieceOfData&lt;T&gt; next;\n  T data;\n}\n\nclass nextPieceOfData&lt;T&gt; {\n  nextPieceOfData&lt;T&gt; next;\n  T data;\n}\nHere, &lt;T&gt; is a type parameter that allows us to define a class with placeholders for the types they use, which we can specify when we create an instance of the class.\nWe can also observe that firstPieceOfData and nextPieceOfData are structurally the same, as both classes store a reference to an object that holds data, and both hold some actual data. We can therefore simplify our structure by using a single class for both, like so:\nclass LinkedList&lt;T&gt; {\n  Node&lt;T&gt; head;\n}\n\nclass Node&lt;T&gt; {\n  Node&lt;T&gt; next;\n  T data;\n}\nThe list we’ve created is known as a singly linked list. In some algorithms, we may want to access the previous node. In that case, we can store a reference to the previous node as well, creating what’s known as a doubly linked list:\nclass LinkedList&lt;T&gt; {\n  Node&lt;T&gt; head;\n  Node&lt;T&gt; tail;\n}\n\nclass Node&lt;T&gt; {\n  Node&lt;T&gt; next;\n  Node&lt;T&gt; prev;\n  T data;\n}\nWhen we create a Node object, the constructor should initialize the next reference to null and data to the data we pass in. This is because at that point, we don’t yet know what the next node will be, but we do know what data we want to store.\nclass SinglyLinkedNode&lt;T&gt; {\n  private SinglyLinkedNode&lt;T&gt; next;\n  private T data;\n\n  public SinglyLinkedNode(T data) {\n    this.data = data;\n    this.next = null;\n  }\n}\nWe also renamed the Node class to SinglyLinkedNode to make it clear that this is for a singly-linked list.\nThe constructor for SinglyLinkedList will just set the head to null, initializing it as an empty list.\nclass SinglyLinkedList&lt;T&gt; {\n  private SinglyLinkedNode&lt;T&gt; head;\n\n  public SinglyLinkedList() {\n    this.head = null;\n  }\n}"
  },
  {
    "objectID": "15_implementing_linkedlists.html#adding-to-a-singly-linked-list",
    "href": "15_implementing_linkedlists.html#adding-to-a-singly-linked-list",
    "title": "15  Implementing Linked Lists",
    "section": "15.5 6.5 Adding to a Singly Linked List",
    "text": "15.5 6.5 Adding to a Singly Linked List\nThe SinglyLinkedNode\nclass needs getter and setter methods for accessing and modifying its private data and next fields.\nclass SinglyLinkedNode&lt;T&gt; {\n  private SinglyLinkedNode&lt;T&gt; next;\n  private T data;\n\n  public SinglyLinkedNode(T data) {\n    this.data = data;\n    this.next = null;\n  }\n\n  public SinglyLinkedNode&lt;T&gt; getNext() {\n    return this.next;\n  }\n\n  public void setNext(SinglyLinkedNode&lt;T&gt; next) {\n    this.next = next;\n  }\n\n  public T getData() {\n    return this.data;\n  }\n\n  public void setData(T data) {\n    this.data = data;\n  }\n}\nWith the getters and setters in place, we can now turn our attention to adding a new node to the list. Here’s what we need to do when the list is empty:\npublic class SinglyLinkedList&lt;T&gt; {\n  private SinglyLinkedNode&lt;T&gt; head;\n\n  public SinglyLinkedList() {\n    this.head = null;\n  }\n\n  public void add(T data) {\n    head = new SinglyLinkedNode&lt;T&gt;(data);\n  }\n}\nWhen we’re adding elements or nodes to our singly linked list, we’ve seen that for the first addition, we simply set the head to a new node with the provided data. This is essentially the creation of the first element or node in our list.\nLet’s take a moment to visualize this scenario:\nhead --&gt; [A]\nIn this ASCII diagram, the --&gt; represents the link from the head to the first node A.\nNow, when we’re adding a second element, we must be careful. We can’t simply overwrite the head again, as it contains our first piece of data. Instead, we want to add our new node at the next available position.\nWe achieve this by setting the next member of the head node to point to our new data, like so:\npublic void add(T data) {\n  if (head == null) {\n    head = new SinglyLinkedNode&lt;T&gt;(data);\n  } else {\n    head.setNext(new SinglyLinkedNode&lt;T&gt;(data));\n  }\n}\nThe updated state of the list is now:\nhead --&gt; [A] --&gt; [B]\nBut what if we want to add a third node? Using the code above, we would inadvertently overwrite the link from A to B, and instead, link A directly to the new node.\nWe have to revise our strategy. What if we checked whether the next of the head was null and then added our new node there?\npublic void add(T data) {\n  if (head == null) {\n    head = new SinglyLinkedNode&lt;T&gt;(data);\n  } else if (head.getNext() == null) {\n    head.setNext(new SinglyLinkedNode&lt;T&gt;(data));\n  } else {\n    head.getNext().setNext(new SinglyLinkedNode&lt;T&gt;(data));\n  }\n}\nThis would result in:\nhead --&gt; [A] --&gt; [B] --&gt; [C]\nThat seems to work, right? But this code has limitations; it only accommodates the addition of up to three nodes.\nLet’s observe what happens when we add a fourth node using this code. We’d end up with:\nhead --&gt; [A] --&gt; [B] --&gt; [D]\nThe third node, [C], disappears! That’s not what we want. We need a way to add nodes to our list, regardless of its current length. Can you see a pattern forming in how we’re adding nodes? How about a loop to iterate over the list until we find a node with a null next field?\npublic void add(T data) {\n  if (head == null) {\n    head = new SinglyLinkedNode&lt;T&gt;(data);\n  } else {\n    SinglyLinkedNode&lt;T&gt; current = head;\n\n    while (current.getNext() != null) {\n      current = current.getNext();\n    }\n\n    current.setNext(new SinglyLinkedNode&lt;T&gt;(data));\n  }\n}\nNow, each time we call add(), we start at the head and follow the next references until we find a node where next is null. We then set that node’s next to the new node, effectively adding it to the end of the list.\nAnd there you have it! We’ve cracked the code for adding nodes to a singly linked list, regardless of its size."
  },
  {
    "objectID": "15_implementing_linkedlists.html#searching-a-singly-linked-list",
    "href": "15_implementing_linkedlists.html#searching-a-singly-linked-list",
    "title": "15  Implementing Linked Lists",
    "section": "15.6 Searching a Singly Linked List",
    "text": "15.6 Searching a Singly Linked List\nHaving explored the intricacies of adding elements to our singly linked list, it’s only logical to now ask, “How can we find an element in our list?” After all, what good is storing data if we can’t retrieve it?\nLet’s delve into the process of developing a search method that behaves similarly to the indexOf method in the Java standard library. Our search method should return the index of the first occurrence of a given element in the list and -1 if the element is not found.\nBefore we get started, there’s an important update we need to make to our generics. We’ll be comparing objects in our list to the search target, which means we need to ensure these objects are comparable. To do this, we’ll update T to T extends Comparable&lt;T&gt;. This ensures that the type T implements the Comparable interface, providing us with the ability to compare objects of this type.\nclass SinglyLinkedNode&lt;T extends Comparable&lt;T&gt;&gt; {\n  private SinglyLinkedNode&lt;T&gt; next;\n  private T data;\n\n  //...\n}\n\nclass SinglyLinkedList&lt;T extends Comparable&lt;T&gt;&gt; {\n  private SinglyLinkedNode&lt;T&gt; head;\n\n  //...\n}\nAlright, with this adjustment in place, let’s get to developing our search method!\nIf you reflect on how we traverse our linked list, it may become clear that the same process can be utilized for searching. We start at the head, and we progress through the list via the next pointers until we either find our target or reach the end of the list.\nHere’s a potential implementation for the search method:\npublic int search(T target) {\n  SinglyLinkedNode&lt;T&gt; current = head;\n  int index = 0;\n\n  while (current != null) {\n    if (current.getData().compareTo(target) == 0) {\n      return index;\n    }\n    index++;\n    current = current.getNext();\n  }\n\n  return -1;  // Target not found\n}\nLet’s examine this piece by piece. We start by creating a reference to the head of our list and initializing an index variable at 0. We then enter a while loop that will continue as long as current is not null, effectively iterating over the entire list.\nInside the loop, we compare the data of the current node to our target using the compareTo method, which is available to us thanks to our Comparable constraint. If the data matches our target (compareTo returns 0), we’ve found our target and we return the current index.\nIf the data does not match our target, we increment our index and move to the next node in the list. If we reach the end of the list without finding our target, the method returns -1, indicating the target is not in the list.\nAnd that’s how we develop a search method for our singly linked list, providing us with the means to locate and retrieve data efficiently!"
  },
  {
    "objectID": "15_implementing_linkedlists.html#removing-from-a-singly-linked-list",
    "href": "15_implementing_linkedlists.html#removing-from-a-singly-linked-list",
    "title": "15  Implementing Linked Lists",
    "section": "15.7 Removing from a Singly Linked List",
    "text": "15.7 Removing from a Singly Linked List\nJust as we learned how to add elements to a singly linked list, let’s now turn our attention to the process of removing elements. As with adding elements, we’ll start with a simpler case and gradually address more complex scenarios. We’ll design a removeLast method, which removes the last node from the list.\nThere are three scenarios we need to consider:\n\nThe list is empty.\nThe list contains only one element.\nThe list contains more than one element.\n\n\n15.7.1 Case 1: Empty Linked List\nThe simplest scenario to handle is an empty list. If the list is empty, we have nothing to remove. Here’s a simple starting point for our removeLast method:\npublic void removeLast() {\n  if (head == null) {\n    return; // Nothing to remove.\n  }\n}\nThis code handles an empty list by simply returning without doing anything.\n\n\n15.7.2 Case 2: Linked List with One Element\nNow, let’s consider the case where our list contains only one element.\nTo remove the only element from the list, we would set head to null, effectively removing the link to that node.\nHere’s how our removeLast method looks now:\npublic void removeLast() {\n  if (head == null) {\n    return; // Nothing to remove.\n  } else if (head.getNext() == null) {\n    head = null; // Remove the only node in the list.\n  }\n}\n\n\n15.7.3 Case 3: Linked List with More Than One Element\nWhen the list contains more than one element, we must find the last node and the node before it. Why? Because to remove the last node, we must set the next field of the node before it to null.\nLet’s think about how we might achieve this. We can’t simply set head.getNext() = null like we did with the one-element list, because this would leave us with only the first node.\nInstead, we could use a loop similar to the one in the add method, but with a twist. Instead of stopping when current.getNext() == null, which would leave us at the last node, we stop when current.getNext().getNext() == null, which will leave us at the second-to-last node.\npublic void removeLast() {\n  if (head == null) {\n    return; // Nothing to remove.\n  } else if (head.getNext() == null) {\n    head = null; // Remove the only node in the list.\n  } else {\n    SinglyLinkedNode&lt;T&gt; current = head;\n    while (current.getNext().getNext() != null) {\n      current = current.getNext();\n    }\n    current.setNext(null); // Remove the last node from the list.\n  }\n}\nNow, we have a removeLast method that handles any size list!\n\n\n15.7.4 Moving to remove(int i)\nRemoving the last node from a list is a good starting point, but what if we want to remove an arbitrary node at index i?\nFor this, we would need to iterate over the nodes until we reach the (i-1)th node (just before the node we want to remove), then update its next field to skip over the ith node and link to the (i+1)th node.\nHowever, this requires careful checking of edge cases, such as when i is 0 (requiring us to update the head), or when i is greater than the size of the list.\nThis will be our next challenge to tackle! Let’s see how we can go about creating a remove(int i) method, which will remove a node at a given index from our singly linked list.\nAs previously mentioned, our approach will be to traverse the list until we reach the (i-1)th node, and then adjust its next field to skip the ith node, effectively removing it from the list.\nHere’s a basic version of this method:\npublic void remove(int i) {\n  if (i == 0) {\n    head = head.getNext(); // If the node to remove is the head, move the head to the next node.\n  } else {\n    SinglyLinkedNode&lt;T&gt; current = head;\n    for (int j = 0; j &lt; i - 1; j++) {\n      current = current.getNext();\n    }\n    current.setNext(current.getNext().getNext());\n  }\n}\nThis is a simple implementation, but it lacks protection against edge cases. For instance, what happens if i is negative or if it’s larger than the size of the list?\nWe can address these cases by adding a few checks to our method:\npublic void remove(int i) {\n  if (i &lt; 0) {\n    return; // Do nothing for negative index.\n  } else if (i == 0) {\n    head = head.getNext(); // If the node to remove is the head, move the head to the next node.\n  } else {\n    SinglyLinkedNode&lt;T&gt; current = head;\n    for (int j = 0; j &lt; i - 1; j++) {\n      if (current.getNext() == null) {\n        return; // If the index is out of range, do nothing.\n      }\n      current = current.getNext();\n    }\n    if (current.getNext() != null) {\n      current.setNext(current.getNext().getNext());\n    }\n  }\n}\nThis updated method takes care of any negative index or index that’s out of range by simply returning without making any changes. If the index is zero, we update the head of the list to be the next node. If the index is within the range of the list, we find the node at (i-1) and update its next reference to skip over the ith node.\nAnd with that, we’ve covered the basics of how to add and remove nodes from a singly linked list!\n\n\n15.7.5 Summary and Conclusion\nIn this chapter, we learned how to implement linked lists in Java using references and objects. We saw how references can point to other objects in memory and how we can use them to create nodes that store data and link to other nodes. We also learned how to define a class for a singly linked list that contains a reference to the first node, called the head, and methods for adding, searching, and removing elements. We compared the advantages and disadvantages of linked lists with array-backed lists and discussed some applications of linked lists in real-world problems. We concluded that linked lists are a dynamic and flexible data structure that can grow and shrink as needed, but they also have some drawbacks such as extra memory overhead, lack of random access, and potential memory leaks."
  },
  {
    "objectID": "21_graphs.html#background-and-motivation",
    "href": "21_graphs.html#background-and-motivation",
    "title": "16  The Graph Data Structure",
    "section": "16.1 Background and Motivation",
    "text": "16.1 Background and Motivation\nImagine you want to represent the connections between you and your Instagram followers in a data structure. The diagram below (Figure 16.1) shows a simple representation of your followers as numbered vertices and the edges represent the connections between you and your followers.\n\n\n\n\n\n\n\n\nG\n\n  \n\nYou\n\n You   \n\n1\n\n 1   \n\nYou–1\n\n   \n\n2\n\n 2   \n\nYou–2\n\n   \n\n3\n\n 3   \n\nYou–3\n\n   \n\n4\n\n 4   \n\nYou–4\n\n   \n\n5\n\n 5   \n\nYou–5\n\n   \n\n6\n\n 6   \n\nYou–6\n\n   \n\n7\n\n 7   \n\nYou–7\n\n   \n\n8\n\n 8   \n\nYou–8\n\n   \n\n9\n\n 9   \n\nYou–9\n\n   \n\n10\n\n 10   \n\nYou–10\n\n   \n\n11\n\n 11   \n\nYou–11\n\n   \n\n12\n\n 12   \n\nYou–12\n\n   \n\n13\n\n 13   \n\nYou–13\n\n   \n\n14\n\n 14   \n\nYou–14\n\n   \n\n15\n\n 15   \n\nYou–15\n\n   \n\n16\n\n 16   \n\nYou–16\n\n   \n\n17\n\n 17   \n\nYou–17\n\n   \n\n18\n\n 18   \n\nYou–18\n\n   \n\n19\n\n 19   \n\nYou–19\n\n   \n\n20\n\n 20   \n\nYou–20\n\n  \n\n\nFigure 16.1: A representation of your Instagram followers.\n\n\n\n\nAt first glance, it might appear to be a tree structure, but that is not the case. Your followers can follow other people, who in turn can have their followers. This creates a recursive relationship that cannot be represented using a tree data structure (see Figure 16.2).\n\n\n\n\n\n\n\n\nG\n\n  \n\nYou\n\n You   \n\n1\n\n 1   \n\nYou–1\n\n   \n\n2\n\n 2   \n\nYou–2\n\n   \n\n3\n\n 3   \n\nYou–3\n\n   \n\n4\n\n 4   \n\nYou–4\n\n   \n\n5\n\n 5   \n\nYou–5\n\n   \n\n6\n\n 6   \n\nYou–6\n\n   \n\n7\n\n 7   \n\nYou–7\n\n   \n\n8\n\n 8   \n\nYou–8\n\n   \n\n9\n\n 9   \n\nYou–9\n\n   \n\n10\n\n 10   \n\nYou–10\n\n   \n\n11\n\n 11   \n\n1–11\n\n   \n\n12\n\n 12   \n\n1–12\n\n   \n\n13\n\n 13   \n\n1–13\n\n   \n\n14\n\n 14   \n\n1–14\n\n   \n\n15\n\n 15   \n\n1–15\n\n   \n\n16\n\n 16   \n\n2–16\n\n   \n\n17\n\n 17   \n\n2–17\n\n   \n\n18\n\n 18   \n\n2–18\n\n   \n\n19\n\n 19   \n\n2–19\n\n   \n\n20\n\n 20   \n\n2–20\n\n   \n\n12–1\n\n   \n\n12–3\n\n   \n\n12–5\n\n   \n\n12–7\n\n   \n\n14–4\n\n   \n\n14–8\n\n   \n\n14–12\n\n   \n\n16–2\n\n   \n\n16–6\n\n   \n\n16–10\n\n   \n\n16–14\n\n   \n\n19–9\n\n   \n\n19–13\n\n   \n\n19–17\n\n  \n\n\nFigure 16.2: A representation of your instagram followers where they’re allowed to follow other people too.\n\n\n\n\nTo accurately capture this complex relationship, we need to use a graph data structure. Graphs consist of a set of vertices (or nodes) and a set of edges that connect them. In the context of Instagram followers, the vertices represent the users, and the edges represent the connections between them.\nUsing a graph data structure allows us to represent the recursive nature of the relationship between you and your followers, enabling us to model and analyze the complex network of connections in a more accurate way.\nSure! I will translate the given block according to the specifications you provided. Here’s the updated version:\nFurthermore, the relationship between you and your followers is even more complex. For instance, you can follow someone who does not follow you back, creating a directed relationship where the edges have a direction. In this case, the edges represent the connections from you to your followers, but not the other way around. Such relationships are often modeled using directed graphs, which are a common use case for graphs. Figure 16.3 visualizes this directed relationship.\n\n\n\n\n\n\n\n\nG\n\n  \n\nYou\n\n You   \n\n1\n\n 1   \n\nYou-&gt;1\n\n    \n\n2\n\n 2   \n\nYou-&gt;2\n\n    \n\n3\n\n 3   \n\nYou-&gt;3\n\n    \n\n4\n\n 4   \n\nYou-&gt;4\n\n    \n\n5\n\n 5   \n\nYou-&gt;5\n\n    \n\n6\n\n 6   \n\nYou-&gt;6\n\n    \n\n7\n\n 7   \n\nYou-&gt;7\n\n    \n\n8\n\n 8   \n\nYou-&gt;8\n\n    \n\n9\n\n 9   \n\nYou-&gt;9\n\n    \n\n10\n\n 10   \n\nYou-&gt;10\n\n    \n\n11\n\n 11   \n\n1-&gt;11\n\n    \n\n12\n\n 12   \n\n1-&gt;12\n\n    \n\n13\n\n 13   \n\n1-&gt;13\n\n    \n\n14\n\n 14   \n\n1-&gt;14\n\n    \n\n15\n\n 15   \n\n1-&gt;15\n\n    \n\n16\n\n 16   \n\n2-&gt;16\n\n    \n\n17\n\n 17   \n\n2-&gt;17\n\n    \n\n18\n\n 18   \n\n2-&gt;18\n\n    \n\n19\n\n 19   \n\n2-&gt;19\n\n    \n\n20\n\n 20   \n\n2-&gt;20\n\n    \n\n12-&gt;1\n\n    \n\n12-&gt;3\n\n    \n\n12-&gt;5\n\n    \n\n12-&gt;7\n\n    \n\n14-&gt;4\n\n    \n\n14-&gt;8\n\n    \n\n14-&gt;12\n\n    \n\n16-&gt;2\n\n    \n\n16-&gt;6\n\n    \n\n16-&gt;10\n\n    \n\n16-&gt;14\n\n    \n\n19-&gt;9\n\n    \n\n19-&gt;13\n\n    \n\n19-&gt;17\n\n   \n\n\nFigure 16.3: A directed representation of your Instagram followers. Here, an arrow going from vertex \\(A\\) to vertex \\(B\\) indicates that \\(A\\) follows \\(B\\), but \\(B\\) does not necessarily follow \\(A\\)."
  },
  {
    "objectID": "21_graphs.html#introduction",
    "href": "21_graphs.html#introduction",
    "title": "16  The Graph Data Structure",
    "section": "16.2 Introduction",
    "text": "16.2 Introduction\nA graph is a non-linear data structure that consists of a set of vertices (also called nodes) and a set of edges (or connections) that connect these vertices. In this data structure, the arrangement of vertices and edges allows for a more flexible and complex representation of relationships between data elements compared to linear data structures like arrays, lists, or queues.\nThe concept of adjacency refers to the relationship between two vertices in a graph. If there is an edge connecting two vertices, they are said to be adjacent. Incidence is the relationship between a vertex and an edge. A vertex is said to be incident to an edge if it is one of the two vertices connected by that edge.\nGraphs have numerous real-life applications, and some examples include:\n\nSocial networks, where vertices represent people and edges represent friendships or connections\nTransportation networks, where vertices represent locations and edges represent roads or routes\nCoronavirus transmission networks, where vertices represent individuals and edges represent transmission paths"
  },
  {
    "objectID": "21_graphs.html#graph-terminology",
    "href": "21_graphs.html#graph-terminology",
    "title": "16  The Graph Data Structure",
    "section": "16.3 Graph Terminology",
    "text": "16.3 Graph Terminology\nBefore diving into the implementation of graph data structures, let’s discuss some basic terms and properties of graphs.\n\n16.3.1 Basic Terms and Properties\n\nA graph is a data structure for representing connections among items and consists of vertices connected by edges.​\nA vertex (or node) represents an item in a graph.​\nAn edge represents a connection between two vertices in a graph.\nTwo vertices are adjacent if connected by an edge.​\nDirected vs Undirected: In an undirected graph, the edges have no specific direction, meaning that if there is an edge between vertices A and B, the connection is mutual. In a directed graph (also called a digraph), the edges have a direction, indicating an asymmetrical relationship between vertices. (See Figure 16.4 and Figure 16.5 for examples.)\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nB–C\n\n   \n\nC–A\n\n  \n\n\nFigure 16.4: Example of an undirected graph.\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA-&gt;B\n\n    \n\nC\n\n C   \n\nB-&gt;C\n\n    \n\nC-&gt;A\n\n   \n\n\nFigure 16.5: Example of a directed graph.\n\n\n\n\n\nWeighted vs Unweighted: In an unweighted graph, all edges have equal importance, while in a weighted graph, each edge is assigned a value (or weight), representing the importance, cost, or distance between the connected vertices. (See Figure 16.6 and Figure 16.7 for examples.)\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nB–C\n\n   \n\nC–A\n\n  \n\n\nFigure 16.6: Example of an unweighted graph.\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n 2   \n\nC\n\n C   \n\nB–C\n\n 3   \n\nC–A\n\n 1  \n\n\nFigure 16.7: Example of a weighted graph.\n\n\n\n\n\nSimple vs Multigraph: A simple graph has no more than one edge between any pair of vertices and does not contain any self-loops (edges that connect a vertex to itself). A multigraph can have multiple edges between the same pair of vertices and may include self-loops. (See Figure 16.8 and Figure 16.9 for examples.)\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nB–C\n\n   \n\nC–A\n\n  \n\n\nFigure 16.8: Example of a simple graph.\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nA–A\n\n self-loop   \n\nB\n\n B   \n\nA–B\n\n   \n\nA–B\n\n e2   \n\nC\n\n C   \n\nB–C\n\n   \n\nC–A\n\n  \n\n\nFigure 16.9: Example of a multigraph.\n\n\n\n\n\nDegree: The degree of a vertex is the number of edges incident to it. In a directed graph, we can distinguish between in-degree (the number of edges directed towards the vertex) and out-degree (the number of edges directed away from the vertex). See Figure 16.10 for an example.\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n A (degree 3)   \n\nD\n\n D   \n\nA–D\n\n D (degree 1)   \n\nC\n\n C   \n\nB–C\n\n B (degree 3)   \n\nC–A\n\n C (degree 3)  \n\n\nFigure 16.10: Example graph with vertex degrees.\n\n\n\n\n\nPath: A path in a graph is a sequence of vertices connected by edges. See Figure 16.11 for an example.\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nB–C\n\n   \n\nD\n\n D   \n\nC–D\n\n  \n\n\nFigure 16.11: Example graph with a path from A to D.\n\n\n\n\n\nCycle: A cycle is a closed path, where the first and last vertices in the path are the same, and no vertex is visited more than once. See Figure 16.12 for an example.\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nB–C\n\n   \n\nC–A\n\n  \n\n\nFigure 16.12: Example graph with a cycle. (A-B-C)\n\n\n\n\n\nConnected vs Disconnected: A graph is connected if there is a path between every pair of vertices. If there is at least one pair of vertices with no path between them, the graph is disconnected. See Figure 16.13 and Figure 16.14 for examples.\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nB–C\n\n   \n\nC–A\n\n  \n\n\nFigure 16.13: Example of a connected graph.\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nB–C\n\n   \n\nD\n\n D   \n\nE\n\n E   \n\nD–E\n\n  \n\n\nFigure 16.14: Example of a disconnected graph.\n\n\n\n\n\n\n16.3.2 Graph Notation\nWe can use a notation like \\(G(V, E)\\), where \\(V\\) is the set of vertices and \\(E\\) is the set of edges, to represent a graph.\n\n\n16.3.3 Special Types of Graphs\n\nComplete Graph: A complete graph is a simple graph in which every pair of vertices is connected by a unique edge. See Figure 16.15 for an example.\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nA–C\n\n   \n\nD\n\n D   \n\nA–D\n\n   \n\nB–C\n\n   \n\nB–D\n\n   \n\nC–D\n\n  \n\n\nFigure 16.15: Example of a complete graph.\n\n\n\n\n\nBipartite Graph: A bipartite graph is a graph whose vertices can be divided into two disjoint sets such that all edges connect vertices from one set to the other, with no edges connecting vertices within the same set. See Figure 16.16 for an example.\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\n1\n\n 1   \n\nA–1\n\n   \n\n2\n\n 2   \n\nA–2\n\n   \n\nB\n\n B   \n\nB–1\n\n   \n\n3\n\n 3   \n\nB–3\n\n   \n\nC\n\n C   \n\nC–2\n\n   \n\nC–3\n\n  \n\n\nFigure 16.16: Example of a bipartite graph.\n\n\n\n\n\nTree: A tree is an undirected graph with no cycles, and all vertices are connected. It has a hierarchical structure, with one vertex acting as the root, and the other vertices connected in a parent-child relationship. See Figure 16.17 for an example.\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nA–C\n\n   \n\nD\n\n D   \n\nB–D\n\n   \n\nE\n\n E   \n\nB–E\n\n   \n\nF\n\n F   \n\nC–F\n\n   \n\nG\n\n G   \n\nC–G\n\n  \n\n\nFigure 16.17: Example of a tree."
  },
  {
    "objectID": "21_graphs.html#graph-representation",
    "href": "21_graphs.html#graph-representation",
    "title": "16  The Graph Data Structure",
    "section": "16.4 Graph Representation",
    "text": "16.4 Graph Representation\nIn order to work with graphs in code or store them in memory, we need efficient ways to represent them. There are multiple methods to represent graphs, and the choice of representation depends on factors such as the density of the graph, the operations to be performed, and memory constraints.\nIn this section, we will discuss two common methods to represent a graph: adjacency list and adjacency matrix.\n\n16.4.1 Adjacency List\nAn adjacency list represents a graph by storing a list of adjacent vertices for each vertex in the graph. This can be implemented using an array of lists or a hash table, where the index or key corresponds to a vertex, and the value is a list of adjacent vertices.\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nA–C\n\n   \n\nD\n\n D   \n\nB–D\n\n   \n\nC–D\n\n  \n\n\nFigure 16.18: See adjacency list for this example Listing 16.1\n\n\n\n\nAdjacency list representation for figure Figure 16.18:\n\nListing 16.1: Adjacency list representation.\nA: [B, C]\nB: [A, D]\nC: [A, D]\nD: [B, C]\n\n// or as an arraylist of arraylists -\n[[B, C], [A, D], [A, D], [B, C]]\n// here, the index of the outer arraylist represents the vertex.\n// in order for this to work, the order of the vertices must be \n// fixed, and stored separately.\n\nThe adjacency list representation is efficient for sparse graphs (graphs with relatively few edges) as it only stores the existing edges, reducing memory usage. This representation also allows for faster traversal of a vertex’s neighbors.\n\n\n16.4.2 Adjacency Matrix\nAn adjacency matrix is a two-dimensional array (or matrix) where the cell at the i-th row and j-th column represents the edge between vertex i and vertex j. For an undirected graph, the adjacency matrix is symmetric. For a weighted graph, the values in the cells represent the weights of the edges; for an unweighted graph, the cells contain either 1 (edge exists) or 0 (no edge).\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nA–C\n\n   \n\nD\n\n D   \n\nB–D\n\n   \n\nC–D\n\n  \n\n\nSee adjacency matrix for this example Listing 16.2\n\n\n\nAdjacency matrix representation (unweighted):\n\nListing 16.2: Adjacency matrix representation.\n  A B C D\nA 0 1 1 0\nB 1 0 0 1\nC 1 0 0 1\nD 0 1 1 0\n\nThe adjacency matrix representation is suitable for dense graphs (graphs with many edges) or when checking for the presence of an edge between two vertices needs to be fast. However, this representation can be inefficient in terms of memory usage, especially for large, sparse graphs, as it stores information for all possible edges, even if they do not exist.\n\n\n16.4.3 Converting Between Representations\nTo convert a graph diagram or notation into an adjacency list or an adjacency matrix, follow these steps:\n\nIdentify the vertices and edges in the graph.\nFor an adjacency list, create an empty list or hash table for each vertex. For each edge, add the adjacent vertices to the corresponding lists.\nFor an adjacency matrix, create a square matrix with dimensions equal to the number of vertices. For each edge, set the corresponding cells in the matrix to 1 (or the edge weight for weighted graphs).\n\nTo convert an adjacency list or an adjacency matrix back into a graph diagram or notation, follow these steps:\n\nIdentify the vertices based on the keys (for an adjacency list) or indices (for an adjacency matrix).\nFor an adjacency list, iterate through the lists and draw an edge for each adjacent vertex.\nFor an adjacency matrix, iterate through the matrix cells and draw an edge for each non-zero value (or the corresponding weight for weighted graphs)."
  },
  {
    "objectID": "21_graphs.html#graph-traversal",
    "href": "21_graphs.html#graph-traversal",
    "title": "16  The Graph Data Structure",
    "section": "16.5 Graph Traversal",
    "text": "16.5 Graph Traversal\nImagine you want to find the average age of all users on Facebook. With billions of users, it is infeasible to hold the entire graph of the friend network in memory. Ideally, we would want to find out information on each user one at a time, on a per-need basis. To achieve this, we can use graph traversal algorithms, which allow us to visit each user, add up their ages, and then calculate the average. A simple way to do this is to load information on a user, add all their friends to a stack, and then keep popping from the stack and requesting data from Facebook for each friend. When we receive the data, we mark that user as visited to avoid recounting their age if we reach the same user again. We then add friends of each loaded user to our stack and keep repeating until we run out of users in our stack.\nThis problem illustrates the importance of graph traversal, a fundamental operation in graph theory. Graphs are a powerful and versatile data structure that can model various kinds of relationships and networks, such as social networks, computer networks, transportation networks, web pages, games, and many other domains. Graph traversal allows us to explore and manipulate graphs in various ways, with applications in domains like searching for specific nodes, finding the shortest path between nodes, and analyzing the structure of a graph.\nGraph traversal algorithms typically begin with a start node and attempt to visit the remaining nodes from there. They must deal with several troublesome cases, such as unreachable nodes, revisited nodes, and choosing which node to visit next among several options. To handle these cases, graph traversal algorithms use different strategies and data structures to keep track of which nodes have been visited and which nodes are still pending. The most common graph traversal algorithms are breadth-first search (BFS) and depth-first search (DFS), which differ in the order in which they visit the nodes.\nIn some situations, we may not know the entire graph at once and instead only have access to a node object and its adjacent nodes. As demonstrated in the Facebook example, graph traversal algorithms can be used to solve problems that involve large and dynamic graphs by visiting each user and analyzing their information on a per-need basis.\nThere are two common methods to traverse a graph:\n\nBreadth-First Search (BFS)\nDepth-First Search (DFS)\n\nBy understanding and implementing these graph traversal methods, you can efficiently explore and manipulate complex graphs to solve a wide range of problems.\n\n16.5.1 Breadth-First Search (BFS)\nBreadth-First Search explores a graph by visiting all the neighbors of the starting vertex before moving on to their neighbors. BFS uses a queue data structure to keep track of the vertices to visit.\nHere’s a step-by-step example of BFS traversal (for the graph in example Figure 16.19):\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nA–C\n\n   \n\nD\n\n D   \n\nB–D\n\n   \n\nC–D\n\n   \n\nE\n\n E   \n\nC–E\n\n  \n\n\nFigure 16.19: Example graph for BFS traversal.\n\n\n\n\nBFS traversal starting from vertex A:\n\nVisit A and add its neighbors B and C to the queue: [B, C]\nVisit B and add its unvisited neighbor D to the queue: [C, D]\nVisit C and add its unvisited neighbor E to the queue: [D, E]\nVisit D: [E]\nVisit E: []\n\nBFS traversal order: A, B, C, D, E\nBFS pseudocode:\nBFS(graph, start):\n  Initialize an empty queue Q\n  Mark start as visited\n  Enqueue start into Q\n  \n  while Q is not empty:\n    vertex = Dequeue(Q)\n    Visit vertex\n    \n    for each neighbor of vertex:\n      if neighbor is not visited:\n        Mark neighbor as visited\n        Enqueue neighbor into Q\n\n\n16.5.2 Depth-First Search (DFS)\nDepth-First Search explores a graph by visiting a vertex and its neighbors as deeply as possible before backtracking. DFS can be implemented using recursion or an explicit stack data structure.\nHere’s a step-by-step example of DFS traversal (for the graph in example Figure 16.20):\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nA–C\n\n   \n\nD\n\n D   \n\nB–D\n\n   \n\nC–D\n\n   \n\nE\n\n E   \n\nC–E\n\n  \n\n\nFigure 16.20: Example graph for DFS traversal.\n\n\n\n\nDFS traversal starting from vertex A:\n\nVisit A and recurse on its first neighbor B\nVisit B and recurse on its first neighbor D\nVisit D and backtrack (no unvisited neighbors)\nBacktrack to A and recurse on its next neighbor C\nVisit C and recurse on its first neighbor E\nVisit E and backtrack (no unvisited neighbors)\n\nDFS traversal order: A, B, D, C, E\nDFS pseudocode (recursive):\nDFS(graph, vertex):\n  Mark vertex as visited\n  Visit vertex\n  \n  for each neighbor of vertex:\n    if neighbor is not visited:\n      DFS(graph, neighbor)\nDFS pseudocode (iterative with a stack):\nDFS(graph, start):\n  Initialize an empty stack S\n  Mark start as visited\n  Push start onto S\n  \n  while S is not empty:\n    vertex = Pop(S)\n    Visit vertex\n    \n    for each neighbor of vertex:\n      if neighbor is not visited:\n        Mark neighbor as visited\n        Push neighbor onto S\n\n\n16.5.3 Applications and Variations of BFS and DFS\nBoth BFS and DFS have numerous applications and can be adapted to solve various graph-related problems:\n\nShortest path: BFS can be used to find the shortest path between two vertices in an unweighted graph. The algorithm can be modified to keep track of the path length or the actual path itself.\nConnected components: Both BFS and DFS can be used to find connected components in an undirected graph. By running the traversal algorithm and marking visited vertices, we can identify the set of vertices reachable from a starting vertex. Repeating this process for all unvisited vertices will find all connected components in the graph.\nTopological sorting: DFS can be adapted to perform a topological sort on a directed acyclic graph (DAG). A topological ordering is a linear ordering of the vertices such that for every directed edge (u, v), vertex u comes before vertex v in the ordering. This can be useful in scheduling tasks with dependencies or determining the order of courses in a curriculum.\nBipartite graph check: BFS or DFS can be used to check if a graph is bipartite. The algorithm can be modified to color vertices while traversing the graph. If at any point during the traversal, two adjacent vertices have the same color, the graph is not bipartite.\nGraph cycle detection: DFS can be used to detect cycles in a graph. By keeping track of the recursion stack, we can determine if a vertex is visited more than once in the same path, indicating a cycle.\n\nIn summary, graph traversal is a fundamental operation in graph theory with various applications. Breadth-First Search (BFS) and Depth-First Search (DFS) are two common techniques to traverse a graph, each with its own advantages and use cases. Understanding these algorithms and their variations can help solve a wide range of graph-related problems."
  },
  {
    "objectID": "22_hashing.html#background-and-motivation",
    "href": "22_hashing.html#background-and-motivation",
    "title": "17  Hashing, Hash Tables, and Hash Maps",
    "section": "17.1 Background and Motivation",
    "text": "17.1 Background and Motivation\n\n17.1.1 Indexing\nIndexing refers to the idea of accessing a certain element of an array by referring to it using a specific number, called the index. Using the same index always returns the same element, as long as the array remains unchanged. For example, to access the 12th element of an array arr, we use arr[11].\nThe math for finding the address of an element in the array works out to be:\nbaseAddress + (index * sizeOfElement)\nNow, keeping that in mind, let’s explore the limitations of indexing.\n\n\n17.1.2 Limitations of Indexing\nSuppose we want to access an element in the array using a string as an index, such as arr[\"dhruv\"]. What is stopping us?\nThe problem is that we cannot calculate baseAddress + (\"dhruv\" * sizeOfElement) because the index, in this case, is a string, not a number. The operation is not defined, and therefore, we can’t directly use strings as indices in an array.\n\n\n17.1.3 Mapping Strings to Numbers\nLet’s consider an array of strings. We can use it to map a number to a string:\n0 -&gt; \"Alice\"\n1 -&gt; \"Bob\"\n2 -&gt; \"Charlie\"\nIf we can use an array to map a number to a string, can we also use it to map strings to numbers? Yes, we can!\nOne way to do this is by searching linearly for a string in the array to find its index. For example, we can find the string “Dhruv” at index 5:\n0 -&gt; \"Alice\"\n1 -&gt; \"Bob\"\n2 -&gt; \"Charlie\"\n...\n5 -&gt; \"Dhruv\"\nThe index at which we found the string “Dhruv” (in this case, 5) can be used as a key in a different array to find the data related to “Dhruv”. However, this method of linear searching can be quite slow for large datasets.\n\n\n17.1.4 Hashing: A Better Solution\nThis is where hashing comes into play. Hashing allows us to efficiently map strings (or any other non-numeric keys) to numbers. By using a hash function, we can convert a string into a number that represents the index in the array.\nA hash function takes a key as input and outputs an index in the hash table’s array. A good hash function has the following criteria:\n\nUniform distribution: The hash function should distribute keys evenly across the array to minimize collisions (when multiple keys map to the same index).\nMinimal collisions: A good hash function should minimize the chance of collisions.\nFast computation: The hash function should be fast to compute, allowing for quick insertion, deletion, and retrieval of data.\nDeterministic output: The hash function should produce the same output for the same input every time it is called.\n\nFor example, let’s consider a simple hash function that converts the first character of a string into its ASCII code:\nhash(\"dhruv\") = ASCII('d') = 100\nThe output of the hash function is 100, which we can use as an index in an array to store or retrieve data related to “dhruv”. This allows us to use strings (and other non-numeric keys) as indices, achieving our goal of efficient mapping."
  },
  {
    "objectID": "22_hashing.html#hash-functions",
    "href": "22_hashing.html#hash-functions",
    "title": "17  Hashing, Hash Tables, and Hash Maps",
    "section": "17.2 Hash Functions",
    "text": "17.2 Hash Functions\n\n17.2.1 Introduction\nPreviously, we managed to map a string “D” to some data, just like an index maps to some data in an array. The resulting data structure that can map any string to any data is called a hash table. The function used to map strings to data is called a hash function. The concept of mapping “D” to some data can be referred to as hashing “D” to an index 3, which is where we found the value corresponding to “D”.\nNow, we’ll learn about a way of mapping any object (called a key) to any other object (called the record, or the value). For instance, your student ID can be a key, and all the data about you on your ID can be stored in a record object.\n\n\n17.2.2 Hashing\nHashing can be thought of as a method for storing and retrieving records from a database. It lets you insert, delete, and search for records based on a search key value. When properly implemented, these operations can be performed in constant time. In fact, a properly tuned hash system typically looks at only one or two records for each search, insert, or delete operation. This is far better than the \\(O(log n)\\) average cost required to do a binary search on a sorted array of n records, or the \\(O(log n)\\) average cost required to do an operation on a binary search tree. However, even though hashing is based on a very simple idea, it is surprisingly difficult to implement properly. Designers need to pay careful attention to all of the details involved with implementing a hash system.\nA hash system stores records in an array called a hash table, which we will call HT. Hashing works by performing a computation on a search key K in a way that is intended to identify the position in HT that contains the record with key K. The function that does this calculation is called the hash function, and will be denoted by the letter h. Since hashing schemes place records in the table in whatever order satisfies the needs of the address calculation, records are not ordered by value. A position in the hash table is also known as a slot. The number of slots in hash table HT will be denoted by the variable M with slots numbered from 0 to M−1. The goal for a hashing system is to arrange things such that, for any key value K and some hash function h, \\(i=h(K)\\) is a slot in the table such that \\(0 \\leq i &lt; M\\), and we have the key of the record stored at HT[i] equal to K.\nHashing is not good for applications where multiple records with the same key value are permitted. Hashing is not a good method for answering range searches. In other words, we cannot easily find all records (if any) whose key values fall within a certain range. Nor can we easily find the record with the minimum or maximum key value or visit the records in key order. Hashing is most appropriate for answering the question, ‘What record, if any, has key value K?’ For applications where all search is done by exact-match queries, hashing is the search method of choice because it is extremely efficient when implemented correctly.\nHashing generally takes records whose key values come from a large range and stores those records in a table with a relatively small number of slots. Since keys have a large range and values have smaller, limited slots for storage – A hash function might sometimes end up hashing two keys to the same slot. We refer to such an event as a collision.\nTo illustrate, consider a classroom full of students. What is the probability that some pair of students shares the same birthday (i.e., the same day of the year, not necessarily the same year)? If there are 23 students, then it is unlikely that more than one student will share the same birthday. There are 365 “slots” or possible days a student can have a birthday on; but only 23 “keys”. As the number of students increases, the probability of a “collision” or two students sharing a birthday increases. To be practical, a database organized by hashing must store records in a hash table that is not so large that it wastes space.\nWe would like to pick a hash function that maps keys to slots in a way that makes each slot in the hash table have equal probability of being filled for the actual set keys being used. Unfortunately, we normally have no control over the distribution of key values for the actual records in a given database or collection. So how well any particular hash function does depends on the actual distribution of the keys used within the allowable key range. In some cases, incoming data are well distributed across their key range. For example, if the input is a set of random numbers selected uniformly from the key range, any hash function that assigns the key range so that each slot in the hash table receives an equal share of the range will likely also distribute the input records uniformly within the table.\nHowever, in many applications the incoming records are highly clustered or otherwise poorly distributed. When input records are not well distributed throughout the key range it can be difficult to devise a hash function that does a good job of distributing the records throughout the table, especially if the input distribution is not known in advance. For example, If the input is a collection of English words, the beginning letter will be poorly distributed. A dictionary of words mapped to their frequency is often used in rudimentary natural language processing algorithms.\nIn conclusion, anything can be a hash function (i.e., map a value to an index), but not everything can be a good hash function. A function that always returns the index 0 is a hash function that maps everything to 0. It’s no good but it’s still a hash function. An example of a commonly used hash function is the modulus operator! It is common for N-sized hash tables to use the modulus of N as a hash function. If N is \\(20\\), data for 113 will be hashed to index \\(113 \\% 20 = 13\\).\nBut if we use the modulo operator as a hash function, what do we do when multiple pieces of data map to the same index? \\(53 \\% 20 = 13\\), \\(73 \\% 20 = 13\\), etc. But if you think about it, we can store everything at \\(13\\)! By using nested data structures… More on this later.\n\n\n17.2.3 Simple Hash Functions\nLet’s apply a simple hash function to a set of keys and compute their indices. In this example, we’ll use the modulo operation as the hash function. Given a hash table with a size of 5, we can compute the indices for the keys as follows:\nHashTable size: 5\nHashFunction: key % size\n\nKeys: 15, 28, 47, 10, 33\n\nIndices:\n15 % 5 = 0\n28 % 5 = 3\n47 % 5 = 2\n10 % 5 = 0\n33 % 5 = 3\n\n\n17.2.4 Other Types of Hash Functions\n\n17.2.4.1 Direct Hashing\nA direct hash function uses the item’s key as the bucket index. For example, if the key is 937, the index is 937. A hash table with a direct hash function is called a direct access table. Given a key, a direct access table search algorithm returns the item at index key if the bucket is not empty, and returns null (indicating item not found) if empty.\nLimitations:\nA direct access table has the advantage of no collisions: Each key is unique (by definition of a key), and each gets a unique bucket, so no collisions can occur. However, a direct access table has two main limitations:\n\nAll keys must be non-negative integers, but for some applications, keys may be negative.\nThe hash table’s size equals the largest key value plus 1, which may be very large.\n\nSimilarly, there are other hash functions each with their own characteristics.\n\n\n17.2.4.2 Modulo Hash\nA modulo hash function computes the index by taking the remainder of the key divided by the table size M. This is a simple and effective way to convert a large key range into a smaller index range. The hash function can be defined as:\nh(K) = K % M\n\n\n17.2.4.3 Mid-Square Hash\nA mid-square hash function computes the index by first squaring the key, and then extracting a portion of the squared value as the index. This approach is especially useful when the keys are not uniformly distributed. The hash function can be defined as:\nh(K) = middle_digits(K^2)\n\n\n17.2.4.4 Mid-Square Hash with Base 2\nA mid-square hash function with base 2 is a variation of the mid-square hash function, where the key is first squared, and then the middle bits of the binary representation of the squared value are extracted as the index. This approach is especially useful for binary keys. The hash function can be defined as:\nh(K) = middle_bits(K^2)\n\n\n17.2.4.5 Multiplicative String Hashing\nA multiplicative string hashing function computes the index by treating the characters in the string as numbers and combining them using a multiplication and a constant. This approach can help achieve a good distribution of string keys in the hash table. The hash function can be defined as:\nh(K) = (c1 * a^(n-1) + c2 * a^(n-2) + ... + cn) % M\nwhere c1, c2, ..., cn are the character codes of the string, a is a constant, n is the length of the string, and M is the size of the hash table.\nHere’s the ASCII representation of the resulting hash table:\nIndex | Key\n-------------\n  0   | 15\n  1   | -\n  2   | 47\n  3   | 28\n  4   | -\nIn this example, we can see that the keys 15 and 10, as well as 28 and 33, have collided, as they both map to the same indices (0 and 3, respectively).\n\n\n\n17.2.5 Trade-offs Between Different Hash Functions\nThere are trade-offs between different hash functions in terms of performance and complexity:\n\nA simple hash function, like the modulo operation, is fast to compute but may not distribute keys uniformly, leading to more collisions and reduced performance.\nMore complex hash functions, such as cryptographic hash functions, can provide a better distribution of keys but may be slower to compute.\n\nIn practice, the choice of a hash function depends on the specific requirements of the application and the data being stored. The goal is to find a balance between uniform distribution, minimal collisions, fast computation, and deterministic output."
  },
  {
    "objectID": "22_hashing.html#hash-collisions",
    "href": "22_hashing.html#hash-collisions",
    "title": "17  Hashing, Hash Tables, and Hash Maps",
    "section": "17.3 Hash Collisions",
    "text": "17.3 Hash Collisions\nHash collisions occur when two or more keys map to the same index in the hash table. Due to the pigeonhole principle, hash collisions are inevitable, as there are typically more possible keys than available indices in the array. Collisions negatively impact the efficiency of hashing, as they can lead to longer access times for insertion, deletion, and retrieval of key-value pairs.\nThere are two primary methods to resolve hash collisions: chaining and open addressing."
  },
  {
    "objectID": "22_hashing.html#chaining",
    "href": "22_hashing.html#chaining",
    "title": "17  Hashing, Hash Tables, and Hash Maps",
    "section": "17.4 Chaining",
    "text": "17.4 Chaining\nChaining is a collision resolution technique that uses a linked list or another data structure to store multiple key-value pairs at the same index. When a collision occurs, the new key-value pair is simply added to the data structure at the index.\n\n17.4.1 Insertion, Search, and Deletion\nHere’s how to perform insertion, search, and deletion operations using chaining:\n\nInsertion: Calculate the index using the hash function. If the index is empty, create a new data structure (e.g., linked list) and insert the key-value pair. If the index is not empty, add the key-value pair to the existing data structure.\nSearch: Calculate the index using the hash function. If the index is empty, the key is not in the hash table. If the index is not empty, search the data structure at the index for the key.\nDeletion: Calculate the index using the hash function. If the index is empty, the key is not in the hash table. If the index is not empty, search the data structure at the index for the key and remove it if found.\n\n\n\n17.4.2 Advantages and Disadvantages of Chaining\nChaining has several advantages and disadvantages:\n\nAdvantages:\n\nEasy implementation: Chaining can be easily implemented using existing data structures like linked lists.\nDynamic size: The data structure at each index can grow or shrink as needed, allowing for efficient use of space.\n\nDisadvantages:\n\nExtra space: Chaining requires additional space for the data structure at each index, which can increase memory overhead.\nVariable access time: The access time for key-value pairs depends on the length of the data structure at the index, which can vary.\n\n\nChaining is a popular method for resolving hash collisions due to its simplicity and dynamic size. However, it may not be the most efficient option for all use cases, especially when memory overhead and variable access times are critical factors."
  },
  {
    "objectID": "22_hashing.html#open-addressing",
    "href": "22_hashing.html#open-addressing",
    "title": "17  Hashing, Hash Tables, and Hash Maps",
    "section": "17.5 Open Addressing",
    "text": "17.5 Open Addressing\nOpen addressing is a collision resolution technique that finds an alternative index for a key-value pair if the original index is occupied. When a collision occurs, the algorithm searches for the next available index using a probing technique. There are three common types of probing techniques: linear probing, quadratic probing, and double hashing.\n\n17.5.1 Probing Techniques\n\nLinear probing: When a collision occurs, search the hash table linearly (one index at a time) until an empty slot is found.\nQuadratic probing: When a collision occurs, search the hash table quadratically (by increasing the index by the square of the probe number) until an empty slot is found.\nDouble hashing: When a collision occurs, use a secondary hash function to compute a new index for the key-value pair, and repeat this process until an empty slot is found.\n\n\n\n17.5.2 Insertion, Search, and Deletion\nHere’s how to perform insertion, search, and deletion operations using open addressing:\n\nInsertion: Calculate the index using the hash function. If the index is empty, insert the key-value pair. If the index is occupied, use the chosen probing technique to find the next available index and insert the key-value pair there.\nSearch: Calculate the index using the hash function. If the index is empty, the key is not in the hash table. If the index is occupied, check if the key matches the stored key. If not, use the chosen probing technique to search for the next index until the key is found or an empty index is encountered.\nDeletion: Calculate the index using the hash function. If the index is empty, the key is not in the hash table. If the index is occupied and the key matches the stored key, remove the key-value pair and mark the index as deleted. Continue searching using the chosen probing technique to handle cases where the removed key-value pair was part of a cluster.\n\n\n\n17.5.3 Advantages and Disadvantages of Open Addressing\nOpen addressing has several advantages and disadvantages:\n\nAdvantages:\n\nNo extra space: Open addressing does not require additional space for data structures at each index, making it more memory-efficient.\nFixed size: The hash table has a fixed size, which can be useful when memory is limited.\n\nDisadvantages:\n\nClustering: Probing techniques can cause clusters of key-value pairs to form, leading to increased access times.\nDeletion issues: Deleting key-value pairs can create complications, as it may leave “holes” in clusters that need to be addressed.\n\n\nOpen addressing is an alternative method for resolving hash collisions that can be more memory-efficient than chaining. However, it may not be the best option for all use cases, especially when clustering and deletion issues are critical factors."
  },
  {
    "objectID": "22_hashing.html#complexity-and-load-factor",
    "href": "22_hashing.html#complexity-and-load-factor",
    "title": "17  Hashing, Hash Tables, and Hash Maps",
    "section": "17.6 Complexity and Load Factor",
    "text": "17.6 Complexity and Load Factor\nWhen analyzing the complexity of hash functions and hash tables, we need to consider the time taken for searching, inserting, or deleting an element. There are two main steps involved in these operations:\n\nComputing the hash function for the given key.\nTraversing the list of key-value pairs present at the computed index.\n\n\n17.6.1 Time Complexity of Hash Computation\nFor the first step, the time taken depends on the key and the hash function. For example, if the key is a string “abcd”, then its hash function may depend on the length of the string. But for very large values of n, the number of entries into the map, the length of the keys is almost negligible in comparison to n, so hash computation can be considered to take place in constant time, i.e., O(1).\n\n\n17.6.2 Time Complexity of List Traversal\nFor the second step, traversal of the list of key-value pairs present at that index needs to be done. In the worst case, all the n entries are at the same index, resulting in a time complexity of O(n). However, enough research has been done to make hash functions uniformly distribute the keys in the array, so this almost never happens.\n\n\n17.6.3 Load Factor\nOn average, if there are n entries and b is the size of the array, there would be n/b entries at each index. This value n/b is called the load factor, which represents the load on our map. The load factor is denoted by the symbol λ:\nλ = n/b\nThis load factor needs to be kept low so that the number of entries at one index is less, and the complexity remains almost constant, i.e., O(1).\n\n\n17.6.4 Balancing Load Factor and Complexity\nTo maintain the load factor at an acceptable level, the hash table can be resized when the load factor exceeds a certain threshold. This helps to keep the complexity of hash table operations near O(1) by redistributing the keys uniformly across a larger array.\nIn conclusion, understanding the complexity and load factor of hash functions is crucial for designing efficient hash tables. By carefully choosing a suitable hash function and managing the load factor, it’s possible to achieve near-constant time complexity for various hash table operations."
  },
  {
    "objectID": "22_hashing.html#rehashing",
    "href": "22_hashing.html#rehashing",
    "title": "17  Hashing, Hash Tables, and Hash Maps",
    "section": "17.7 Rehashing",
    "text": "17.7 Rehashing\nRehashing, as the name suggests, means hashing again. When the load factor increases to more than its pre-defined value (the default value of the load factor is 0.75), the complexity increases. To overcome this issue, the size of the array is increased (typically doubled) and all the values are hashed again and stored in the new, larger array. This helps maintain a low load factor and low complexity.\n\n17.7.1 Why?\nRehashing is done because whenever key-value pairs are inserted into the map, the load factor increases, which implies that the time complexity also increases, as explained earlier. This might not provide the desired time complexity of O(1). Hence, rehashing must be performed, increasing the size of the bucketArray to reduce the load factor and the time complexity.\n\n\n17.7.2 How?\nRehashing can be done as follows:\n\nFor each addition of a new entry to the map, check the load factor.\nIf the load factor is greater than its pre-defined value (or the default value of 0.75 if not given), then perform rehashing.\nTo rehash, create a new array of double the previous size and make it the new bucketArray.\nTraverse each element in the old bucketArray and call the insert() method for each, to insert it into the new larger bucketArray.\n\nThe following diagram illustrates the rehashing process:\nInitial bucketArray (size = 4):\n+---+---+---+---+\n|   | K1|   | K2|\n+---+---+---+---+\n\nAfter inserting a new key K3 (load factor &gt; 0.75):\n\nNew bucketArray (size = 8):\n+---+---+---+---+---+---+---+---+\n|   | K1|   | K2|   |   |   | K3|\n+---+---+---+---+---+---+---+---+\nBy rehashing, the hash table maintains its desired time complexity of O(1) even as the number of elements increases. It is important to note that rehashing can be a costly operation, especially if the number of elements in the hash table is large. However, since rehashing is done infrequently and only when the load factor surpasses a certain threshold, the amortized cost of rehashing remains low, allowing the hash table operations to maintain near-constant time complexity."
  },
  {
    "objectID": "22_hashing.html#hash-tables-vs-hash-maps",
    "href": "22_hashing.html#hash-tables-vs-hash-maps",
    "title": "17  Hashing, Hash Tables, and Hash Maps",
    "section": "17.8 Hash Tables vs Hash Maps",
    "text": "17.8 Hash Tables vs Hash Maps\nHash tables and hash maps differ in their implementation and functionality.\n\nHash tables use direct hashing, where the key is an integer or can be directly converted to an integer (e.g., a string of digits). The integer is then used to compute the index in the hash table.\nHash maps use indirect hashing, where the key can be any data type. A separate hash function is needed to convert the key into an index in the hash table.\n\nWhen deciding whether to use a hash table or a hash map, consider the problem domain and the data type of the keys:\n\nIf the keys are integers or can be directly converted to integers, a hash table may be a more suitable choice. For example, if you’re working with student IDs as keys, a hash table would be a good fit.\nIf the keys are of any other data type or cannot be directly converted to integers, a hash map would be more appropriate. For example, if you’re working with strings, such as usernames or URLs, a hash map would be a better choice."
  },
  {
    "objectID": "22_hashing.html#hashmaps-in-java",
    "href": "22_hashing.html#hashmaps-in-java",
    "title": "17  Hashing, Hash Tables, and Hash Maps",
    "section": "17.9 HashMaps in Java",
    "text": "17.9 HashMaps in Java\nA HashMap is a collection in Java that implements the Map interface and uses a hash table for storage. It stores key-value pairs, where each key is unique, and the keys are not ordered.\nHere’s how to use a HashMap in Java:\n\nImport the HashMap class: To use the HashMap class in your Java code, you’ll need to import it from the java.util package:\nimport java.util.HashMap;\nCreate a HashMap: To create a new HashMap, use the following syntax:\nHashMap&lt;String, Integer&gt; myMap = new HashMap&lt;String, Integer&gt;();\nAdd elements: To add key-value pairs to the HashMap, use the put() method:\nmyMap.put(\"apple\", 3);\nmyMap.put(\"banana\", 5);\nmyMap.put(\"orange\", 2);\nAccess elements: To access the value associated with a key, use the get() method:\nint apples = myMap.get(\"apple\"); // 3\nint oranges = myMap.get(\"orange\"); // 2\nRemove elements: To remove a key-value pair from the HashMap, use the remove() method:\nmyMap.remove(\"banana\");\nCheck if a key exists: To check if a key is in the HashMap, use the containsKey() method:\nboolean hasApple = myMap.containsKey(\"apple\"); // true\nboolean hasGrape = myMap.containsKey(\"grape\"); // false\nIterate over keys: To iterate over the keys in a HashMap, you can use a for-each loop with the keySet() method:\nfor (String fruit : myMap.keySet()) {\n    System.out.println(fruit + \": \" + myMap.get(fruit));\n}\nIterate over values: To iterate over the values in a HashMap, you can use a for-each loop with the values() method:\nfor (Integer count : myMap.values()) {\n    System.out.println(count);\n}\nIterate over key-value pairs: To iterate over the key-value pairs in a HashMap, you can use a for-each loop with the entrySet() method:\nfor (HashMap.Entry&lt;String, Integer&gt; entry : myMap.entrySet()) {\n    System.out.println(entry.getKey() + \": \" + entry.getValue());\n}\n\nA HashMap can be a useful data structure when you need to store key-value pairs efficiently. It provides constant-time performance for common operations like put, get, and remove, making it an ideal choice for various applications."
  },
  {
    "objectID": "22_hashing.html#hashtables-in-java",
    "href": "22_hashing.html#hashtables-in-java",
    "title": "17  Hashing, Hash Tables, and Hash Maps",
    "section": "17.10 HashTables in Java",
    "text": "17.10 HashTables in Java\nA HashTable is a collection in Java that implements the Map interface and uses a hash table for storage. It is similar to a HashMap but with some differences, such as being synchronized, which makes it thread-safe. HashTable stores key-value pairs, where each key is unique, and the keys are not ordered.\nHere’s how to use a HashTable in Java:\n\nImport the HashTable class: To use the HashTable class in your Java code, you’ll need to import it from the java.util package:\nimport java.util.Hashtable;\nCreate a HashTable: To create a new HashTable, use the following syntax:\nHashtable&lt;String, Integer&gt; myTable = new Hashtable&lt;String, Integer&gt;();\nAdd elements: To add key-value pairs to the HashTable, use the put() method:\nmyTable.put(\"apple\", 3);\nmyTable.put(\"banana\", 5);\nmyTable.put(\"orange\", 2);\nAccess elements: To access the value associated with a key, use the get() method:\nint apples = myTable.get(\"apple\"); // 3\nint oranges = myTable.get(\"orange\"); // 2\nRemove elements: To remove a key-value pair from the HashTable, use the remove() method:\nmyTable.remove(\"banana\");\nCheck if a key exists: To check if a key is in the HashTable, use the containsKey() method:\nboolean hasApple = myTable.containsKey(\"apple\"); // true\nboolean hasGrape = myTable.containsKey(\"grape\"); // false\nIterate over keys: To iterate over the keys in a HashTable, you can use a for-each loop with the keySet() method:\nfor (String fruit : myTable.keySet()) {\n    System.out.println(fruit + \": \" + myTable.get(fruit));\n}\nIterate over values: To iterate over the values in a HashTable, you can use a for-each loop with the values() method:\nfor (Integer count : myTable.values()) {\n    System.out.println(count);\n}\nIterate over key-value pairs: To iterate over the key-value pairs in a HashTable, you can use a for-each loop with the entrySet() method:\nfor (Hashtable.Entry&lt;String, Integer&gt; entry : myTable.entrySet()) {\n    System.out.println(entry.getKey() + \": \" + entry.getValue());\n}\n\nA HashTable can be a useful data structure when you need to store key-value pairs and require thread-safe operations. However, it has some performance overhead due to synchronization, so if thread safety is not a concern, a HashMap is generally a more efficient choice."
  },
  {
    "objectID": "22_hashing.html#hashsets-in-java",
    "href": "22_hashing.html#hashsets-in-java",
    "title": "17  Hashing, Hash Tables, and Hash Maps",
    "section": "17.11 HashSets in Java",
    "text": "17.11 HashSets in Java\nA HashSet is a collection in Java that implements the Set interface and uses a hash table for storage. It does not store key-value pairs like hash tables or hash maps, but instead stores unique elements. The elements in a HashSet are not ordered, and duplicate values are not allowed.\nHere’s how to use a HashSet in Java:\n\nImport the HashSet class: To use the HashSet class in your Java code, you’ll need to import it from the java.util package:\nimport java.util.HashSet;\nCreate a HashSet: To create a new HashSet, use the following syntax:\nHashSet&lt;String&gt; mySet = new HashSet&lt;String&gt;();\nAdd elements: To add elements to the HashSet, use the add() method:\nmySet.add(\"apple\");\nmySet.add(\"banana\");\nmySet.add(\"orange\");\nRemove elements: To remove elements from the HashSet, use the remove() method:\nmySet.remove(\"banana\");\nCheck if an element exists: To check if an element is in the HashSet, use the contains() method:\nboolean hasApple = mySet.contains(\"apple\"); // true\nboolean hasGrape = mySet.contains(\"grape\"); // false\nIterate over elements: To iterate over the elements in a HashSet, you can use a for-each loop:\nfor (String fruit : mySet) {\n    System.out.println(fruit);\n}\n\nA HashSet can be a useful data structure when you need to store a collection of unique elements without any specific order. It provides constant-time performance for common operations like add, remove, and contains, making it an efficient choice for many applications."
  },
  {
    "objectID": "22_hashing.html#hascode-and-equals-in-java",
    "href": "22_hashing.html#hascode-and-equals-in-java",
    "title": "17  Hashing, Hash Tables, and Hash Maps",
    "section": "17.12 hasCode and equals in Java",
    "text": "17.12 hasCode and equals in Java\nIn Java, the hashCode method is part of the Object class, which is the superclass of all Java classes. The purpose of the hashCode method is to provide a default implementation for generating hash codes, which are integer values that represent the memory address of an object.\n\n17.12.1 The hashCode Method\nThe hashCode method has the following signature:\npublic int hashCode()\nThis method returns an integer hash code for the object on which it is called. By default, it returns a hash code that is based on the object’s memory address, but this behavior can be overridden in subclasses to provide custom hash code generation.\nA well-implemented hashCode method should follow these general rules:\n\nIf two objects are equal according to their equals() method, they must have the same hash code.\nIf two objects have the same hash code, they are not necessarily equal according to their equals() method.\nThe hash code of an object should not change over time unless the information used in the equals() method also changes.\n\n\n\n17.12.2 Overriding the hashCode Method\nWhen creating custom classes, it is important to override the hashCode method if the equals() method is also overridden. This ensures that the general contract of the hashCode method is maintained, which is essential for the correct functioning of hash-based data structures like HashSet and HashMap.\nHere’s an example of a custom Person class that overrides both the equals() and hashCode() methods:\npublic class Person {\n    private String name;\n    private int age;\n\n    // Constructor, getters, and setters\n\n    @Override\n    public boolean equals(Object obj) {\n        if (this == obj) {\n            return true;\n        }\n        if (obj == null || getClass() != obj.getClass()) {\n            return false;\n        }\n        Person person = (Person) obj;\n        return age == person.age && Objects.equals(name, person.name);\n    }\n\n    @Override\n    public int hashCode() {\n        return Objects.hash(name, age);\n    }\n}\nIn this example, the equals() method checks if two Person objects have the same name and age. The hashCode() method uses the Objects.hash() utility method, which generates a hash code based on the name and age fields.\n\n\n17.12.3 Using hashCode with Java Collections\nThe hashCode method plays a crucial role in the performance of Java’s hash-based data structures, such as HashSet, HashMap, and HashTable. These data structures rely on the hashCode method to efficiently store and retrieve objects based on their hash codes.\nWhen working with these collections, it is important to ensure that the hashCode method is correctly implemented for the objects being stored. Failing to do so can lead to poor performance or incorrect behavior.\nIn summary, the hashCode method in Java is a critical part of the Object class that provides a default implementation for generating hash codes. When creating custom classes, it is essential to override the hashCode method if the equals() method is also overridden, ensuring the correct functioning of hash-based data structures like HashSet and HashMap."
  }
]