[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ITSC 2214 - Data Structures and Algorithms - Summer 2023",
    "section": "",
    "text": "Preface\nThis website contains a set of readings for ITSC 2214 - Data Structures and Algorithms."
  },
  {
    "objectID": "01_intro_to_cs_dsa.html#computing-capabilities",
    "href": "01_intro_to_cs_dsa.html#computing-capabilities",
    "title": "1  Background on Computer Science and Data Structures",
    "section": "1.1 Computing Capabilities",
    "text": "1.1 Computing Capabilities\nComputers, in the broadest sense, are devices that can perform calculations or manipulate information. Throughout history, humans have invented and used various types of computers, each with increasing capabilities and complexity. Here are some examples of how computing capabilities have evolved over time:\n\nTally stick - One of the earliest forms of computers, dating back to prehistoric times. A tally stick is a piece of wood or bone with notches carved into it to record numbers or events. The computations it could do were incrementing and retrieving one piece of information. For example, a shepherd could use a tally stick to keep track of his sheep by making a notch for each one.\nAbacus - A manual device used for calculations by sliding counters along rods or in grooves. The abacus was invented in ancient times and is still used today in some parts of the world. It could store one set of numbers, add, subtract, multiply, divide, and perform other arithmetic operations to the stored information which can later be retrieved. For example, a merchant could use an abacus to keep track of his transactions and profits.\nAstrolabe - A sophisticated instrument used for astronomy and navigation by measuring the positions and movements of celestial bodies. The astrolabe was developed in ancient Greece and reached its peak in the Islamic Golden Age. It could perform complex calculations such as determining the time, latitude, longitude, and direction based on the observation of stars and planets. For example, a sailor could use an astrolabe to find his way across the sea by aligning it with the sun or the pole star.\nAntikythera mechanism - A mechanical device that simulated the motions of the sun, moon, and planets according to a geocentric model. The Antikythera mechanism was discovered in a shipwreck near the Greek island of Antikythera in 1901. It is estimated to date back to the 2nd century BC and is considered one of the first analog computers. It could predict astronomical phenomena such as eclipses, phases of the moon, and positions of the zodiac signs. For example, a priest could use the Antikythera mechanism to plan religious ceremonies and festivals based on the celestial calendar.\nDifference engine - A mechanical calculator that could compute polynomial functions using the method of finite differences. The difference engine was designed by Charles Babbage in the early 19th century but was never fully completed due to technical and financial difficulties. It could generate accurate tables of values for various mathematical functions such as logarithms, trigonometry, and navigation. For example, a mathematician could use the difference engine to check his calculations and avoid errors.\nAnalytical engine - A proposed mechanical computer that could perform any calculation given a set of instructions or a program. The analytical engine was also designed by Charles Babbage in the mid-19th century but was never built due to his death and lack of funding. It is considered the first general-purpose computer and the precursor of modern computers. It could store data in memory, process data using arithmetic and logical operations, control the flow of execution using conditional branching and looping, and output data using a printer or a punch card. For example, Ada Lovelace, who wrote the first algorithm for the analytical engine, envisioned that it could compose music based on mathematical rules.\n\nHere is a summary of the computing capabilities of some of these devices -\n\nTally stick: Store and retrieve one piece of data\nAbacus: Store and retrieve one piece of data, and perform basic arithmetical operations on them with another operand.\nSystem of Gears: Store and retrieve one piece of data. Each gear can use stored data and scale it up or down (multiply or divide) by a fixed constant determined by the gear ratio.​\nDifference engine: Can perform complex, but non-programmable computations. Produced tables of input-output pairs.\nAnalytical engine: Can perform complex, programmable computations. Can store data in memory, process data using arithmetic and logical operations, control the flow of execution using conditional branching and looping, and output data using a printer or a punch card."
  },
  {
    "objectID": "01_intro_to_cs_dsa.html#what-is-computer-science-then",
    "href": "01_intro_to_cs_dsa.html#what-is-computer-science-then",
    "title": "1  Background on Computer Science and Data Structures",
    "section": "1.2 What is Computer Science then?",
    "text": "1.2 What is Computer Science then?\nThe science of computers, or Computer Science, seeks to answer fundamental questions like: What are the essential parts of a computer? What can be computed, and what cannot? What determines the ease and speed of computation? This field provides the foundation for understanding the principles that drive computational systems."
  },
  {
    "objectID": "01_intro_to_cs_dsa.html#data-structures-and-algorithms",
    "href": "01_intro_to_cs_dsa.html#data-structures-and-algorithms",
    "title": "1  Background on Computer Science and Data Structures",
    "section": "1.3 Data Structures and Algorithms",
    "text": "1.3 Data Structures and Algorithms\nThe field of Data Structures and Algorithms expands upon the principles of Computer Science. It determines the efficiency of computation by answering the question: How easily or quickly can something be computed, and what factors influence these metrics? It outlines the necessary building blocks or tools required for programming, and explores the common patterns and problems in programs, offering known ways to enhance their speed. It is this branch of Computer Science that gives us the skills to design, write and analyze the efficiency of our programs. This understanding is crucial to becoming proficient in the broader field of Computer Science.\nIn essence, the intertwined journey of computers and computing science is a testament to human ingenuity and the relentless pursuit of understanding and harnessing the principles that underlie our world. As we delve deeper into the concepts of data structures and algorithms, we continue to contribute to this exciting journey."
  },
  {
    "objectID": "01_intro_to_cs_dsa.html#review",
    "href": "01_intro_to_cs_dsa.html#review",
    "title": "1  Background on Computer Science and Data Structures",
    "section": "1.4 Review",
    "text": "1.4 Review\nWe talked a little about what computers are, and what capabilities a device needs to have to be able to compute certain types of problems.​ The next chapter will be about “general purpose, programmable computers”, and how they work.\nHere’s a fun exercise for you -\n\nAre analog wristwatches computers? What do they compute?\nCan you design a system of gears that can convert Fahrenheit to Celcius?"
  },
  {
    "objectID": "02_general_purpose_computers.html#the-working-of-a-processor",
    "href": "02_general_purpose_computers.html#the-working-of-a-processor",
    "title": "2  Modern General-Purpose, Programmable Computers",
    "section": "2.1 The Working of a Processor",
    "text": "2.1 The Working of a Processor\nThe way Cory and Jamal communicate parallels how a computer’s processor operates. A processor has a set of operations it can perform - like copying, moving, adding, subtracting data, jumping to another instruction, conditional branching, and more.\nTo tell a computer to perform a certain operation, we need several indicators, just like Cory and Jamal’s system. Computers use tiny wires that may or may not have electricity running through them. The presence or absence of electricity indicates what the processor should do.\nWe refer to the absence as a 0 and the presence of electricity as a 1. Each wire conveying either a 0 or a 1 is said to convey one bit of information.\nThis is a fixed group of wires going into the processor to convey such information on what needs to be done. This grouping is known as “Instruction.” The number of bits in an instruction tells us the `size`` of the instruction.\nA processor usually has a fixed instruction size (64 for a 64-bit processor, 32 for a 32-bit processor, etc.).\nSometimes even more information is needed. For example, for operations like jumping to a different instruction, the processor needs to know where to jump to. For operations like addition, two numbers are needed. Sometimes, along with the operation, the processor also needs to be instructed about where to store the result. We call these pieces of data on which we perform instructions as operands. We often refer to instructions as operations.\nWe cannot have an extra set of wires for each time we need to share another operand with the processor, so we share this data in sequence.\nSay 0010 1010 is the code for the add operation, 0000 0000`` is the number 0, and0000 0001` is the number 1. If we want an 8-bit processor to add these together, we might share the following data through the wires -\n...\n0000 0001 // third cycle - second operand\n0000 0000 // second cycle - first operand\n0010 1010 // first cycle - add operation\nThere is a “clock” mechanism within the processor to signal the start and end of “cycles”. Generally the processor “pipelines” its operation such that it can complete one instruction in each cycle. However, many complex instructions like division or square roots take &gt;20 cycles to complete.\nA computer processor can be compared to a complex Rube Goldberg machine, with wires that take in the presence or absence of electricity similar to how many Rube Goldberg machines’ mechanism is kicked off by a rolling ball or marbles. The “marbles” of electricity roll through the processor, along each wire, triggering complex chain reactions or side effects that are designed to elicit the intended operation. Consider that a processor, as small as it is, frequently contains billions of transistors - use this information to imagine how complex these “Rube Goldberg machines” are."
  },
  {
    "objectID": "02_general_purpose_computers.html#memory",
    "href": "02_general_purpose_computers.html#memory",
    "title": "2  Modern General-Purpose, Programmable Computers",
    "section": "2.2 Memory",
    "text": "2.2 Memory\nLet’s compare the computer’s memory to a vast grid filled with tiny cells. These cells can each store some electricity - and again the presence or absence of electricity conveys a bit of information. If we continue the previous analogy, it can be compared to a large grid storing balls (data).\nThe processor has instructions that allow it to select which cells to “read” and transfer those bits to the processor - to be interpreted either as data or instructions. The processor also has instructions that allow it to write to memory.\nIt is this interaction of a processor being able to “write” instructions and data to memory, and then conditionally “read” and execute them, and then write new instructions or data that makes a processor “programmable”.\nWhat we describe in this section is Active memory or RAM. It is the data in RAM that is readily available for the processor to read and write."
  },
  {
    "objectID": "02_general_purpose_computers.html#programs",
    "href": "02_general_purpose_computers.html#programs",
    "title": "2  Modern General-Purpose, Programmable Computers",
    "section": "2.3 Programs",
    "text": "2.3 Programs\nPrograms are essentially detailed sets of instructions that command a computer to execute specific operations. They can be likened to a recipe that a computer follows to achieve a particular task. Programs dictate what steps the computer must take and in what sequence to reach the desired outcome.\nThe program is stored on disk in a format that is standard for that operating system, and where the first instruction is stored in this format is always known - say, the first instruction in a “main” section. When a program is executed, it is loaded from the computer’s storage into RAM, and then the processor reads and executes the first instruction; and so the execution begins.\nThe complexity of programs can vary greatly, from a simple one that performs basic arithmetic to an intricate operating system like Windows or Linux that manages every aspect of a computer. However, the core characteristic of all programs is the same: they are sequences of instructions that the computer follows."
  },
  {
    "objectID": "02_general_purpose_computers.html#the-java-compiler",
    "href": "02_general_purpose_computers.html#the-java-compiler",
    "title": "2  Modern General-Purpose, Programmable Computers",
    "section": "2.4 The Java Compiler",
    "text": "2.4 The Java Compiler\nAs instructions are hard to write directly, we make use of programs called compilers that take in “code” in human-readable text format and output a list of instructions. The program is designed in such a way that the produced list of valid instructions always carries out the task described in “code” faithfully.\nThe Java compiler is such a program for Java code. When a Java program is compiled, the compiler reviews the code for syntax errors and then translates it into bytecode, a type of intermediate language closer to machine language. The Java Virtual Machine (JVM) then interprets this bytecode into machine code that your computer’s processor can execute.\nSyntax errors or other kinds of errors essentially refer to situations where the compiler doesn’t know how to, or cannot produce a valid set of instructions that can carry out what the code is describing.\nCompilers like the Java compiler don’t just translate code. They also optimize it, making it more efficient so that the resulting program runs faster and consumes less memory. For example, they may look at your for loop that is adding up the first N natural numbers and decide to replace the loop with the formula for this computation instead."
  },
  {
    "objectID": "02_general_purpose_computers.html#operating-systems",
    "href": "02_general_purpose_computers.html#operating-systems",
    "title": "2  Modern General-Purpose, Programmable Computers",
    "section": "2.5 Operating Systems",
    "text": "2.5 Operating Systems\nWith a sound understanding of the fundamental components of modern computing, it’s important to highlight the role of the operating system.\nAn operating system (OS) is a type of system software that manages computer hardware and software resources and provides various services for computer programs. It acts as a mediator between users and the computer hardware. Users interact with the operating system through user interfaces such as a command-line interface (CLI) or a graphical user interface (GUI).\nOperating systems bear the responsibility of managing the computer’s resources, including the processor, memory, disk space, and input/output devices. They coordinate tasks, ensuring that the processor’s time is used judiciously, and manage the memory, keeping track of which parts are in use and which are available.\nIn essence, the operating system provides the platform on which all other software runs. It is the environment in which programs, written in languages like Java and then compiled, operate.\nThis fundamental understanding of modern computing components helps elucidate the intricate operations that are continuously happening within our laptops, desktops, and even our smartphones. These fundamental aspects form the backbone of the digital age we live in."
  },
  {
    "objectID": "03_algorithmic_analysis.html#problems-algorithms-and-programs",
    "href": "03_algorithmic_analysis.html#problems-algorithms-and-programs",
    "title": "3  Algorithmic Analysis",
    "section": "3.1 Problems, Algorithms, and Programs",
    "text": "3.1 Problems, Algorithms, and Programs\nBefore understanding algorithmic analysis, it’s essential to differentiate between problems, algorithms, and computer programs. These are three distinct concepts that are interrelated.\n\n3.1.1 Problems\nA problem in computer science refers to a specific task that needs to be solved. It can be thought of in terms of inputs and matching outputs. For instance, to solve the problem of finding the youngest student in our class, the input would be the names and ages of all students in the class. The output would be the name of the youngest student.\nIt’s helpful to perceive problems as functions in a mathematical sense. In mathematics, a function is a relationship or correspondence between two sets — the input set (domain) and the output set (range).\n\n\n3.1.2 Algorithms\nAn algorithm, on the other hand, is a method or a process followed to solve a problem. If we perceive the problem as a function, then an algorithm can be seen as an implementation of this function that transforms an input into the corresponding output.\nSince there are typically numerous ways to solve a problem, there could be many different algorithms for the same problem. Having multiple solutions is advantageous because a specific solution might be more efficient than others for certain variations of the problem or specific types of inputs.\nFor instance, one sorting algorithm might be best suited for sorting a small collection of integers. Another might excel in sorting a large collection of integers, while a third might be ideal for sorting a collection of variable-length strings.\nBy definition, a sequence of steps can only be called an algorithm if it fulfills the following properties:\n\nIt must be correct.\nIt consists of a series of concrete steps.\nThere is no ambiguity about the step to be performed next.\nIt must comprise a finite number of steps.\nIt must terminate.\n\n\n\n3.1.3 Programs and Their Building Blocks\nBefore discussing programs in detail, let’s briefly review two essential components that make a program run: the CPU (Central Processing Unit) and memory.\nA CPU is the electronic circuitry within a computer that has the ability to execute certain instructions. Its primary function is to fetch, decode, and execute instructions. It has slots to store data, referred to as registers. Communicating with a CPU involves telling it what operation you want to perform and on which data. For instance, you might instruct the CPU to add two numbers stored in registers A and B and store the result in register C.\nCPU instructions are binary codes that specify which operation the CPU should perform. Here’s an example of what they look like:\n10010011001100111110000111011111\nSome bits in the instruction form the opcode, the operation code. The opcode is a unique identifier for an operation, like adding integers. Other bits form the operand(s), the data on which to operate. The operand can be where the data is stored (the name of a register or an address in memory), or where to store the result of the operation (again, the name of a register or an address in memory).\nFor human readability, there are notations to represent these binary instructions. Here is an example of a set of instructions in a human-readable form:\n.global main\nmain:\naddi   sp, sp, -16\nsd     t0, 0(sp)\nsd     t1, 8(sp)\ncall   some_function\nld     t0, 0(sp)\nld     t1, 8(sp)\n# Use t0 and t1 here as if nothing happened.\naddi   sp, sp, 16\nPrograms are structured into sections. They include code sections, which contain a list of instructions, and data sections, which hold binary data such as text, images, or numbers that the program needs to use. Typically, there is a designated “main” section that contains the instructions to be executed first.\nWhen you initiate an executable (with the exception of Mac “applications”), the binary data (“bits”) are read from the hard disk and transferred to the main memory (RAM). The execution of the program begins when the first instruction from the “main” section is transferred to the CPU.\nIn this context, a computer program’s code can be seen as an instance, or concrete representation, of an algorithm. Although the terms “algorithm” and “program” are distinct, they are often used interchangeably for simplification."
  },
  {
    "objectID": "03_algorithmic_analysis.html#comparing-the-performance-of-programs",
    "href": "03_algorithmic_analysis.html#comparing-the-performance-of-programs",
    "title": "3  Algorithmic Analysis",
    "section": "3.2 Comparing the Performance of Programs",
    "text": "3.2 Comparing the Performance of Programs\nWhen you compile or build a program, its code is converted into a series of instructions and data in memory. However, the execution time of the same program can vary across different machines due to differences in the processor’s capabilities.\nAs each machine can potentially have a different processor, comparing the speed of programs can be a complex task, and is only meaningful when the processor is the same or standardized. Even when the processor is standardized, many factors affect performance -\n\nBackground tasks on one machine can interfere with performance measurements.\nEven if you have the exact same processor, differences in manufacturing mean each can run at a different clock frequency.\nSmall differences in ambient temperature affect how high a processor can clock.\nThe mounting pressure of a cooler can affect heat transfer and in turn how high a processor can clock.\nMany cooling solutions involve vapor chambers. The orientation of vapor chambers can affect heat transfer and in turn how high a processor can clock.\nEven cosmic radiation can affect processors and memory.\n\nTherefore, we usually prefer to compare algorithms instead. The methods of comparing algorithms will be discussed in the following sections of this chapter."
  },
  {
    "objectID": "03_algorithmic_analysis.html#analyzing-algorithms",
    "href": "03_algorithmic_analysis.html#analyzing-algorithms",
    "title": "3  Algorithmic Analysis",
    "section": "3.3 Analyzing Algorithms",
    "text": "3.3 Analyzing Algorithms\nOne of the key components of this course is to provide a framework for predicting the performance of algorithms just by inspecting their structure. Let’s dive into the process of analyzing an algorithm’s time complexity.\n\n3.3.1 Predicting Execution Time\nConsider a simple method that adds two numbers:\npublic int add(int lhs, int rhs)\nSuppose calling add(2, 4) takes 1 second. How long would add(40, 50) or add(343245634, 32432423) take? As you might expect, all these operations, despite the difference in magnitude of the numbers involved, take approximately the same time. That’s because the time complexity of an addition operation does not depend on the values of the numbers but on the number of operations involved, which, in this case, is a single addition.\n\n\n3.3.2 Impact of Input Size\nNow, let’s examine a slightly more complex method that sums up a list of numbers:\npublic int sumOfList(List&lt;int&gt; l)\nAssuming that adding two numbers takes one second, how long would summing a list of 10 numbers take? We can infer that the time taken by sumOfList depends on the size of the list we provide. For instance, summing up a list of 10 numbers would take about half the time needed to sum up a list of 20 numbers.\nHere’s an implementation of sumOfList:\nimport java.util.ArrayList;\n\nclass Square {\n  static int sumOfList(ArrayList&lt;Integer&gt; l) {\n    int sum = 0;\n    for (int i : l) {\n      sum += i;\n    }\n    return sum;\n  }\n}\nThe key insight is that the execution time of this method depends on the number of elements in the list - which is the size of the input. The time complexity is directly proportional to the number of times the addition statement is executed, which is equal to the size of the list.\n\n\n3.3.3 Iterations and Input Size\nLet’s take it a step further. If you’re summing a list of N items, each of which is another list of M items, the operation would take N * M addition statements. Here, the time complexity is a function of both N and M.\n\n\n3.3.4 Gauging Relative Execution Time\nThe crux of analyzing an algorithm’s performance lies in understanding how many times statements in the program run as a function of the size of the input. This approach enables us to estimate how long two invocations of the same method will take relative to each other, given the size of the input for each.\nUnderstanding this concept will allow you to better predict the performance of algorithms, which is a crucial skill in efficient programming and system design."
  },
  {
    "objectID": "03_algorithmic_analysis.html#algorithm-complexities",
    "href": "03_algorithmic_analysis.html#algorithm-complexities",
    "title": "3  Algorithmic Analysis",
    "section": "3.4 Algorithm Complexities",
    "text": "3.4 Algorithm Complexities\nIn order to evaluate an algorithm’s efficiency, we analyze its time complexity and space complexity, both of which describe how the algorithm’s performance scales with the size of the input.\n\n3.4.1 Time Complexity\nTime complexity measures how the execution time of an algorithm increases with the size of the input. For instance, counting how many times the number \\(5\\) appears in a list requires checking each number in the list. Hence, the time complexity is directly proportional to the size of the list.\n\n\n3.4.2 Space Complexity\nSpace complexity quantifies the amount of memory an algorithm requires relative to the size of the input. Using the previous example, we would need enough space to store the list and an additional space to store the counter. Thus, the space complexity is proportional to the size of the list, or more precisely, \\(N + 1\\).\n\n\n3.4.3 Conditional Statements and Complexity\nIn cases where conditional statements are present, the number of executed statements depends on which branch the program takes. One branch may contain more statements than the other. To handle such scenarios, we introduce the concept of Big-O, Big-Ω, and Big-Θ notations.\n\n3.4.3.1 Big-O Notation\nThe Big-O notation describes the worst-case time complexity of an algorithm, essentially providing an upper bound on the time taken. This notation considers the scenario where the program consistently takes the path with the most statements. While Big-O is commonly used for time complexity, it can also describe space complexity.\nThe notation comprises two parts: the function itself and the variable representing the input size. Generally, ‘n’ is used to represent the input size, and constants and coefficients are typically ignored. For instance, the following functions are all \\(O(n)\\):\n\n\\(n + 1\\)\n\\(2n\\)\n\\(103n + 124\\)\nOnly the highest degree of ‘n’ is considered when determining Big-O notation. Therefore, functions such as n², n/2, or √n are not considered O(n).\n\n\n\n3.4.3.2 Big-Ω Notation\nThe Big-Ω notation represents the best-case complexity of an algorithm. It follows the same format as Big-O notation but focuses on the scenario where the program consistently takes the path with the fewest statements.\n\n\n3.4.3.3 Big-Θ Notation\nThe Big-Θ notation is used to denote the average-case complexity of an algorithm. It again follows the same structure as Big-O, but it considers both the best and worst-case scenarios to provide an average estimate of the algorithm’s performance.\nRemember, these notations and complexities are pivotal in estimating the performance of an algorithm based on the size or other properties of the input, correlating to the number of steps taken by the algorithm. This understanding is crucial when designing efficient and effective solutions in computer science."
  },
  {
    "objectID": "03_algorithmic_analysis.html#common-big-o-complexities",
    "href": "03_algorithmic_analysis.html#common-big-o-complexities",
    "title": "3  Algorithmic Analysis",
    "section": "3.5 Common Big-O Complexities",
    "text": "3.5 Common Big-O Complexities\nBig-O notation is a way of expressing the worst-case time complexity of an algorithm. It describes how the running time of an algorithm changes as the size of its input grows. The most common Big-O complexities are:\n\n3.5.1 Constant Complexity and Growth\nConstant complexity, often represented as \\(O(1)\\), occurs when the running time of an algorithm or the amount of work needed does not change with the size of the input (\\(N\\)). This means the algorithm takes a fixed amount of time, regardless of how many elements it is processing.\nFor example, accessing an element in an array by its index is an operation of constant complexity. This is because it takes roughly the same amount of time, regardless of the size of the array:\n\n\\(f(n) = 1\\) for all \\(n\\)\n\nIn this case, no matter how large or small our input size is, the amount of work we have to do remains the same. This is the most efficient complexity an algorithm can have.\n\n\n3.5.2 Linear Complexity and Growth\nLinear complexity, often represented as \\(O(n)\\), occurs when the running time of an algorithm or the amount of work needed scales proportionally with the size of the input (\\(N\\)). For every additional element in the input, a fixed amount of work is added.\nFor example, finding an element in an unsorted list is an operation of linear complexity, as the algorithm might need to look at every element once:\n\n\\(f(n) = n\\)\n\nIn this case, if we add one more element to our input size, we add one more unit of work. This is because every additional element requires the same amount of work.\n\n\n3.5.3 Quadratic Complexity and Growth\nQuadratic complexity, often represented as \\(O(n^2)\\), occurs when the running time of an algorithm or the amount of work needed scales with the square of the size of the input (\\(N\\)). For every additional element in the input, the work increases by a factor of \\(n\\).\nFor example, a simple nested loop for comparing pairs of elements in a list has quadratic complexity. This is because each element is compared to every other element:\n\n\\(f(n) = n^2\\)\n\nIn this case, if we add one more element to our input size, we add \\(n\\) units of work, as we have to compare this new element with every other element already in the list.\n\n\n3.5.4 Exponential Functions and Growth\nAn exponential function is one that includes a variable in the exponent. To illustrate, the function \\(2^n\\) is an exponential function. Here, with each unit increase in the input, the output is multiplied (or scaled up) by a factor of 2. For instance:\n\n\\(f(1) = 2^1 = 2\\)\n\\(f(2) = 2^2 = 4\\)\n\\(f(n+1) = f(n) * 2\\)\n\nExponential growth is characterized by a constant factor scaling up the running time of the algorithm, or the amount of work needed, for each unit increase in the size of the input (often denoted as \\(N\\)). This constant factor is the base of the exponent. This means that for an algorithm with a time complexity of \\(2^N\\), adding one more element to the input could potentially double the amount of work required. Examples of algorithms exhibiting exponential growth include certain solutions to problems like the Towers of Hanoi and calculations of the Fibonacci sequence.\n\n\n3.5.5 Logarithmic Functions and Growth\nLogarithmic functions serve as the inverse of exponential functions. For example, \\(log_2(n)\\) is a logarithmic function. Here, the output is decremented by 1 each time the input is divided (or scaled down) by a factor of 2. For instance:\n\n\\(f(16) = log_2(16) = 4\\)\n\\(f(8) = log_2(8) = 3\\)\n\\(f(n/2) = f(n) - 1\\)\n\nLogarithmic growth is characterized by a decrement of 1 in the running time of the algorithm or the amount of work needed, each time the size of the input (denoted as \\(N\\)) is divided by a constant factor. This constant factor is the base of the logarithm. For an algorithm with a time complexity of \\(log_2(n)\\), if we have an input size of 16, doubling the input size will only increase the amount of work by 1 unit. Similarly, halving the input size will decrease the work by 1 unit.\nAlgorithms often display complexities such as \\(N*log_2(N)\\), which represents a combination of linear and logarithmic growth. Examples of such algorithms that include logarithmic growth are binary search and certain sorting algorithms.\n\n\n3.5.6 Examples\nLet’s look at some examples of algorithms and their Big-O complexities.\nstatic int findMin(x, y) {\n  if (x &lt; y) {\n    return x;\n  } else {\n    return y;\n  }\n}\nThis algorithm finds the minimum of two numbers x and y. It does not depend on the input size, since it only performs one comparison and one return statement. Therefore, its worst-case complexity is \\(O(1)\\). Its best case and average case are also \\(O(1)\\) since they are the same as the worst case.\nstatic int linearSearch(numbers[], target)\n  for (int i = 0; i &lt; numbers.length; i++) {\n    if (numbers[i] == target) {\n      return i;\n    }\n  }\n  return -1;\n}\nThis algorithm performs a linear search on an array of numbers to find a target value. It iterates through each element of the array until it finds the target or reaches the end of the array. In the worst case, it has to check every element of the array, which means its worst-case complexity is \\(O(n)\\), where n is the length of the array. In the best case, it finds the target in the first element, which means its best-case complexity is \\(O(1)\\). In the average case, it finds the target somewhere in the middle of the array, which means its average-case complexity is also \\(O(n)\\).\nWe will talk about more examples of other common worst-case complexities throughout this course."
  },
  {
    "objectID": "03_algorithmic_analysis.html#growth-rate",
    "href": "03_algorithmic_analysis.html#growth-rate",
    "title": "3  Algorithmic Analysis",
    "section": "3.6 Growth Rate",
    "text": "3.6 Growth Rate\nThe growth rate for an algorithm is the rate at which the cost of the algorithm grows as the size of its input grows. The cost can be measured in terms of time, space, or other resources. The worst-case complexity notation essentially denotes the growth rate of the time complexity with respect to the size of a worst-case input.\nThe table below summarizes how different Big-O complexities compare in terms of their growth rates.\n\n\n\nComplexity\nGrowth Rate\n\n\n\n\nO(1)\nConstant\n\n\nO(\\(log(n)\\))\nLogarithmic\n\n\nO(n)\nLinear\n\n\nO(n\\(^2\\))\nQuadratic\n\n\nO(\\(2^n\\))\nExponential\n\n\n\nAs we can see, constant and logarithmic complexities have very low growth rates, meaning that they are very efficient and scalable algorithms. Linear complexity has a moderate growth rate, meaning that it can handle reasonably large inputs but may become slow for very large inputs. Quadratic and exponential complexities have very high growth rates, meaning that they are very inefficient and unscalable algorithms that can only handle small inputs."
  },
  {
    "objectID": "03_algorithmic_analysis.html#summary-review",
    "href": "03_algorithmic_analysis.html#summary-review",
    "title": "3  Algorithmic Analysis",
    "section": "3.7 Summary / Review",
    "text": "3.7 Summary / Review\nIn this section, we learned how to analyze the performance of algorithms using Big-O notation. We saw that measuring the actual running time of an algorithm on real hardware is difficult and impractical because it depends on many factors such as the hardware specifications, the programming language, the compiler, the input size, and the input distribution. Therefore, we need a way to simplify and standardize the performance analysis across different hardware and software platforms.\nWe learned that one way to simplify the performance analysis is to count the number of steps or instructions that an algorithm needs to execute before finishing. We saw that these steps are analogous to the instructions generated by a compiler when it translates our code into machine code. We also learned that different steps may have different costs depending on their complexity, but we can ignore these differences for simplicity and focus on the overall number of steps.\nWe learned that Big-O notation is a mathematical tool that allows us to express the worst-case complexity of an algorithm. It describes how the number of steps grows as a function of the input size in the worst possible scenario. It gives us an upper bound on the performance of an algorithm, meaning that it tells us how slow an algorithm can get in the worst case. We also learned that Big-O notation ignores constant factors and lower-order terms because they become insignificant as the input size grows.\nWe learned about some common Big-O complexities and their growth rates, such as constant, logarithmic, linear, quadratic, and exponential. We saw some examples of algorithms and their Big-O complexities, and how to analyze them using simple rules such as adding complexities for sequential steps, multiplying complexities for nested steps, and taking the maximum complexity for conditional steps.\nWe learned that Big-O notation helps us compare different algorithms and choose the most efficient one for a given problem. It also helps us estimate how well an algorithm can scale to larger inputs and how it can affect the performance of our applications."
  },
  {
    "objectID": "04_unit_testing.html#background",
    "href": "04_unit_testing.html#background",
    "title": "4  Unit Testing and Test-Driven Development",
    "section": "4.1 Background",
    "text": "4.1 Background\nImagine you are an architect and you’ve just designed a large, complex building - let’s say a skyscraper. This skyscraper is not merely a single entity; it’s composed of thousands of individual components - the plumbing, the electrical wiring, the elevators, the heating system, and the building’s structural elements, among many others. Now, how would you ensure that the skyscraper works as intended? Would you wait until the entire building is constructed and then start testing every possible scenario? Obviously, this approach is time-consuming, and it exposes you to significant risk.\nA similar challenge exists in the world of software development. Take, for example, a web browser. This is a complex piece of software with hundreds of classes interacting in intricate ways. These classes and methods perform various tasks such as rendering HTML and CSS, processing JavaScript, managing cookies, implementing security features, and many others. Ensuring the correct functionality of this software is a daunting task, given the vast range of potential inputs. After all, there are billions of web pages on the internet, each with its unique combination of technologies, designs, and user interactions. How can you guarantee that your browser works flawlessly with all of them? A naive approach would be to load each web page and observe the output, but this process is not only time-consuming but also practically impossible.\nThis conundrum begs the question: How can we validate the correct functionality of a software product efficiently? The direction points towards automation - the ability to conduct tests without manual intervention. But how can we achieve this, especially given the enormous application surface area?\nThis is where unit testing and Test-Driven Development (TDD) come in."
  },
  {
    "objectID": "04_unit_testing.html#the-power-of-unit-testing",
    "href": "04_unit_testing.html#the-power-of-unit-testing",
    "title": "4  Unit Testing and Test-Driven Development",
    "section": "4.2 The Power of Unit Testing",
    "text": "4.2 The Power of Unit Testing\nWhile the surface area of an entire application is vast, the surface area of individual classes and units of code within the software project is significantly smaller. If we can write tests to verify that each method within each class functions correctly for all possible inputs, we reduce the complexity of the problem.\nAt first glance, it might seem like an overwhelming task. Even a moderately complex software project can have thousands of methods spread across hundreds of classes. Writing tests for all of them could result in thousands of test cases. But this is precisely where automation proves its worth. By automating these tests, we can execute them each time we modify our code, ensuring the functionality remains intact. This method gives us confidence that our changes have not inadvertently introduced bugs into existing functionality.\nThe key principle here is that by ensuring each individual unit of our software behaves correctly, we can be reasonably confident that the application as a whole operates as expected, provided the software architecture is sound. In this manner, unit testing allows us to break down the monumental task of verifying a complex software system’s functionality into manageable, automated tasks.\nIn the following sections, we will delve deeper into the concept of unit testing, its implementation in Java, and the practice of Test-Driven Development, where tests actually guide and shape the development of the software. Buckle up, for we’re about to embark on an exciting journey that will fundamentally change how you approach software development!"
  },
  {
    "objectID": "04_unit_testing.html#a-basic-approach-to-unit-testing",
    "href": "04_unit_testing.html#a-basic-approach-to-unit-testing",
    "title": "4  Unit Testing and Test-Driven Development",
    "section": "4.3 A Basic Approach to Unit Testing",
    "text": "4.3 A Basic Approach to Unit Testing\nIn order to illustrate the process of unit testing in Java, let’s consider a simple utility class named MathUtil. This class defines basic arithmetic operations such as add, subtract, etc.\npublic class MathUtil {\n    public int add(int a, int b) {\n        return a + b;\n    }\n    \n    // More methods for subtract, multiply, etc.\n}\nAs we discussed in the previous section, to ensure our MathUtil functions correctly, we associate it with a MathUtilTest class. This class contains multiple test methods, each designed to verify a different scenario of the operations provided by MathUtil.\npublic class MathUtilTest {\n\n    public boolean testAdd1() {\n        MathUtil m = new MathUtil();\n        int lhs = 5;\n        int rhs = 7;\n\n        if (m.add(lhs, rhs) == lhs + rhs) {\n            return true;\n        } else {\n            return false;\n        }\n    }\n    \n    // More test methods for other cases...\n}\nIn the above example, the testAdd1 method tests the addition of two positive numbers. We could also add methods like testAdd2 to test adding a positive and a negative number, testAdd3 to test adding two negative numbers, and so forth. Each of these methods tests a specific scenario and validates that the result is as expected."
  },
  {
    "objectID": "04_unit_testing.html#recognizing-the-inefficiencies-and-redundancies",
    "href": "04_unit_testing.html#recognizing-the-inefficiencies-and-redundancies",
    "title": "4  Unit Testing and Test-Driven Development",
    "section": "4.4 Recognizing the Inefficiencies and Redundancies",
    "text": "4.4 Recognizing the Inefficiencies and Redundancies\nWhile the above approach accomplishes our objective of validating the methods in our MathUtil class, you might have already noticed that it’s far from optimal. There are several glaring issues that can make this method tedious and inefficient:\n\n4.4.1 Redundancy\nEvery test method follows a similar pattern - we perform an operation and then verify if the result matches the expected outcome. This redundancy suggests we could abstract out the verification part into a separate method.\nIn the following expanded MathUtilTest class, you can observe that testAdd2 and testAdd3 follow the exact same pattern as testAdd1. They create an instance of MathUtil, perform an operation, and then compare the result with the expected outcome. This repetitive pattern across multiple tests highlights the redundancy and inefficiency of this approach.\npublic class MathUtilTest {\n\n    public boolean testAdd1() {\n        MathUtil m = new MathUtil();\n        int lhs = 5;\n        int rhs = 7;\n\n        if (m.add(lhs, rhs) == lhs + rhs) {\n            return true;\n        } else {\n            return false;\n        }\n    }\n    \n    public boolean testAdd2() {\n        MathUtil m = new MathUtil();\n        int lhs = -5;\n        int rhs = 7;\n\n        if (m.add(lhs, rhs) == lhs + rhs) {\n            return true;\n        } else {\n            return false;\n        }\n    }\n\n    public boolean testAdd3() {\n        MathUtil m = new MathUtil();\n        int lhs = -5;\n        int rhs = -7;\n\n        if (m.add(lhs, rhs) == lhs + rhs) {\n            return true;\n        } else {\n            return false;\n        }\n    }\n    \n    // More test methods for other cases...\n}\n\n\n4.4.2 Lack of Automation\nLet’s see how we need to currently run the tests we’ve written.\npublic class MathUtilTest {\n\n    // testAdd1, testAdd2, testAdd3, etc. test methods...\n\n    public static void main(String[] args) {\n        MathUtilTest test = new MathUtilTest();\n\n        System.out.println(\"testAdd1 result: \" + (test.testAdd1() ? \"PASS\" : \"FAIL\"));\n        System.out.println(\"testAdd2 result: \" + (test.testAdd2() ? \"PASS\" : \"FAIL\"));\n        System.out.println(\"testAdd3 result: \" + (test.testAdd3() ? \"PASS\" : \"FAIL\"));\n\n        // add more prints for other test cases\n    }\n}\nWith this main method, you can now run the MathUtilTest class, and it will execute each of the testAdd methods and print whether each test passed or failed. This method is a basic way to manually execute the tests and check their results.\nCurrently, we need to call each test method manually to run our tests. An automated system that could execute all tests for us would save time and reduce the chances of human error. However, as we will see later, there are better approaches to automation than the one we’ve used here."
  },
  {
    "objectID": "04_unit_testing.html#a-slightly-more-sophisticated-approach-to-unit-testing",
    "href": "04_unit_testing.html#a-slightly-more-sophisticated-approach-to-unit-testing",
    "title": "4  Unit Testing and Test-Driven Development",
    "section": "4.5 A slightly more sophisticated approach to Unit Testing",
    "text": "4.5 A slightly more sophisticated approach to Unit Testing\nTo alleviate the redundancy, we can create a method that compares the expected and actual results and raises an error if they do not match. This method, which we can call assertEquals, would look something like this:\npublic static void assertEquals(String testCaseName, int expected, int actual) {\n    if (expected != actual) {\n        System.out.println(testCaseName + \" result: FAIL\");\n    } else {\n        System.out.println(testCaseName + \" result: PASS\");\n    }\n}\nThen we can simplify our test methods by using assertEquals:\npublic void testAdd() {\n    MathUtil m = new MathUtil();\n\n    assertEquals(\"testAddTwoPositive\", m.add(5, 7), 13);\n    assertEquals(\"testAddTwoNegative\", m.add(-5, -7), -13);\n    assertEquals(\"testAddNegPos\", m.add(-5, 7), 2);\n}\nNow, our test case looks cleaner and easier to understand. The assertEquals method abstracts away the comparison details, leaving only the test logic in the test case. We can apply this simplification to all our test methods.\nThis approach significantly reduces the redundancy in our test code, making it easier to write and maintain our tests. However, we are still manually running each test method from the main method. What if we could also automate the execution of all test methods?"
  },
  {
    "objectID": "04_unit_testing.html#automating-test-execution",
    "href": "04_unit_testing.html#automating-test-execution",
    "title": "4  Unit Testing and Test-Driven Development",
    "section": "4.6 Automating Test Execution",
    "text": "4.6 Automating Test Execution\nWhat if we could just call a single method that runs all our test methods? Let’s define a simple runTests method that does exactly that.\npublic void runTests() {\n    testAdd();\n    // Call all other test methods here...\n}\nAnd then you can simply call the runTests method to execute all your tests:\npublic static void main(String[] args) {\n    MathUtilTest test = new MathUtilTest();\n    test.runTests();\n}\nThis approach is an improvement over manually running each test. However, it still has some drawbacks. For instance, when you add a new test method, you need to remember to add a call to this method in the runTests method. It would be better if our test framework could automatically detect and run all test methods without requiring any modifications to the runTests method. As we’ll see later, this is precisely what test frameworks like JUnit offer."
  },
  {
    "objectID": "04_unit_testing.html#summary",
    "href": "04_unit_testing.html#summary",
    "title": "4  Unit Testing and Test-Driven Development",
    "section": "4.7 Summary",
    "text": "4.7 Summary\nSo far, we have seen how unit testing can be a powerful tool in ensuring that individual units of code within a larger software application function as expected. We have also discussed and implemented a basic system for automating unit tests in Java, gradually refining this system to make it more efficient and less redundant.\nIn the following sections, we will discuss JUnit, a popular unit testing framework in Java that takes automation and convenience to the next level. We will also explore the practice of Test-Driven Development, where we let our tests guide the development of our software, helping us to write cleaner, more robust code. Stay tuned!"
  },
  {
    "objectID": "05_writing_junit_tests.html#introduction-to-junit",
    "href": "05_writing_junit_tests.html#introduction-to-junit",
    "title": "5  Writing JUnit Tests and Test-Driven Development",
    "section": "5.1 Introduction to JUnit",
    "text": "5.1 Introduction to JUnit\nJUnit is a widely used testing framework in the Java world. It automates the process of running tests and provides us with a wide range of assertion methods to validate our code. JUnit helps to simplify our test code, making it easier to read and maintain.\nSo, why is JUnit so popular?\n\nSimplicity: JUnit simplifies the process of writing and running tests. The framework handles the boilerplate code, allowing us to focus solely on writing the test cases.\nAssertion Library: JUnit provides a comprehensive set of assertion methods that help us validate our code against a wide range of conditions.\nAnnotations: JUnit uses annotations to define test methods and setup methods, making our test code easier to read and understand.\nAutomatic Test Discovery: JUnit automatically finds and runs all test methods, so we don’t have to manually list them in our code.\nIDE Integration: Most modern IDEs provide first-class support for JUnit, including features such as generating test cases and displaying test results in a friendly format.\n\nNow that you understand what JUnit is and why it’s beneficial let’s see how to use it in our MathUtil class."
  },
  {
    "objectID": "05_writing_junit_tests.html#useful-junit-assertions",
    "href": "05_writing_junit_tests.html#useful-junit-assertions",
    "title": "5  Writing JUnit Tests and Test-Driven Development",
    "section": "5.2 Useful JUnit Assertions",
    "text": "5.2 Useful JUnit Assertions\nJUnit provides a set of methods called assertions that are used to test the expected output of your code. These assertions help verify that your code behaves as expected under different conditions.\nLet’s take a look at some commonly used assertions:\n\n5.2.1 assertEquals\nThis assertion checks if two values are equal:\nassertEquals(expected, actual);\nIf actual is not equal to expected, the assertion fails, and the test method will terminate immediately.\nLet’s rewrite our addTest1 method using JUnit’s assertEquals:\n@Test\npublic void addTest1() {\n    MathUtil m = new MathUtil();\n    int lhs = 5;\n    int rhs = 7;\n\n    assertEquals(lhs + rhs, m.add(lhs, rhs));\n}\n\n\n5.2.2 assertTrue and assertFalse\nThese assertions verify if a condition is true or false, respectively:\nassertTrue(condition);\nassertFalse(condition);\nIf the condition does not meet the expectation (i.e., true for assertTrue and false for assertFalse), the assertion fails, and the test method will terminate immediately.\n\n\n5.2.3 assertNotNull and assertNull\nThese assertions check if an object is null or not:\nassertNotNull(object);\nassertNull(object);\nIf the object does not meet the expectation (i.e., not null for assertNotNull and null for assertNull), the assertion fails, and the test method will terminate immediately.\nThese are just a few examples. JUnit provides a comprehensive set of assertions to cover almost any condition you might want to verify."
  },
  {
    "objectID": "05_writing_junit_tests.html#setting-up-junit",
    "href": "05_writing_junit_tests.html#setting-up-junit",
    "title": "5  Writing JUnit Tests and Test-Driven Development",
    "section": "5.3 Setting Up JUnit",
    "text": "5.3 Setting Up JUnit\nBefore you can use JUnit, you need to make sure the library is on your classpath. This process can vary depending on the IDE and build system you’re using.\nFor our labs, we will ensure the JUnit library is on our classpath by pre-configuring the project and IDE for you. However, if you’re working on your own project, you’ll need to add the JUnit library to your project’s classpath.\nWhen working on your own projects, you might be interested in using a build system like Maven or Gradle to manage your dependencies. These build systems make it easy to add and manage dependencies in your project. For example, if you’re using Maven, you can add the following dependency to your pom.xml file:\n&lt;dependency&gt;\n    &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt;\n    &lt;artifactId&gt;junit-jupiter-api&lt;/artifactId&gt;\n   \n\n &lt;version&gt;5.7.0&lt;/version&gt;\n    &lt;scope&gt;test&lt;/scope&gt;\n&lt;/dependency&gt;\nThis will automatically download the JUnit library and add it to your project’s classpath.\nRegardless of how you added the JUnit library to your project, next, we need to import the necessary classes and annotations from JUnit. At the top of our MathUtilTest class, we add the following import statements:\nimport static org.junit.jupiter.api.Assertions.*;\nimport org.junit.jupiter.api.Test;\nimport org.junit.jupiter.api.BeforeEach;\nThe first import statement statically imports all assertion methods from Assertions, allowing us to use them directly in our code. The second import statement imports the Test annotation, which we use to denote our test methods. The third import statement imports the BeforeEach annotation, which we’ll discuss in a moment."
  },
  {
    "objectID": "05_writing_junit_tests.html#utilizing-test-and-beforeeach-annotations",
    "href": "05_writing_junit_tests.html#utilizing-test-and-beforeeach-annotations",
    "title": "5  Writing JUnit Tests and Test-Driven Development",
    "section": "5.4 Utilizing @Test and @BeforeEach Annotations",
    "text": "5.4 Utilizing @Test and @BeforeEach Annotations\nIn JUnit, we use the @Test annotation to indicate that a method is a test method. This allows JUnit to automatically discover and run this method as a test.\n@Test\npublic void addTest1() {\n    // test code...\n}\nHowever, what if we have some setup code that we want to run before each test? This is where the @BeforeEach annotation comes in. Any method annotated with @BeforeEach will be run before each @Test method.\nLet’s say we want to create a new MathUtil instance before each test:\nMathUtil m;\n\n@BeforeEach\npublic void setup() {\n    m = new MathUtil();\n}\nNow, before each test method is run, JUnit will first execute the setup method, ensuring that we have a fresh MathUtil instance for each test."
  },
  {
    "objectID": "05_writing_junit_tests.html#interpreting-junit-test-runner-output",
    "href": "05_writing_junit_tests.html#interpreting-junit-test-runner-output",
    "title": "5  Writing JUnit Tests and Test-Driven Development",
    "section": "5.5 Interpreting JUnit Test Runner Output",
    "text": "5.5 Interpreting JUnit Test Runner Output\nUnderstanding the output of the JUnit test runner is crucial for interpreting the results of your tests. This helps you diagnose issues in your code and identify exactly what went wrong. Let’s analyze the output of the junit-platform-console-standalone test runner to get a feel for how this works.\n╷\n├─ JUnit Jupiter ✔\n├─ JUnit Vintage ✔\n│  └─ BinarySearchTreeHiddenTest ✔\n│     ├─ testInsertAndSearch ✔\n│     ├─ testDeleteSingleNode ✔\n│     ├─ testTreeTraversals ✘ expected:&lt;[2, 3, 4, 5, [6, 7], 8]&gt; but was:&lt;[2, 3, 4, 5, [7, 6], 8]&gt;\n│     ├─ testContainsElementNotInTree ✔\n│     ├─ testContainsEmptyTree ✔\n│     ├─ testDeleteEmptyTree ✔\n│     ├─ testContainsElementInTree ✔\n│     ├─ testSearchElementInTree ✔\n│     ├─ testInsertMultipleElements ✔\n│     ├─ testDeleteDuplicateElements ✔\n│     ├─ testDeleteElementNotInTree ✔\n│     ├─ testInsertNegativeNumbers ✔\n│     ├─ testInsertAndSize ✔\n│     ├─ testSearchEmptyTree ✔\n│     ├─ testDeleteNodeWithMultipleElements ✔\n│     ├─ testInsertDuplicatesAndRemove ✔\n│     ├─ testInsertSingleElement ✔\n│     └─ testSearchElementNotInTree ✔\n└─ JUnit Platform Suite ✔\n\nFailures (1):\n  JUnit Vintage:BinarySearchTreeHiddenTest:testTreeTraversals\n    =&gt; org.junit.ComparisonFailure: expected:&lt;[2, 3, 4, 5, [6, 7], 8]&gt; but was:&lt;[2, 3, 4, 5, [7, 6], 8]&gt;\n       DataStructures.BinarySearchTreeHiddenTest.testTreeTraversals(BinarySearchTreeHiddenTest.java:220)\n       [...]\nThe JUnit console output provides a tree structure representing the test execution. The topmost nodes represent the test engines used, in this case, JUnit Jupiter and JUnit Vintage. Underneath each engine are the individual test classes, such as BinarySearchTreeHiddenTest.\nWithin each test class node, there are child nodes representing each test method, such as testInsertAndSearch or testDeleteSingleNode. These methods are marked with a ✔ symbol if they passed, and with a ✘ symbol if they failed. In this case, we see that testTreeTraversals has failed.\nAccompanying the failure symbol is a brief description of the failure, which is the assertion message from the test method. In this example, the test expected the array [2, 3, 4, 5, 6, 7, 8], but received the array [2, 3, 4, 5, 7, 6, 8]. This discrepancy caused the test to fail.\nAfter the tree structure, there is a section titled Failures which provides more detailed information about each failure. For each failure, it lists:\n\nThe test class and method that failed.\nThe type of assertion failure that occurred, which is org.junit.ComparisonFailure in this case.\nThe detailed assertion failure message, which is the same as what’s shown in the tree structure.\nThe location in the code where the failure occurred, which can be very useful (and is often the first thing you should look at when debugging a test failure). In this case, the failure occurred on line 220 of BinarySearchTreeHiddenTest.java (see the DataStructures.BinarySearchTreeHiddenTest.testTreeTraversals(BinarySearchTreeHiddenTest.java:220) line)."
  },
  {
    "objectID": "05_writing_junit_tests.html#conclusion",
    "href": "05_writing_junit_tests.html#conclusion",
    "title": "5  Writing JUnit Tests and Test-Driven Development",
    "section": "5.6 Conclusion",
    "text": "5.6 Conclusion\nIn conclusion, JUnit simplifies the process of writing and managing tests. It provides a comprehensive set of assertion methods to verify our code and uses annotations to define and organize our tests, making them easier to read and understand. By taking advantage of these features, we can write more effective and maintainable tests. In the next section, we’ll dive deeper into Test-Driven Development, a methodology that leverages the power of testing to guide and improve the development process."
  },
  {
    "objectID": "05_writing_junit_tests.html#test-driven-development",
    "href": "05_writing_junit_tests.html#test-driven-development",
    "title": "5  Writing JUnit Tests and Test-Driven Development",
    "section": "5.7 Test-Driven Development",
    "text": "5.7 Test-Driven Development\nTest-Driven Development (TDD) is a software development methodology that is centered around the idea of writing tests before writing the actual code. It is a highly disciplined process that follows a strict order of operations: red, green, refactor. This method has profound implications on the design, quality, and reliability of the software.\nLet’s dive into what these steps entail.\n\nRed: Write a test that covers a specific functionality you want to implement. This test should fail initially because you haven’t written the actual code yet. This stage helps you think about the functionality in detail, ponder on the inputs and expected outputs, and outline the structure of your code.\nGreen: Write the minimal amount of code needed to pass the test. At this stage, don’t worry about the elegance of your code. Your primary focus is on functionality. Run your test, and it should pass this time.\nRefactor: Refactor the code you just wrote in the green stage to eliminate duplication, improve readability, and ensure the code adheres to the best practices. After refactoring, all tests should still pass. If a test fails, it means the refactoring broke the functionality, and you need to revise your changes.\n\nThis cycle repeats for every small chunk of functionality you add to your software. With this approach, you are incrementally building your software with the assurance that at each step, the implemented functionality is working as expected.\n\n5.7.1 The Motivation Behind Test-Driven Development\nYou might be wondering, why would you want to put in the extra effort to write tests before writing the actual code? Here are a few motivating factors:\n\nConfidence: With TDD, you can be confident that your code works because you have tests that prove it. This confidence is especially important when you need to modify your code later. Changes can break existing functionality, but with a robust set of tests, you can quickly catch and fix these regressions.\nBetter Design: Writing tests first forces you to think about your code from a user’s perspective. This shift in viewpoint often results in better code organization and modularity because you design your code to be easy to test, which typically means it is also easy to use and modify.\nDocumentation: Tests act as a form of documentation that shows how the code is supposed to work. New team members can look at the tests to understand what each function is supposed to do and what edge cases it handles.\nDevelopment Speed: While TDD might seem to slow you down at the beginning, it typically results in faster development in the long run. With TDD, you spend less time debugging and fixing bugs because you catch them early in the development process, before they become entangled with other parts of the code.\n\nIn conclusion, TDD is a powerful methodology that can significantly improve the quality of your code and your efficiency as a developer. While it might seem difficult at first, with practice, it becomes a natural part of the development process."
  },
  {
    "objectID": "06_mutation_testing.html#unit-testing-a-recap",
    "href": "06_mutation_testing.html#unit-testing-a-recap",
    "title": "6  Introduction to Mutation Testing",
    "section": "6.1 Unit Testing, a Recap",
    "text": "6.1 Unit Testing, a Recap\nWriting correct code is challenging. Owning your code, maintaining it, and ensuring it always performs as expected can be even more difficult. After all, we are human, and mistakes are a part of our nature. Therefore, we can’t always rely solely on our ability to catch these mistakes. Instead, automated processes that can help us identify and correct these errors are truly invaluable.\nUnit testing is one such automated process that tests your code. These tests target individual units of your code and are designed to verify their behavior. In programming, a method can be thought of as the smallest unit of code, and Classes, Packages, and Modules are larger units of code. Every unit test is designed to validate a single behavior of a single unit of code.\nUnit testing is so crucial that they are a mandatory requirement for many software development projects. For instance, if you are contributing a Java source class to most projects, you are also required to provide unit tests for that class. In this course, where we deal with Data Structures and Algorithms, learning how to implement them in Java, etc., submitting your implementations without Unit tests would be like submitting a paper without a bibliography. It might make us question, “Sure, you wrote this and it looks convincing, but where’s your evidence? Why should I believe you?”\nAnd that’s why you will be required to write unit tests for your Data Structures and Algorithms implementations. But then, how do you know that your unit tests are correct?\nConsider this example:\n@Test\npublic void testAdd() {\n   assertTrue(1 == 1);\n}\nThe test passes, but does it mean it’s correct? Let’s examine the test."
  },
  {
    "objectID": "06_mutation_testing.html#writing-correct-unit-tests",
    "href": "06_mutation_testing.html#writing-correct-unit-tests",
    "title": "6  Introduction to Mutation Testing",
    "section": "6.2 Writing Correct Unit Tests",
    "text": "6.2 Writing Correct Unit Tests\nTaking a closer look at the unit test above, two problems become evident:\n\nThe add method isn’t called in the test.\nThe test can’t fail.\n\nTo address the first issue, we can leverage tools that detect when a unit test doesn’t cover a particular line of code. Here “cover” means “execute”.\nSo what about a test like this?\n@Test\npublic void testAdd() {\n   assertTrue(add(1, 1) == add(1, 1));\n}\nThe add method is called, and test coverage is at 100%. However, this test can’t fail, meaning it is not correct."
  },
  {
    "objectID": "06_mutation_testing.html#mutation-testing-a-solution-to-test-the-correctness-of-your-unit-tests",
    "href": "06_mutation_testing.html#mutation-testing-a-solution-to-test-the-correctness-of-your-unit-tests",
    "title": "6  Introduction to Mutation Testing",
    "section": "6.3 Mutation Testing: A Solution to Test the Correctness of Your Unit Tests",
    "text": "6.3 Mutation Testing: A Solution to Test the Correctness of Your Unit Tests\nMutation testing provides a way to test the correctness of your unit tests. It can help you find tests that can’t fail, tests that need more test cases, and even logic errors in your code.\nThe basic idea of mutation testing is as follows: if you claim your source code is correct, and that the tests prove it, mutation testing will challenge that claim. Mutation testing introduces small changes, called mutations, to your source code. If your tests still pass after these changes, it suggests that your test or source code may not be correct.\nThese mutations are created by algorithms called mutators or mutation operators. Each time a mutator runs, it receives a fresh copy of your source code and makes only one change. The result of a mutator is a mutant - a mutated version of your source code."
  },
  {
    "objectID": "06_mutation_testing.html#mutation-testing-in-practice",
    "href": "06_mutation_testing.html#mutation-testing-in-practice",
    "title": "6  Introduction to Mutation Testing",
    "section": "6.4 Mutation Testing in Practice",
    "text": "6.4 Mutation Testing in Practice\nHere’s how mutation testing works in practice:\n\nApply a set of mutators to your source code to produce a collection of mutants.\nFor each mutant, run your unit tests.\n\nIf the unit tests pass, the mutant has survived.\nIf the unit tests fail, the mutant was killed\n\n\n.\nThe aim is to kill as many mutants as possible. If too many mutants survive, it indicates that your unit tests are not sufficient to prove that your source code is correct. If you manage to kill all mutants, you can use your unit tests to argue that your source code is indeed correct.\nIn our course, only unit tests that leave no surviving mutants will be accepted as a valid submission.\n\n6.4.1 What are these mutators?\nMutators introduce specific types of changes to your code. For instance, a primitive returns mutator (PRIMITIVE_RETURNS) replaces int, short, long, char, float, and double return values with 0. So, for example, this method:\npublic int add(int lhs, int rhs) {\n    return lhs + rhs;\n}\nbecomes:\npublic int add(int lhs, int rhs) {\n    return 0;\n}\nThis mutant would survive in two cases: if the add method is never tested or if the only test cases for add are ones where the result is 0. To fix this, you can add more test cases that test add with non-zero results.\nAnother example of a mutator is the Remove Conditionals Mutator (REMOVE_CONDITIONALS), which removes all conditional statements such that the guarded statements always execute.\nFor example, this code:\nif (a == b) {\n  // do something\n}\nbecomes:\nif (true) {\n  // do something\n}\nThis mutant would survive if the source method is never tested, if all test cases for the source method only ever test the true case, or if both branches have the exact same code or equivalent code, to begin with. To fix this, you can add more test cases that test the false case, and ensure that the true and false branches are not equivalent.\nThrough mutation testing, you will get valuable feedback to improve your code and tests, making them more robust and reliable.\nRemember, a well-tested code base is not just about coverage—it’s about the quality of tests, their ability to catch mistakes, and their resilience against possible errors. Mutation testing is an invaluable tool in achieving this.\nCertainly, let’s decipher the feedback from the Coding Rooms."
  },
  {
    "objectID": "06_mutation_testing.html#understanding-codingrooms-feedback",
    "href": "06_mutation_testing.html#understanding-codingrooms-feedback",
    "title": "6  Introduction to Mutation Testing",
    "section": "6.5 Understanding CodingRooms Feedback",
    "text": "6.5 Understanding CodingRooms Feedback\nIn the feedback provided by the autograder in Coding Rooms, you can find two main parts. The first part lists the mutation tests that have been run, and the second part provides a grading overview.\n\n6.5.1 Understanding Mutation Test Feedback\nEach mutation test feedback starts with “Running Mutation tests”, followed by details about each mutation test performed.\nRunning Mutation tests -\nRan mutation tests for Calculator.CalculatorTest -\n-[ RECORD 0 ]---------+------------------------------------\nMutation type         | RemoveConditionalMutator_EQUAL_ELSE\nSource method mutated | divide\nLine no. of mutation  | 56\nTest examined         | None\nResult                | SURVIVED\nIn the example above, the autograder has run a mutation test on the “divide” method of your “Calculator” class. Here is what each line in the record means:\n\nMutation type: The type of mutation that was made to your code. In this case, a RemoveConditionalMutator_EQUAL_ELSE mutation was made. This mutation type removes a conditional (==) operator and always takes the else branch.\nSource method mutated: The method in your code that was mutated for the test. In this case, it’s the “divide” method.\nLine no. of mutation: The line number in your code where the mutation was applied.\nTest examined: This line indicates which test case was run against the mutant. “None” means no specific test case was chosen, and all available tests were run.\nResult: The outcome of the mutation test. If your tests fail against the mutated code (which is a good thing!), this line will read “KILLED”. If your tests pass (which implies your tests didn’t catch the error introduced by the mutation), this line will read “SURVIVED”. In this case, the mutant has survived, indicating your tests didn’t catch the error.\n\nIf any mutants survive, the autograder lists those under the line, “Problematic mutation test failures printed about.”\n\n\n6.5.2 Understanding the Grading Overview\nThe grading overview gives you a quick summary of how your code has performed in the tests. It breaks down the grading by requirements.\n┌──────────────────────────────────────────────────────┬\n│                   Grading Overview                   │\n├──────────────────────────────────────────────────────┼\n│ Requirement │    Grade    │          Reason          │\n├─────────────┼─────────────┼──────────────────────────┤\n│      1      │ 10.00/10.00 │   - 4/4 tests passing.   │\n├─────────────┼─────────────┼──────────────────────────┤\n│      2      │ 32.00/40.00 │ -8 Penalty due to surviv │\n│             │             │       ing muations       │\n├─────────────┼─────────────┼──────────────────────────┤\n│                  Total: 42.00/50.00                  │\n└──────────────────────────────────────────────────────┴\nIn the example above, we have:\n\nRequirement 1: This section corresponds to the first requirement of the assignment. The student received a full score (10.00/10.00) because all 4 test cases associated with this requirement passed.\nRequirement 2: This section corresponds to the second requirement of the assignment. The student lost 8 points because of surviving mutations, resulting in a score of 32.00/40.00 for this requirement.\n\nThe total grade of the assignment is\n42.00/50.00, indicating the need for further improvement.\nRemember, each surviving mutation indicates a flaw in your test cases—they didn’t catch an erroneous mutation. To improve your grade, aim to update your test cases to ensure that they effectively detect the mutations."
  },
  {
    "objectID": "09_graphs.html#background-and-motivation",
    "href": "09_graphs.html#background-and-motivation",
    "title": "7  The Graph Data Structure",
    "section": "7.1 Background and Motivation",
    "text": "7.1 Background and Motivation\nImagine you want to represent the connections between you and your Instagram followers in a data structure. The diagram below (Figure 7.1) shows a simple representation of your followers as numbered vertices and the edges represent the connections between you and your followers.\n\n\n\n\n\n\n\n\nG\n\n  \n\nYou\n\n You   \n\n1\n\n 1   \n\nYou–1\n\n   \n\n2\n\n 2   \n\nYou–2\n\n   \n\n3\n\n 3   \n\nYou–3\n\n   \n\n4\n\n 4   \n\nYou–4\n\n   \n\n5\n\n 5   \n\nYou–5\n\n   \n\n6\n\n 6   \n\nYou–6\n\n   \n\n7\n\n 7   \n\nYou–7\n\n   \n\n8\n\n 8   \n\nYou–8\n\n   \n\n9\n\n 9   \n\nYou–9\n\n   \n\n10\n\n 10   \n\nYou–10\n\n   \n\n11\n\n 11   \n\nYou–11\n\n   \n\n12\n\n 12   \n\nYou–12\n\n   \n\n13\n\n 13   \n\nYou–13\n\n   \n\n14\n\n 14   \n\nYou–14\n\n   \n\n15\n\n 15   \n\nYou–15\n\n   \n\n16\n\n 16   \n\nYou–16\n\n   \n\n17\n\n 17   \n\nYou–17\n\n   \n\n18\n\n 18   \n\nYou–18\n\n   \n\n19\n\n 19   \n\nYou–19\n\n   \n\n20\n\n 20   \n\nYou–20\n\n  \n\n\nFigure 7.1: A representation of your Instagram followers.\n\n\n\n\nAt first glance, it might appear to be a tree structure, but that is not the case. Your followers can follow other people, who in turn can have their followers. This creates a recursive relationship that cannot be represented using a tree data structure (see Figure 7.2).\n\n\n\n\n\n\n\n\nG\n\n  \n\nYou\n\n You   \n\n1\n\n 1   \n\nYou–1\n\n   \n\n2\n\n 2   \n\nYou–2\n\n   \n\n3\n\n 3   \n\nYou–3\n\n   \n\n4\n\n 4   \n\nYou–4\n\n   \n\n5\n\n 5   \n\nYou–5\n\n   \n\n6\n\n 6   \n\nYou–6\n\n   \n\n7\n\n 7   \n\nYou–7\n\n   \n\n8\n\n 8   \n\nYou–8\n\n   \n\n9\n\n 9   \n\nYou–9\n\n   \n\n10\n\n 10   \n\nYou–10\n\n   \n\n11\n\n 11   \n\n1–11\n\n   \n\n12\n\n 12   \n\n1–12\n\n   \n\n13\n\n 13   \n\n1–13\n\n   \n\n14\n\n 14   \n\n1–14\n\n   \n\n15\n\n 15   \n\n1–15\n\n   \n\n16\n\n 16   \n\n2–16\n\n   \n\n17\n\n 17   \n\n2–17\n\n   \n\n18\n\n 18   \n\n2–18\n\n   \n\n19\n\n 19   \n\n2–19\n\n   \n\n20\n\n 20   \n\n2–20\n\n   \n\n12–1\n\n   \n\n12–3\n\n   \n\n12–5\n\n   \n\n12–7\n\n   \n\n14–4\n\n   \n\n14–8\n\n   \n\n14–12\n\n   \n\n16–2\n\n   \n\n16–6\n\n   \n\n16–10\n\n   \n\n16–14\n\n   \n\n19–9\n\n   \n\n19–13\n\n   \n\n19–17\n\n  \n\n\nFigure 7.2: A representation of your instagram followers where they’re allowed to follow other people too.\n\n\n\n\nTo accurately capture this complex relationship, we need to use a graph data structure. Graphs consist of a set of vertices (or nodes) and a set of edges that connect them. In the context of Instagram followers, the vertices represent the users, and the edges represent the connections between them.\nUsing a graph data structure allows us to represent the recursive nature of the relationship between you and your followers, enabling us to model and analyze the complex network of connections in a more accurate way.\nSure! I will translate the given block according to the specifications you provided. Here’s the updated version:\nFurthermore, the relationship between you and your followers is even more complex. For instance, you can follow someone who does not follow you back, creating a directed relationship where the edges have a direction. In this case, the edges represent the connections from you to your followers, but not the other way around. Such relationships are often modeled using directed graphs, which are a common use case for graphs. Figure 7.3 visualizes this directed relationship.\n\n\n\n\n\n\n\n\nG\n\n  \n\nYou\n\n You   \n\n1\n\n 1   \n\nYou-&gt;1\n\n    \n\n2\n\n 2   \n\nYou-&gt;2\n\n    \n\n3\n\n 3   \n\nYou-&gt;3\n\n    \n\n4\n\n 4   \n\nYou-&gt;4\n\n    \n\n5\n\n 5   \n\nYou-&gt;5\n\n    \n\n6\n\n 6   \n\nYou-&gt;6\n\n    \n\n7\n\n 7   \n\nYou-&gt;7\n\n    \n\n8\n\n 8   \n\nYou-&gt;8\n\n    \n\n9\n\n 9   \n\nYou-&gt;9\n\n    \n\n10\n\n 10   \n\nYou-&gt;10\n\n    \n\n11\n\n 11   \n\n1-&gt;11\n\n    \n\n12\n\n 12   \n\n1-&gt;12\n\n    \n\n13\n\n 13   \n\n1-&gt;13\n\n    \n\n14\n\n 14   \n\n1-&gt;14\n\n    \n\n15\n\n 15   \n\n1-&gt;15\n\n    \n\n16\n\n 16   \n\n2-&gt;16\n\n    \n\n17\n\n 17   \n\n2-&gt;17\n\n    \n\n18\n\n 18   \n\n2-&gt;18\n\n    \n\n19\n\n 19   \n\n2-&gt;19\n\n    \n\n20\n\n 20   \n\n2-&gt;20\n\n    \n\n12-&gt;1\n\n    \n\n12-&gt;3\n\n    \n\n12-&gt;5\n\n    \n\n12-&gt;7\n\n    \n\n14-&gt;4\n\n    \n\n14-&gt;8\n\n    \n\n14-&gt;12\n\n    \n\n16-&gt;2\n\n    \n\n16-&gt;6\n\n    \n\n16-&gt;10\n\n    \n\n16-&gt;14\n\n    \n\n19-&gt;9\n\n    \n\n19-&gt;13\n\n    \n\n19-&gt;17\n\n   \n\n\nFigure 7.3: A directed representation of your Instagram followers. Here, an arrow going from vertex \\(A\\) to vertex \\(B\\) indicates that \\(A\\) follows \\(B\\), but \\(B\\) does not necessarily follow \\(A\\)."
  },
  {
    "objectID": "09_graphs.html#introduction",
    "href": "09_graphs.html#introduction",
    "title": "7  The Graph Data Structure",
    "section": "7.2 Introduction",
    "text": "7.2 Introduction\nA graph is a non-linear data structure that consists of a set of vertices (also called nodes) and a set of edges (or connections) that connect these vertices. In this data structure, the arrangement of vertices and edges allows for a more flexible and complex representation of relationships between data elements compared to linear data structures like arrays, lists, or queues.\nThe concept of adjacency refers to the relationship between two vertices in a graph. If there is an edge connecting two vertices, they are said to be adjacent. Incidence is the relationship between a vertex and an edge. A vertex is said to be incident to an edge if it is one of the two vertices connected by that edge.\nGraphs have numerous real-life applications, and some examples include:\n\nSocial networks, where vertices represent people and edges represent friendships or connections\nTransportation networks, where vertices represent locations and edges represent roads or routes\nCoronavirus transmission networks, where vertices represent individuals and edges represent transmission paths"
  },
  {
    "objectID": "09_graphs.html#graph-terminology",
    "href": "09_graphs.html#graph-terminology",
    "title": "7  The Graph Data Structure",
    "section": "7.3 Graph Terminology",
    "text": "7.3 Graph Terminology\nBefore diving into the implementation of graph data structures, let’s discuss some basic terms and properties of graphs.\n\n7.3.1 Basic Terms and Properties\n\nA graph is a data structure for representing connections among items and consists of vertices connected by edges.​\nA vertex (or node) represents an item in a graph.​\nAn edge represents a connection between two vertices in a graph.\nTwo vertices are adjacent if connected by an edge.​\nDirected vs Undirected: In an undirected graph, the edges have no specific direction, meaning that if there is an edge between vertices A and B, the connection is mutual. In a directed graph (also called a digraph), the edges have a direction, indicating an asymmetrical relationship between vertices. (See Figure 7.4 and Figure 7.5 for examples.)\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nB–C\n\n   \n\nC–A\n\n  \n\n\nFigure 7.4: Example of an undirected graph.\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA-&gt;B\n\n    \n\nC\n\n C   \n\nB-&gt;C\n\n    \n\nC-&gt;A\n\n   \n\n\nFigure 7.5: Example of a directed graph.\n\n\n\n\n\nWeighted vs Unweighted: In an unweighted graph, all edges have equal importance, while in a weighted graph, each edge is assigned a value (or weight), representing the importance, cost, or distance between the connected vertices. (See Figure 7.6 and Figure 7.7 for examples.)\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nB–C\n\n   \n\nC–A\n\n  \n\n\nFigure 7.6: Example of an unweighted graph.\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n 2   \n\nC\n\n C   \n\nB–C\n\n 3   \n\nC–A\n\n 1  \n\n\nFigure 7.7: Example of a weighted graph.\n\n\n\n\n\nSimple vs Multigraph: A simple graph has no more than one edge between any pair of vertices and does not contain any self-loops (edges that connect a vertex to itself). A multigraph can have multiple edges between the same pair of vertices and may include self-loops. (See Figure 7.8 and Figure 7.9 for examples.)\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nB–C\n\n   \n\nC–A\n\n  \n\n\nFigure 7.8: Example of a simple graph.\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nA–A\n\n self-loop   \n\nB\n\n B   \n\nA–B\n\n   \n\nA–B\n\n e2   \n\nC\n\n C   \n\nB–C\n\n   \n\nC–A\n\n  \n\n\nFigure 7.9: Example of a multigraph.\n\n\n\n\n\nDegree: The degree of a vertex is the number of edges incident to it. In a directed graph, we can distinguish between in-degree (the number of edges directed towards the vertex) and out-degree (the number of edges directed away from the vertex). See Figure 7.10 for an example.\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n A (degree 3)   \n\nD\n\n D   \n\nA–D\n\n D (degree 1)   \n\nC\n\n C   \n\nB–C\n\n B (degree 3)   \n\nC–A\n\n C (degree 3)  \n\n\nFigure 7.10: Example graph with vertex degrees.\n\n\n\n\n\nPath: A path in a graph is a sequence of vertices connected by edges. See Figure 7.11 for an example.\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nB–C\n\n   \n\nD\n\n D   \n\nC–D\n\n  \n\n\nFigure 7.11: Example graph with a path from A to D.\n\n\n\n\n\nCycle: A cycle is a closed path, where the first and last vertices in the path are the same, and no vertex is visited more than once. See Figure 7.12 for an example.\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nB–C\n\n   \n\nC–A\n\n  \n\n\nFigure 7.12: Example graph with a cycle. (A-B-C)\n\n\n\n\n\nConnected vs Disconnected: A graph is connected if there is a path between every pair of vertices. If there is at least one pair of vertices with no path between them, the graph is disconnected. See Figure 7.13 and Figure 7.14 for examples.\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nB–C\n\n   \n\nC–A\n\n  \n\n\nFigure 7.13: Example of a connected graph.\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nB–C\n\n   \n\nD\n\n D   \n\nE\n\n E   \n\nD–E\n\n  \n\n\nFigure 7.14: Example of a disconnected graph.\n\n\n\n\n\n\n7.3.2 Graph Notation\nWe can use a notation like \\(G(V, E)\\), where \\(V\\) is the set of vertices and \\(E\\) is the set of edges, to represent a graph.\n\n\n7.3.3 Special Types of Graphs\n\nComplete Graph: A complete graph is a simple graph in which every pair of vertices is connected by a unique edge. See Figure 7.15 for an example.\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nA–C\n\n   \n\nD\n\n D   \n\nA–D\n\n   \n\nB–C\n\n   \n\nB–D\n\n   \n\nC–D\n\n  \n\n\nFigure 7.15: Example of a complete graph.\n\n\n\n\n\nBipartite Graph: A bipartite graph is a graph whose vertices can be divided into two disjoint sets such that all edges connect vertices from one set to the other, with no edges connecting vertices within the same set. See Figure 7.16 for an example.\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\n1\n\n 1   \n\nA–1\n\n   \n\n2\n\n 2   \n\nA–2\n\n   \n\nB\n\n B   \n\nB–1\n\n   \n\n3\n\n 3   \n\nB–3\n\n   \n\nC\n\n C   \n\nC–2\n\n   \n\nC–3\n\n  \n\n\nFigure 7.16: Example of a bipartite graph.\n\n\n\n\n\nTree: A tree is an undirected graph with no cycles, and all vertices are connected. It has a hierarchical structure, with one vertex acting as the root, and the other vertices connected in a parent-child relationship. See Figure 7.17 for an example.\n\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nA–C\n\n   \n\nD\n\n D   \n\nB–D\n\n   \n\nE\n\n E   \n\nB–E\n\n   \n\nF\n\n F   \n\nC–F\n\n   \n\nG\n\n G   \n\nC–G\n\n  \n\n\nFigure 7.17: Example of a tree."
  },
  {
    "objectID": "09_graphs.html#graph-representation",
    "href": "09_graphs.html#graph-representation",
    "title": "7  The Graph Data Structure",
    "section": "7.4 Graph Representation",
    "text": "7.4 Graph Representation\nIn order to work with graphs in code or store them in memory, we need efficient ways to represent them. There are multiple methods to represent graphs, and the choice of representation depends on factors such as the density of the graph, the operations to be performed, and memory constraints.\nIn this section, we will discuss two common methods to represent a graph: adjacency list and adjacency matrix.\n\n7.4.1 Adjacency List\nAn adjacency list represents a graph by storing a list of adjacent vertices for each vertex in the graph. This can be implemented using an array of lists or a hash table, where the index or key corresponds to a vertex, and the value is a list of adjacent vertices.\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nA–C\n\n   \n\nD\n\n D   \n\nB–D\n\n   \n\nC–D\n\n  \n\n\nFigure 7.18: See adjacency list for this example Listing 7.1\n\n\n\n\nAdjacency list representation for figure Figure 7.18:\n\nListing 7.1: Adjacency list representation.\nA: [B, C]\nB: [A, D]\nC: [A, D]\nD: [B, C]\n\n// or as an arraylist of arraylists -\n[[B, C], [A, D], [A, D], [B, C]]\n// here, the index of the outer arraylist represents the vertex.\n// in order for this to work, the order of the vertices must be \n// fixed, and stored separately.\n\nThe adjacency list representation is efficient for sparse graphs (graphs with relatively few edges) as it only stores the existing edges, reducing memory usage. This representation also allows for faster traversal of a vertex’s neighbors.\n\n\n7.4.2 Adjacency Matrix\nAn adjacency matrix is a two-dimensional array (or matrix) where the cell at the i-th row and j-th column represents the edge between vertex i and vertex j. For an undirected graph, the adjacency matrix is symmetric. For a weighted graph, the values in the cells represent the weights of the edges; for an unweighted graph, the cells contain either 1 (edge exists) or 0 (no edge).\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nA–C\n\n   \n\nD\n\n D   \n\nB–D\n\n   \n\nC–D\n\n  \n\n\nSee adjacency matrix for this example Listing 7.2\n\n\n\nAdjacency matrix representation (unweighted):\n\nListing 7.2: Adjacency matrix representation.\n  A B C D\nA 0 1 1 0\nB 1 0 0 1\nC 1 0 0 1\nD 0 1 1 0\n\nThe adjacency matrix representation is suitable for dense graphs (graphs with many edges) or when checking for the presence of an edge between two vertices needs to be fast. However, this representation can be inefficient in terms of memory usage, especially for large, sparse graphs, as it stores information for all possible edges, even if they do not exist.\n\n\n7.4.3 Converting Between Representations\nTo convert a graph diagram or notation into an adjacency list or an adjacency matrix, follow these steps:\n\nIdentify the vertices and edges in the graph.\nFor an adjacency list, create an empty list or hash table for each vertex. For each edge, add the adjacent vertices to the corresponding lists.\nFor an adjacency matrix, create a square matrix with dimensions equal to the number of vertices. For each edge, set the corresponding cells in the matrix to 1 (or the edge weight for weighted graphs).\n\nTo convert an adjacency list or an adjacency matrix back into a graph diagram or notation, follow these steps:\n\nIdentify the vertices based on the keys (for an adjacency list) or indices (for an adjacency matrix).\nFor an adjacency list, iterate through the lists and draw an edge for each adjacent vertex.\nFor an adjacency matrix, iterate through the matrix cells and draw an edge for each non-zero value (or the corresponding weight for weighted graphs)."
  },
  {
    "objectID": "09_graphs.html#graph-traversal",
    "href": "09_graphs.html#graph-traversal",
    "title": "7  The Graph Data Structure",
    "section": "7.5 Graph Traversal",
    "text": "7.5 Graph Traversal\nImagine you want to find the average age of all users on Facebook. With billions of users, it is infeasible to hold the entire graph of the friend network in memory. Ideally, we would want to find out information on each user one at a time, on a per-need basis. To achieve this, we can use graph traversal algorithms, which allow us to visit each user, add up their ages, and then calculate the average. A simple way to do this is to load information on a user, add all their friends to a stack, and then keep popping from the stack and requesting data from Facebook for each friend. When we receive the data, we mark that user as visited to avoid recounting their age if we reach the same user again. We then add friends of each loaded user to our stack and keep repeating until we run out of users in our stack.\nThis problem illustrates the importance of graph traversal, a fundamental operation in graph theory. Graphs are a powerful and versatile data structure that can model various kinds of relationships and networks, such as social networks, computer networks, transportation networks, web pages, games, and many other domains. Graph traversal allows us to explore and manipulate graphs in various ways, with applications in domains like searching for specific nodes, finding the shortest path between nodes, and analyzing the structure of a graph.\nGraph traversal algorithms typically begin with a start node and attempt to visit the remaining nodes from there. They must deal with several troublesome cases, such as unreachable nodes, revisited nodes, and choosing which node to visit next among several options. To handle these cases, graph traversal algorithms use different strategies and data structures to keep track of which nodes have been visited and which nodes are still pending. The most common graph traversal algorithms are breadth-first search (BFS) and depth-first search (DFS), which differ in the order in which they visit the nodes.\nIn some situations, we may not know the entire graph at once and instead only have access to a node object and its adjacent nodes. As demonstrated in the Facebook example, graph traversal algorithms can be used to solve problems that involve large and dynamic graphs by visiting each user and analyzing their information on a per-need basis.\nThere are two common methods to traverse a graph:\n\nBreadth-First Search (BFS)\nDepth-First Search (DFS)\n\nBy understanding and implementing these graph traversal methods, you can efficiently explore and manipulate complex graphs to solve a wide range of problems.\n\n7.5.1 Breadth-First Search (BFS)\nBreadth-First Search explores a graph by visiting all the neighbors of the starting vertex before moving on to their neighbors. BFS uses a queue data structure to keep track of the vertices to visit.\nHere’s a step-by-step example of BFS traversal (for the graph in example Figure 7.19):\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nA–C\n\n   \n\nD\n\n D   \n\nB–D\n\n   \n\nC–D\n\n   \n\nE\n\n E   \n\nC–E\n\n  \n\n\nFigure 7.19: Example graph for BFS traversal.\n\n\n\n\nBFS traversal starting from vertex A:\n\nVisit A and add its neighbors B and C to the queue: [B, C]\nVisit B and add its unvisited neighbor D to the queue: [C, D]\nVisit C and add its unvisited neighbor E to the queue: [D, E]\nVisit D: [E]\nVisit E: []\n\nBFS traversal order: A, B, C, D, E\nBFS pseudocode:\nBFS(graph, start):\n  Initialize an empty queue Q\n  Mark start as visited\n  Enqueue start into Q\n  \n  while Q is not empty:\n    vertex = Dequeue(Q)\n    Visit vertex\n    \n    for each neighbor of vertex:\n      if neighbor is not visited:\n        Mark neighbor as visited\n        Enqueue neighbor into Q\n\n\n7.5.2 Depth-First Search (DFS)\nDepth-First Search explores a graph by visiting a vertex and its neighbors as deeply as possible before backtracking. DFS can be implemented using recursion or an explicit stack data structure.\nHere’s a step-by-step example of DFS traversal (for the graph in example Figure 7.20):\n\n\n\n\n\n\n\n\nG\n\n  \n\nA\n\n A   \n\nB\n\n B   \n\nA–B\n\n   \n\nC\n\n C   \n\nA–C\n\n   \n\nD\n\n D   \n\nB–D\n\n   \n\nC–D\n\n   \n\nE\n\n E   \n\nC–E\n\n  \n\n\nFigure 7.20: Example graph for DFS traversal.\n\n\n\n\nDFS traversal starting from vertex A:\n\nVisit A and recurse on its first neighbor B\nVisit B and recurse on its first neighbor D\nVisit D and backtrack (no unvisited neighbors)\nBacktrack to A and recurse on its next neighbor C\nVisit C and recurse on its first neighbor E\nVisit E and backtrack (no unvisited neighbors)\n\nDFS traversal order: A, B, D, C, E\nDFS pseudocode (recursive):\nDFS(graph, vertex):\n  Mark vertex as visited\n  Visit vertex\n  \n  for each neighbor of vertex:\n    if neighbor is not visited:\n      DFS(graph, neighbor)\nDFS pseudocode (iterative with a stack):\nDFS(graph, start):\n  Initialize an empty stack S\n  Mark start as visited\n  Push start onto S\n  \n  while S is not empty:\n    vertex = Pop(S)\n    Visit vertex\n    \n    for each neighbor of vertex:\n      if neighbor is not visited:\n        Mark neighbor as visited\n        Push neighbor onto S\n\n\n7.5.3 Applications and Variations of BFS and DFS\nBoth BFS and DFS have numerous applications and can be adapted to solve various graph-related problems:\n\nShortest path: BFS can be used to find the shortest path between two vertices in an unweighted graph. The algorithm can be modified to keep track of the path length or the actual path itself.\nConnected components: Both BFS and DFS can be used to find connected components in an undirected graph. By running the traversal algorithm and marking visited vertices, we can identify the set of vertices reachable from a starting vertex. Repeating this process for all unvisited vertices will find all connected components in the graph.\nTopological sorting: DFS can be adapted to perform a topological sort on a directed acyclic graph (DAG). A topological ordering is a linear ordering of the vertices such that for every directed edge (u, v), vertex u comes before vertex v in the ordering. This can be useful in scheduling tasks with dependencies or determining the order of courses in a curriculum.\nBipartite graph check: BFS or DFS can be used to check if a graph is bipartite. The algorithm can be modified to color vertices while traversing the graph. If at any point during the traversal, two adjacent vertices have the same color, the graph is not bipartite.\nGraph cycle detection: DFS can be used to detect cycles in a graph. By keeping track of the recursion stack, we can determine if a vertex is visited more than once in the same path, indicating a cycle.\n\nIn summary, graph traversal is a fundamental operation in graph theory with various applications. Breadth-First Search (BFS) and Depth-First Search (DFS) are two common techniques to traverse a graph, each with its own advantages and use cases. Understanding these algorithms and their variations can help solve a wide range of graph-related problems."
  },
  {
    "objectID": "10_hashing.html#background-and-motivation",
    "href": "10_hashing.html#background-and-motivation",
    "title": "8  Hashing, Hash Tables, and Hash Maps",
    "section": "8.1 Background and Motivation",
    "text": "8.1 Background and Motivation\n\n8.1.1 Indexing\nIndexing refers to the idea of accessing a certain element of an array by referring to it using a specific number, called the index. Using the same index always returns the same element, as long as the array remains unchanged. For example, to access the 12th element of an array arr, we use arr[11].\nThe math for finding the address of an element in the array works out to be:\nbaseAddress + (index * sizeOfElement)\nNow, keeping that in mind, let’s explore the limitations of indexing.\n\n\n8.1.2 Limitations of Indexing\nSuppose we want to access an element in the array using a string as an index, such as arr[\"dhruv\"]. What is stopping us?\nThe problem is that we cannot calculate baseAddress + (\"dhruv\" * sizeOfElement) because the index, in this case, is a string, not a number. The operation is not defined, and therefore, we can’t directly use strings as indices in an array.\n\n\n8.1.3 Mapping Strings to Numbers\nLet’s consider an array of strings. We can use it to map a number to a string:\n0 -&gt; \"Alice\"\n1 -&gt; \"Bob\"\n2 -&gt; \"Charlie\"\nIf we can use an array to map a number to a string, can we also use it to map strings to numbers? Yes, we can!\nOne way to do this is by searching linearly for a string in the array to find its index. For example, we can find the string “Dhruv” at index 5:\n0 -&gt; \"Alice\"\n1 -&gt; \"Bob\"\n2 -&gt; \"Charlie\"\n...\n5 -&gt; \"Dhruv\"\nThe index at which we found the string “Dhruv” (in this case, 5) can be used as a key in a different array to find the data related to “Dhruv”. However, this method of linear searching can be quite slow for large datasets.\n\n\n8.1.4 Hashing: A Better Solution\nThis is where hashing comes into play. Hashing allows us to efficiently map strings (or any other non-numeric keys) to numbers. By using a hash function, we can convert a string into a number that represents the index in the array.\nA hash function takes a key as input and outputs an index in the hash table’s array. A good hash function has the following criteria:\n\nUniform distribution: The hash function should distribute keys evenly across the array to minimize collisions (when multiple keys map to the same index).\nMinimal collisions: A good hash function should minimize the chance of collisions.\nFast computation: The hash function should be fast to compute, allowing for quick insertion, deletion, and retrieval of data.\nDeterministic output: The hash function should produce the same output for the same input every time it is called.\n\nFor example, let’s consider a simple hash function that converts the first character of a string into its ASCII code:\nhash(\"dhruv\") = ASCII('d') = 100\nThe output of the hash function is 100, which we can use as an index in an array to store or retrieve data related to “dhruv”. This allows us to use strings (and other non-numeric keys) as indices, achieving our goal of efficient mapping."
  },
  {
    "objectID": "10_hashing.html#hash-functions",
    "href": "10_hashing.html#hash-functions",
    "title": "8  Hashing, Hash Tables, and Hash Maps",
    "section": "8.2 Hash Functions",
    "text": "8.2 Hash Functions\n\n8.2.1 Introduction\nPreviously, we managed to map a string “D” to some data, just like an index maps to some data in an array. The resulting data structure that can map any string to any data is called a hash table. The function used to map strings to data is called a hash function. The concept of mapping “D” to some data can be referred to as hashing “D” to an index 3, which is where we found the value corresponding to “D”.\nNow, we’ll learn about a way of mapping any object (called a key) to any other object (called the record, or the value). For instance, your student ID can be a key, and all the data about you on your ID can be stored in a record object.\n\n\n8.2.2 Hashing\nHashing can be thought of as a method for storing and retrieving records from a database. It lets you insert, delete, and search for records based on a search key value. When properly implemented, these operations can be performed in constant time. In fact, a properly tuned hash system typically looks at only one or two records for each search, insert, or delete operation. This is far better than the \\(O(log n)\\) average cost required to do a binary search on a sorted array of n records, or the \\(O(log n)\\) average cost required to do an operation on a binary search tree. However, even though hashing is based on a very simple idea, it is surprisingly difficult to implement properly. Designers need to pay careful attention to all of the details involved with implementing a hash system.\nA hash system stores records in an array called a hash table, which we will call HT. Hashing works by performing a computation on a search key K in a way that is intended to identify the position in HT that contains the record with key K. The function that does this calculation is called the hash function, and will be denoted by the letter h. Since hashing schemes place records in the table in whatever order satisfies the needs of the address calculation, records are not ordered by value. A position in the hash table is also known as a slot. The number of slots in hash table HT will be denoted by the variable M with slots numbered from 0 to M−1. The goal for a hashing system is to arrange things such that, for any key value K and some hash function h, \\(i=h(K)\\) is a slot in the table such that \\(0 \\leq i &lt; M\\), and we have the key of the record stored at HT[i] equal to K.\nHashing is not good for applications where multiple records with the same key value are permitted. Hashing is not a good method for answering range searches. In other words, we cannot easily find all records (if any) whose key values fall within a certain range. Nor can we easily find the record with the minimum or maximum key value or visit the records in key order. Hashing is most appropriate for answering the question, ‘What record, if any, has key value K?’ For applications where all search is done by exact-match queries, hashing is the search method of choice because it is extremely efficient when implemented correctly.\nHashing generally takes records whose key values come from a large range and stores those records in a table with a relatively small number of slots. Since keys have a large range and values have smaller, limited slots for storage – A hash function might sometimes end up hashing two keys to the same slot. We refer to such an event as a collision.\nTo illustrate, consider a classroom full of students. What is the probability that some pair of students shares the same birthday (i.e., the same day of the year, not necessarily the same year)? If there are 23 students, then it is unlikely that more than one student will share the same birthday. There are 365 “slots” or possible days a student can have a birthday on; but only 23 “keys”. As the number of students increases, the probability of a “collision” or two students sharing a birthday increases. To be practical, a database organized by hashing must store records in a hash table that is not so large that it wastes space.\nWe would like to pick a hash function that maps keys to slots in a way that makes each slot in the hash table have equal probability of being filled for the actual set keys being used. Unfortunately, we normally have no control over the distribution of key values for the actual records in a given database or collection. So how well any particular hash function does depends on the actual distribution of the keys used within the allowable key range. In some cases, incoming data are well distributed across their key range. For example, if the input is a set of random numbers selected uniformly from the key range, any hash function that assigns the key range so that each slot in the hash table receives an equal share of the range will likely also distribute the input records uniformly within the table.\nHowever, in many applications the incoming records are highly clustered or otherwise poorly distributed. When input records are not well distributed throughout the key range it can be difficult to devise a hash function that does a good job of distributing the records throughout the table, especially if the input distribution is not known in advance. For example, If the input is a collection of English words, the beginning letter will be poorly distributed. A dictionary of words mapped to their frequency is often used in rudimentary natural language processing algorithms.\nIn conclusion, anything can be a hash function (i.e., map a value to an index), but not everything can be a good hash function. A function that always returns the index 0 is a hash function that maps everything to 0. It’s no good but it’s still a hash function. An example of a commonly used hash function is the modulus operator! It is common for N-sized hash tables to use the modulus of N as a hash function. If N is \\(20\\), data for 113 will be hashed to index \\(113 \\% 20 = 13\\).\nBut if we use the modulo operator as a hash function, what do we do when multiple pieces of data map to the same index? \\(53 \\% 20 = 13\\), \\(73 \\% 20 = 13\\), etc. But if you think about it, we can store everything at \\(13\\)! By using nested data structures… More on this later.\n\n\n8.2.3 Simple Hash Functions\nLet’s apply a simple hash function to a set of keys and compute their indices. In this example, we’ll use the modulo operation as the hash function. Given a hash table with a size of 5, we can compute the indices for the keys as follows:\nHashTable size: 5\nHashFunction: key % size\n\nKeys: 15, 28, 47, 10, 33\n\nIndices:\n15 % 5 = 0\n28 % 5 = 3\n47 % 5 = 2\n10 % 5 = 0\n33 % 5 = 3\n\n\n8.2.4 Other Types of Hash Functions\n\n8.2.4.1 Direct Hashing\nA direct hash function uses the item’s key as the bucket index. For example, if the key is 937, the index is 937. A hash table with a direct hash function is called a direct access table. Given a key, a direct access table search algorithm returns the item at index key if the bucket is not empty, and returns null (indicating item not found) if empty.\nLimitations:\nA direct access table has the advantage of no collisions: Each key is unique (by definition of a key), and each gets a unique bucket, so no collisions can occur. However, a direct access table has two main limitations:\n\nAll keys must be non-negative integers, but for some applications, keys may be negative.\nThe hash table’s size equals the largest key value plus 1, which may be very large.\n\nSimilarly, there are other hash functions each with their own characteristics.\n\n\n8.2.4.2 Modulo Hash\nA modulo hash function computes the index by taking the remainder of the key divided by the table size M. This is a simple and effective way to convert a large key range into a smaller index range. The hash function can be defined as:\nh(K) = K % M\n\n\n8.2.4.3 Mid-Square Hash\nA mid-square hash function computes the index by first squaring the key, and then extracting a portion of the squared value as the index. This approach is especially useful when the keys are not uniformly distributed. The hash function can be defined as:\nh(K) = middle_digits(K^2)\n\n\n8.2.4.4 Mid-Square Hash with Base 2\nA mid-square hash function with base 2 is a variation of the mid-square hash function, where the key is first squared, and then the middle bits of the binary representation of the squared value are extracted as the index. This approach is especially useful for binary keys. The hash function can be defined as:\nh(K) = middle_bits(K^2)\n\n\n8.2.4.5 Multiplicative String Hashing\nA multiplicative string hashing function computes the index by treating the characters in the string as numbers and combining them using a multiplication and a constant. This approach can help achieve a good distribution of string keys in the hash table. The hash function can be defined as:\nh(K) = (c1 * a^(n-1) + c2 * a^(n-2) + ... + cn) % M\nwhere c1, c2, ..., cn are the character codes of the string, a is a constant, n is the length of the string, and M is the size of the hash table.\nHere’s the ASCII representation of the resulting hash table:\nIndex | Key\n-------------\n  0   | 15\n  1   | -\n  2   | 47\n  3   | 28\n  4   | -\nIn this example, we can see that the keys 15 and 10, as well as 28 and 33, have collided, as they both map to the same indices (0 and 3, respectively).\n\n\n\n8.2.5 Trade-offs Between Different Hash Functions\nThere are trade-offs between different hash functions in terms of performance and complexity:\n\nA simple hash function, like the modulo operation, is fast to compute but may not distribute keys uniformly, leading to more collisions and reduced performance.\nMore complex hash functions, such as cryptographic hash functions, can provide a better distribution of keys but may be slower to compute.\n\nIn practice, the choice of a hash function depends on the specific requirements of the application and the data being stored. The goal is to find a balance between uniform distribution, minimal collisions, fast computation, and deterministic output."
  },
  {
    "objectID": "10_hashing.html#hash-collisions",
    "href": "10_hashing.html#hash-collisions",
    "title": "8  Hashing, Hash Tables, and Hash Maps",
    "section": "8.3 Hash Collisions",
    "text": "8.3 Hash Collisions\nHash collisions occur when two or more keys map to the same index in the hash table. Due to the pigeonhole principle, hash collisions are inevitable, as there are typically more possible keys than available indices in the array. Collisions negatively impact the efficiency of hashing, as they can lead to longer access times for insertion, deletion, and retrieval of key-value pairs.\nThere are two primary methods to resolve hash collisions: chaining and open addressing."
  },
  {
    "objectID": "10_hashing.html#chaining",
    "href": "10_hashing.html#chaining",
    "title": "8  Hashing, Hash Tables, and Hash Maps",
    "section": "8.4 Chaining",
    "text": "8.4 Chaining\nChaining is a collision resolution technique that uses a linked list or another data structure to store multiple key-value pairs at the same index. When a collision occurs, the new key-value pair is simply added to the data structure at the index.\n\n8.4.1 Insertion, Search, and Deletion\nHere’s how to perform insertion, search, and deletion operations using chaining:\n\nInsertion: Calculate the index using the hash function. If the index is empty, create a new data structure (e.g., linked list) and insert the key-value pair. If the index is not empty, add the key-value pair to the existing data structure.\nSearch: Calculate the index using the hash function. If the index is empty, the key is not in the hash table. If the index is not empty, search the data structure at the index for the key.\nDeletion: Calculate the index using the hash function. If the index is empty, the key is not in the hash table. If the index is not empty, search the data structure at the index for the key and remove it if found.\n\n\n\n8.4.2 Advantages and Disadvantages of Chaining\nChaining has several advantages and disadvantages:\n\nAdvantages:\n\nEasy implementation: Chaining can be easily implemented using existing data structures like linked lists.\nDynamic size: The data structure at each index can grow or shrink as needed, allowing for efficient use of space.\n\nDisadvantages:\n\nExtra space: Chaining requires additional space for the data structure at each index, which can increase memory overhead.\nVariable access time: The access time for key-value pairs depends on the length of the data structure at the index, which can vary.\n\n\nChaining is a popular method for resolving hash collisions due to its simplicity and dynamic size. However, it may not be the most efficient option for all use cases, especially when memory overhead and variable access times are critical factors."
  },
  {
    "objectID": "10_hashing.html#open-addressing",
    "href": "10_hashing.html#open-addressing",
    "title": "8  Hashing, Hash Tables, and Hash Maps",
    "section": "8.5 Open Addressing",
    "text": "8.5 Open Addressing\nOpen addressing is a collision resolution technique that finds an alternative index for a key-value pair if the original index is occupied. When a collision occurs, the algorithm searches for the next available index using a probing technique. There are three common types of probing techniques: linear probing, quadratic probing, and double hashing.\n\n8.5.1 Probing Techniques\n\nLinear probing: When a collision occurs, search the hash table linearly (one index at a time) until an empty slot is found.\nQuadratic probing: When a collision occurs, search the hash table quadratically (by increasing the index by the square of the probe number) until an empty slot is found.\nDouble hashing: When a collision occurs, use a secondary hash function to compute a new index for the key-value pair, and repeat this process until an empty slot is found.\n\n\n\n8.5.2 Insertion, Search, and Deletion\nHere’s how to perform insertion, search, and deletion operations using open addressing:\n\nInsertion: Calculate the index using the hash function. If the index is empty, insert the key-value pair. If the index is occupied, use the chosen probing technique to find the next available index and insert the key-value pair there.\nSearch: Calculate the index using the hash function. If the index is empty, the key is not in the hash table. If the index is occupied, check if the key matches the stored key. If not, use the chosen probing technique to search for the next index until the key is found or an empty index is encountered.\nDeletion: Calculate the index using the hash function. If the index is empty, the key is not in the hash table. If the index is occupied and the key matches the stored key, remove the key-value pair and mark the index as deleted. Continue searching using the chosen probing technique to handle cases where the removed key-value pair was part of a cluster.\n\n\n\n8.5.3 Advantages and Disadvantages of Open Addressing\nOpen addressing has several advantages and disadvantages:\n\nAdvantages:\n\nNo extra space: Open addressing does not require additional space for data structures at each index, making it more memory-efficient.\nFixed size: The hash table has a fixed size, which can be useful when memory is limited.\n\nDisadvantages:\n\nClustering: Probing techniques can cause clusters of key-value pairs to form, leading to increased access times.\nDeletion issues: Deleting key-value pairs can create complications, as it may leave “holes” in clusters that need to be addressed.\n\n\nOpen addressing is an alternative method for resolving hash collisions that can be more memory-efficient than chaining. However, it may not be the best option for all use cases, especially when clustering and deletion issues are critical factors."
  },
  {
    "objectID": "10_hashing.html#complexity-and-load-factor",
    "href": "10_hashing.html#complexity-and-load-factor",
    "title": "8  Hashing, Hash Tables, and Hash Maps",
    "section": "8.6 Complexity and Load Factor",
    "text": "8.6 Complexity and Load Factor\nWhen analyzing the complexity of hash functions and hash tables, we need to consider the time taken for searching, inserting, or deleting an element. There are two main steps involved in these operations:\n\nComputing the hash function for the given key.\nTraversing the list of key-value pairs present at the computed index.\n\n\n8.6.1 Time Complexity of Hash Computation\nFor the first step, the time taken depends on the key and the hash function. For example, if the key is a string “abcd”, then its hash function may depend on the length of the string. But for very large values of n, the number of entries into the map, the length of the keys is almost negligible in comparison to n, so hash computation can be considered to take place in constant time, i.e., O(1).\n\n\n8.6.2 Time Complexity of List Traversal\nFor the second step, traversal of the list of key-value pairs present at that index needs to be done. In the worst case, all the n entries are at the same index, resulting in a time complexity of O(n). However, enough research has been done to make hash functions uniformly distribute the keys in the array, so this almost never happens.\n\n\n8.6.3 Load Factor\nOn average, if there are n entries and b is the size of the array, there would be n/b entries at each index. This value n/b is called the load factor, which represents the load on our map. The load factor is denoted by the symbol λ:\nλ = n/b\nThis load factor needs to be kept low so that the number of entries at one index is less, and the complexity remains almost constant, i.e., O(1).\n\n\n8.6.4 Balancing Load Factor and Complexity\nTo maintain the load factor at an acceptable level, the hash table can be resized when the load factor exceeds a certain threshold. This helps to keep the complexity of hash table operations near O(1) by redistributing the keys uniformly across a larger array.\nIn conclusion, understanding the complexity and load factor of hash functions is crucial for designing efficient hash tables. By carefully choosing a suitable hash function and managing the load factor, it’s possible to achieve near-constant time complexity for various hash table operations."
  },
  {
    "objectID": "10_hashing.html#rehashing",
    "href": "10_hashing.html#rehashing",
    "title": "8  Hashing, Hash Tables, and Hash Maps",
    "section": "8.7 Rehashing",
    "text": "8.7 Rehashing\nRehashing, as the name suggests, means hashing again. When the load factor increases to more than its pre-defined value (the default value of the load factor is 0.75), the complexity increases. To overcome this issue, the size of the array is increased (typically doubled) and all the values are hashed again and stored in the new, larger array. This helps maintain a low load factor and low complexity.\n\n8.7.1 Why?\nRehashing is done because whenever key-value pairs are inserted into the map, the load factor increases, which implies that the time complexity also increases, as explained earlier. This might not provide the desired time complexity of O(1). Hence, rehashing must be performed, increasing the size of the bucketArray to reduce the load factor and the time complexity.\n\n\n8.7.2 How?\nRehashing can be done as follows:\n\nFor each addition of a new entry to the map, check the load factor.\nIf the load factor is greater than its pre-defined value (or the default value of 0.75 if not given), then perform rehashing.\nTo rehash, create a new array of double the previous size and make it the new bucketArray.\nTraverse each element in the old bucketArray and call the insert() method for each, to insert it into the new larger bucketArray.\n\nThe following diagram illustrates the rehashing process:\nInitial bucketArray (size = 4):\n+---+---+---+---+\n|   | K1|   | K2|\n+---+---+---+---+\n\nAfter inserting a new key K3 (load factor &gt; 0.75):\n\nNew bucketArray (size = 8):\n+---+---+---+---+---+---+---+---+\n|   | K1|   | K2|   |   |   | K3|\n+---+---+---+---+---+---+---+---+\nBy rehashing, the hash table maintains its desired time complexity of O(1) even as the number of elements increases. It is important to note that rehashing can be a costly operation, especially if the number of elements in the hash table is large. However, since rehashing is done infrequently and only when the load factor surpasses a certain threshold, the amortized cost of rehashing remains low, allowing the hash table operations to maintain near-constant time complexity."
  },
  {
    "objectID": "10_hashing.html#hash-tables-vs-hash-maps",
    "href": "10_hashing.html#hash-tables-vs-hash-maps",
    "title": "8  Hashing, Hash Tables, and Hash Maps",
    "section": "8.8 Hash Tables vs Hash Maps",
    "text": "8.8 Hash Tables vs Hash Maps\nHash tables and hash maps differ in their implementation and functionality.\n\nHash tables use direct hashing, where the key is an integer or can be directly converted to an integer (e.g., a string of digits). The integer is then used to compute the index in the hash table.\nHash maps use indirect hashing, where the key can be any data type. A separate hash function is needed to convert the key into an index in the hash table.\n\nWhen deciding whether to use a hash table or a hash map, consider the problem domain and the data type of the keys:\n\nIf the keys are integers or can be directly converted to integers, a hash table may be a more suitable choice. For example, if you’re working with student IDs as keys, a hash table would be a good fit.\nIf the keys are of any other data type or cannot be directly converted to integers, a hash map would be more appropriate. For example, if you’re working with strings, such as usernames or URLs, a hash map would be a better choice."
  },
  {
    "objectID": "10_hashing.html#hashmaps-in-java",
    "href": "10_hashing.html#hashmaps-in-java",
    "title": "8  Hashing, Hash Tables, and Hash Maps",
    "section": "8.9 HashMaps in Java",
    "text": "8.9 HashMaps in Java\nA HashMap is a collection in Java that implements the Map interface and uses a hash table for storage. It stores key-value pairs, where each key is unique, and the keys are not ordered.\nHere’s how to use a HashMap in Java:\n\nImport the HashMap class: To use the HashMap class in your Java code, you’ll need to import it from the java.util package:\nimport java.util.HashMap;\nCreate a HashMap: To create a new HashMap, use the following syntax:\nHashMap&lt;String, Integer&gt; myMap = new HashMap&lt;String, Integer&gt;();\nAdd elements: To add key-value pairs to the HashMap, use the put() method:\nmyMap.put(\"apple\", 3);\nmyMap.put(\"banana\", 5);\nmyMap.put(\"orange\", 2);\nAccess elements: To access the value associated with a key, use the get() method:\nint apples = myMap.get(\"apple\"); // 3\nint oranges = myMap.get(\"orange\"); // 2\nRemove elements: To remove a key-value pair from the HashMap, use the remove() method:\nmyMap.remove(\"banana\");\nCheck if a key exists: To check if a key is in the HashMap, use the containsKey() method:\nboolean hasApple = myMap.containsKey(\"apple\"); // true\nboolean hasGrape = myMap.containsKey(\"grape\"); // false\nIterate over keys: To iterate over the keys in a HashMap, you can use a for-each loop with the keySet() method:\nfor (String fruit : myMap.keySet()) {\n    System.out.println(fruit + \": \" + myMap.get(fruit));\n}\nIterate over values: To iterate over the values in a HashMap, you can use a for-each loop with the values() method:\nfor (Integer count : myMap.values()) {\n    System.out.println(count);\n}\nIterate over key-value pairs: To iterate over the key-value pairs in a HashMap, you can use a for-each loop with the entrySet() method:\nfor (HashMap.Entry&lt;String, Integer&gt; entry : myMap.entrySet()) {\n    System.out.println(entry.getKey() + \": \" + entry.getValue());\n}\n\nA HashMap can be a useful data structure when you need to store key-value pairs efficiently. It provides constant-time performance for common operations like put, get, and remove, making it an ideal choice for various applications."
  },
  {
    "objectID": "10_hashing.html#hashtables-in-java",
    "href": "10_hashing.html#hashtables-in-java",
    "title": "8  Hashing, Hash Tables, and Hash Maps",
    "section": "8.10 HashTables in Java",
    "text": "8.10 HashTables in Java\nA HashTable is a collection in Java that implements the Map interface and uses a hash table for storage. It is similar to a HashMap but with some differences, such as being synchronized, which makes it thread-safe. HashTable stores key-value pairs, where each key is unique, and the keys are not ordered.\nHere’s how to use a HashTable in Java:\n\nImport the HashTable class: To use the HashTable class in your Java code, you’ll need to import it from the java.util package:\nimport java.util.Hashtable;\nCreate a HashTable: To create a new HashTable, use the following syntax:\nHashtable&lt;String, Integer&gt; myTable = new Hashtable&lt;String, Integer&gt;();\nAdd elements: To add key-value pairs to the HashTable, use the put() method:\nmyTable.put(\"apple\", 3);\nmyTable.put(\"banana\", 5);\nmyTable.put(\"orange\", 2);\nAccess elements: To access the value associated with a key, use the get() method:\nint apples = myTable.get(\"apple\"); // 3\nint oranges = myTable.get(\"orange\"); // 2\nRemove elements: To remove a key-value pair from the HashTable, use the remove() method:\nmyTable.remove(\"banana\");\nCheck if a key exists: To check if a key is in the HashTable, use the containsKey() method:\nboolean hasApple = myTable.containsKey(\"apple\"); // true\nboolean hasGrape = myTable.containsKey(\"grape\"); // false\nIterate over keys: To iterate over the keys in a HashTable, you can use a for-each loop with the keySet() method:\nfor (String fruit : myTable.keySet()) {\n    System.out.println(fruit + \": \" + myTable.get(fruit));\n}\nIterate over values: To iterate over the values in a HashTable, you can use a for-each loop with the values() method:\nfor (Integer count : myTable.values()) {\n    System.out.println(count);\n}\nIterate over key-value pairs: To iterate over the key-value pairs in a HashTable, you can use a for-each loop with the entrySet() method:\nfor (Hashtable.Entry&lt;String, Integer&gt; entry : myTable.entrySet()) {\n    System.out.println(entry.getKey() + \": \" + entry.getValue());\n}\n\nA HashTable can be a useful data structure when you need to store key-value pairs and require thread-safe operations. However, it has some performance overhead due to synchronization, so if thread safety is not a concern, a HashMap is generally a more efficient choice."
  },
  {
    "objectID": "10_hashing.html#hashsets-in-java",
    "href": "10_hashing.html#hashsets-in-java",
    "title": "8  Hashing, Hash Tables, and Hash Maps",
    "section": "8.11 HashSets in Java",
    "text": "8.11 HashSets in Java\nA HashSet is a collection in Java that implements the Set interface and uses a hash table for storage. It does not store key-value pairs like hash tables or hash maps, but instead stores unique elements. The elements in a HashSet are not ordered, and duplicate values are not allowed.\nHere’s how to use a HashSet in Java:\n\nImport the HashSet class: To use the HashSet class in your Java code, you’ll need to import it from the java.util package:\nimport java.util.HashSet;\nCreate a HashSet: To create a new HashSet, use the following syntax:\nHashSet&lt;String&gt; mySet = new HashSet&lt;String&gt;();\nAdd elements: To add elements to the HashSet, use the add() method:\nmySet.add(\"apple\");\nmySet.add(\"banana\");\nmySet.add(\"orange\");\nRemove elements: To remove elements from the HashSet, use the remove() method:\nmySet.remove(\"banana\");\nCheck if an element exists: To check if an element is in the HashSet, use the contains() method:\nboolean hasApple = mySet.contains(\"apple\"); // true\nboolean hasGrape = mySet.contains(\"grape\"); // false\nIterate over elements: To iterate over the elements in a HashSet, you can use a for-each loop:\nfor (String fruit : mySet) {\n    System.out.println(fruit);\n}\n\nA HashSet can be a useful data structure when you need to store a collection of unique elements without any specific order. It provides constant-time performance for common operations like add, remove, and contains, making it an efficient choice for many applications."
  },
  {
    "objectID": "10_hashing.html#hascode-and-equals-in-java",
    "href": "10_hashing.html#hascode-and-equals-in-java",
    "title": "8  Hashing, Hash Tables, and Hash Maps",
    "section": "8.12 hasCode and equals in Java",
    "text": "8.12 hasCode and equals in Java\nIn Java, the hashCode method is part of the Object class, which is the superclass of all Java classes. The purpose of the hashCode method is to provide a default implementation for generating hash codes, which are integer values that represent the memory address of an object.\n\n8.12.1 The hashCode Method\nThe hashCode method has the following signature:\npublic int hashCode()\nThis method returns an integer hash code for the object on which it is called. By default, it returns a hash code that is based on the object’s memory address, but this behavior can be overridden in subclasses to provide custom hash code generation.\nA well-implemented hashCode method should follow these general rules:\n\nIf two objects are equal according to their equals() method, they must have the same hash code.\nIf two objects have the same hash code, they are not necessarily equal according to their equals() method.\nThe hash code of an object should not change over time unless the information used in the equals() method also changes.\n\n\n\n8.12.2 Overriding the hashCode Method\nWhen creating custom classes, it is important to override the hashCode method if the equals() method is also overridden. This ensures that the general contract of the hashCode method is maintained, which is essential for the correct functioning of hash-based data structures like HashSet and HashMap.\nHere’s an example of a custom Person class that overrides both the equals() and hashCode() methods:\npublic class Person {\n    private String name;\n    private int age;\n\n    // Constructor, getters, and setters\n\n    @Override\n    public boolean equals(Object obj) {\n        if (this == obj) {\n            return true;\n        }\n        if (obj == null || getClass() != obj.getClass()) {\n            return false;\n        }\n        Person person = (Person) obj;\n        return age == person.age && Objects.equals(name, person.name);\n    }\n\n    @Override\n    public int hashCode() {\n        return Objects.hash(name, age);\n    }\n}\nIn this example, the equals() method checks if two Person objects have the same name and age. The hashCode() method uses the Objects.hash() utility method, which generates a hash code based on the name and age fields.\n\n\n8.12.3 Using hashCode with Java Collections\nThe hashCode method plays a crucial role in the performance of Java’s hash-based data structures, such as HashSet, HashMap, and HashTable. These data structures rely on the hashCode method to efficiently store and retrieve objects based on their hash codes.\nWhen working with these collections, it is important to ensure that the hashCode method is correctly implemented for the objects being stored. Failing to do so can lead to poor performance or incorrect behavior.\nIn summary, the hashCode method in Java is a critical part of the Object class that provides a default implementation for generating hash codes. When creating custom classes, it is essential to override the hashCode method if the equals() method is also overridden, ensuring the correct functioning of hash-based data structures like HashSet and HashMap."
  }
]